[
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": " int perf_config(config_fn_t fn, void *data)\n {\n \tint ret = 0, found = 0;\n\tchar *repo_config = NULL;\n \tconst char *home = NULL;\n \n \t/* Setting $PERF_CONFIG makes perf read _only_ the given config file. */\n\tif (config_exclusive_filename)\n\t\treturn perf_config_from_file(fn, config_exclusive_filename, data);\n\tif (perf_config_system() && !access(perf_etc_perfconfig(), R_OK)) {\n\t\tret += perf_config_from_file(fn, perf_etc_perfconfig(),\n\t\t\t\t\t    data);\n\t\tfound += 1;\n\t}\n\n\thome = getenv(\"HOME\");\n\tif (perf_config_global() && home) {\n\t\tchar *user_config = strdup(mkpath(\"%s/.perfconfig\", home));\n\t\tif (!access(user_config, R_OK)) {\n\t\t\tret += perf_config_from_file(fn, user_config, data);\n\t\t\tfound += 1;\n\t\t}\n \t\tfree(user_config);\n \t}\n \n\trepo_config = perf_pathdup(\"config\");\n\tif (!access(repo_config, R_OK)) {\n\t\tret += perf_config_from_file(fn, repo_config, data);\n\t\tfound += 1;\n\t}\n\tfree(repo_config);\n \tif (found == 0)\n \t\treturn -1;\n \treturn ret;\n}\n",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 141835,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": " int perf_config(config_fn_t fn, void *data)\n {\n \tint ret = 0, found = 0;\n \tconst char *home = NULL;\n \n \t/* Setting $PERF_CONFIG makes perf read _only_ the given config file. */\n\tif (config_exclusive_filename)\n\t\treturn perf_config_from_file(fn, config_exclusive_filename, data);\n\tif (perf_config_system() && !access(perf_etc_perfconfig(), R_OK)) {\n\t\tret += perf_config_from_file(fn, perf_etc_perfconfig(),\n\t\t\t\t\t    data);\n\t\tfound += 1;\n\t}\n\n\thome = getenv(\"HOME\");\n\tif (perf_config_global() && home) {\n\t\tchar *user_config = strdup(mkpath(\"%s/.perfconfig\", home));\n\t\tif (!access(user_config, R_OK)) {\n\t\t\tret += perf_config_from_file(fn, user_config, data);\n\t\t\tfound += 1;\n\t\t}\n \t\tfree(user_config);\n \t}\n \n \tif (found == 0)\n \t\treturn -1;\n \treturn ret;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 141836,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static inline int process_nested_data(UNSERIALIZE_PARAMETER, HashTable *ht, long elements, int objprops)\n{\n\twhile (elements-- > 0) {\n\t\tzval *key, *data, **old_data;\n\n\t\tALLOC_INIT_ZVAL(key);\n\n\t\tif (!php_var_unserialize(&key, p, max, NULL TSRMLS_CC)) {\n\t\t\tzval_dtor(key);\n\t\t\tFREE_ZVAL(key);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (Z_TYPE_P(key) != IS_LONG && Z_TYPE_P(key) != IS_STRING) {\n\t\t\tzval_dtor(key);\n\t\t\tFREE_ZVAL(key);\n\t\t\treturn 0;\n\t\t}\n\n\t\tALLOC_INIT_ZVAL(data);\n\n\t\tif (!php_var_unserialize(&data, p, max, var_hash TSRMLS_CC)) {\n\t\t\tzval_dtor(key);\n\t\t\tFREE_ZVAL(key);\n\t\t\tzval_dtor(data);\n\t\t\tFREE_ZVAL(data);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!objprops) {\n\t\t\tswitch (Z_TYPE_P(key)) {\n\t\t\tcase IS_LONG:\n\t\t\t\tif (zend_hash_index_find(ht, Z_LVAL_P(key), (void **)&old_data)==SUCCESS) {\n\t\t\t\t\tvar_push_dtor(var_hash, old_data);\n\t\t\t\t}\n\t\t\t\tzend_hash_index_update(ht, Z_LVAL_P(key), &data, sizeof(data), NULL);\n\t\t\t\tbreak;\n\t\t\tcase IS_STRING:\n\t\t\t\tif (zend_symtable_find(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, (void **)&old_data)==SUCCESS) {\n\t\t\t\t\tvar_push_dtor(var_hash, old_data);\n\t\t\t\t}\n\t\t\t\tzend_symtable_update(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, &data, sizeof(data), NULL);\n\t\t\t\tbreak;\n\t\t\t}\n \t\t} else {\n \t\t\t/* object properties should include no integers */\n \t\t\tconvert_to_string(key);\n\t\t\tif (zend_symtable_find(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, (void **)&old_data)==SUCCESS) {\n \t\t\t\tvar_push_dtor(var_hash, old_data);\n \t\t\t}\n \t\t\tzend_hash_update(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, &data,\n\t\t\t\t\tsizeof data, NULL);\n\t\t}\n\t\t\n\t\tzval_dtor(key);\n\t\tFREE_ZVAL(key);\n\n\t\tif (elements && *(*p-1) != ';' && *(*p-1) != '}') {\n\t\t\t(*p)--;\n\t\t\treturn 0;\n\t\t}\n\t}\n",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 142005,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static inline int process_nested_data(UNSERIALIZE_PARAMETER, HashTable *ht, long elements, int objprops)\n{\n\twhile (elements-- > 0) {\n\t\tzval *key, *data, **old_data;\n\n\t\tALLOC_INIT_ZVAL(key);\n\n\t\tif (!php_var_unserialize(&key, p, max, NULL TSRMLS_CC)) {\n\t\t\tzval_dtor(key);\n\t\t\tFREE_ZVAL(key);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (Z_TYPE_P(key) != IS_LONG && Z_TYPE_P(key) != IS_STRING) {\n\t\t\tzval_dtor(key);\n\t\t\tFREE_ZVAL(key);\n\t\t\treturn 0;\n\t\t}\n\n\t\tALLOC_INIT_ZVAL(data);\n\n\t\tif (!php_var_unserialize(&data, p, max, var_hash TSRMLS_CC)) {\n\t\t\tzval_dtor(key);\n\t\t\tFREE_ZVAL(key);\n\t\t\tzval_dtor(data);\n\t\t\tFREE_ZVAL(data);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!objprops) {\n\t\t\tswitch (Z_TYPE_P(key)) {\n\t\t\tcase IS_LONG:\n\t\t\t\tif (zend_hash_index_find(ht, Z_LVAL_P(key), (void **)&old_data)==SUCCESS) {\n\t\t\t\t\tvar_push_dtor(var_hash, old_data);\n\t\t\t\t}\n\t\t\t\tzend_hash_index_update(ht, Z_LVAL_P(key), &data, sizeof(data), NULL);\n\t\t\t\tbreak;\n\t\t\tcase IS_STRING:\n\t\t\t\tif (zend_symtable_find(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, (void **)&old_data)==SUCCESS) {\n\t\t\t\t\tvar_push_dtor(var_hash, old_data);\n\t\t\t\t}\n\t\t\t\tzend_symtable_update(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, &data, sizeof(data), NULL);\n\t\t\t\tbreak;\n\t\t\t}\n \t\t} else {\n \t\t\t/* object properties should include no integers */\n \t\t\tconvert_to_string(key);\n\t\t\tif (zend_hash_find(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, (void **)&old_data)==SUCCESS) {\n \t\t\t\tvar_push_dtor(var_hash, old_data);\n \t\t\t}\n \t\t\tzend_hash_update(ht, Z_STRVAL_P(key), Z_STRLEN_P(key) + 1, &data,\n\t\t\t\t\tsizeof data, NULL);\n\t\t}\n\t\t\n\t\tzval_dtor(key);\n\t\tFREE_ZVAL(key);\n\n\t\tif (elements && *(*p-1) != ';' && *(*p-1) != '}') {\n\t\t\t(*p)--;\n\t\t\treturn 0;\n\t\t}\n\t}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 142006,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "cmsPipeline* DefaultICCintents(cmsContext       ContextID,\n                               cmsUInt32Number  nProfiles,\n                               cmsUInt32Number  TheIntents[],\n                               cmsHPROFILE      hProfiles[],\n                               cmsBool          BPC[],\n                               cmsFloat64Number AdaptationStates[],\n                               cmsUInt32Number  dwFlags)\n{\n    cmsPipeline* Lut = NULL;\n    cmsPipeline* Result;\n    cmsHPROFILE hProfile;\n    cmsMAT3 m;\n    cmsVEC3 off;\n    cmsColorSpaceSignature ColorSpaceIn, ColorSpaceOut, CurrentColorSpace;\n    cmsProfileClassSignature ClassSig;\n    cmsUInt32Number  i, Intent;\n\n    if (nProfiles == 0) return NULL;\n\n    Result = cmsPipelineAlloc(ContextID, 0, 0);\n    if (Result == NULL) return NULL;\n\n    CurrentColorSpace = cmsGetColorSpace(hProfiles[0]);\n\n    for (i=0; i < nProfiles; i++) {\n\n        cmsBool  lIsDeviceLink, lIsInput;\n\n        hProfile      = hProfiles[i];\n        ClassSig      = cmsGetDeviceClass(hProfile);\n        lIsDeviceLink = (ClassSig == cmsSigLinkClass || ClassSig == cmsSigAbstractClass );\n\n        if ((i == 0) && !lIsDeviceLink) {\n            lIsInput = TRUE;\n        }\n        else {\n        lIsInput      = (CurrentColorSpace != cmsSigXYZData) &&\n                        (CurrentColorSpace != cmsSigLabData);\n        }\n\n        Intent        = TheIntents[i];\n\n        if (lIsInput || lIsDeviceLink) {\n\n            ColorSpaceIn    = cmsGetColorSpace(hProfile);\n            ColorSpaceOut   = cmsGetPCS(hProfile);\n        }\n        else {\n\n            ColorSpaceIn    = cmsGetPCS(hProfile);\n            ColorSpaceOut   = cmsGetColorSpace(hProfile);\n        }\n\n        if (!ColorSpaceIsCompatible(ColorSpaceIn, CurrentColorSpace)) {\n\n            cmsSignalError(ContextID, cmsERROR_COLORSPACE_CHECK, \"ColorSpace mismatch\");\n            goto Error;\n        }\n\n        if (lIsDeviceLink || ((ClassSig == cmsSigNamedColorClass) && (nProfiles == 1))) {\n\n            Lut = _cmsReadDevicelinkLUT(hProfile, Intent);\n            if (Lut == NULL) goto Error;\n\n             if (ClassSig == cmsSigAbstractClass && i > 0) {\n                if (!ComputeConversion(i, hProfiles, Intent, BPC[i], AdaptationStates[i], &m, &off)) goto Error;\n             }\n             else {\n                _cmsMAT3identity(&m);\n                _cmsVEC3init(&off, 0, 0, 0);\n             }\n\n\n            if (!AddConversion(Result, CurrentColorSpace, ColorSpaceIn, &m, &off)) goto Error;\n\n        }\n        else {\n\n            if (lIsInput) {\n                Lut = _cmsReadInputLUT(hProfile, Intent);\n                if (Lut == NULL) goto Error;\n            }\n            else {\n\n                Lut = _cmsReadOutputLUT(hProfile, Intent);\n                if (Lut == NULL) goto Error;\n\n\n                if (!ComputeConversion(i, hProfiles, Intent, BPC[i], AdaptationStates[i], &m, &off)) goto Error;\n                if (!AddConversion(Result, CurrentColorSpace, ColorSpaceIn, &m, &off)) goto Error;\n\n            }\n        }\n\n         if (!cmsPipelineCat(Result, Lut))\n             goto Error;\n         cmsPipelineFree(Lut);\n \n         CurrentColorSpace = ColorSpaceOut;\n    }\n\n    return Result;\n \n Error:\n \n    cmsPipelineFree(Lut);\n     if (Result != NULL) cmsPipelineFree(Result);\n     return NULL;\n \n    cmsUNUSED_PARAMETER(dwFlags);\n}\n",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 142237,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "cmsPipeline* DefaultICCintents(cmsContext       ContextID,\n                               cmsUInt32Number  nProfiles,\n                               cmsUInt32Number  TheIntents[],\n                               cmsHPROFILE      hProfiles[],\n                               cmsBool          BPC[],\n                               cmsFloat64Number AdaptationStates[],\n                               cmsUInt32Number  dwFlags)\n{\n    cmsPipeline* Lut = NULL;\n    cmsPipeline* Result;\n    cmsHPROFILE hProfile;\n    cmsMAT3 m;\n    cmsVEC3 off;\n    cmsColorSpaceSignature ColorSpaceIn, ColorSpaceOut, CurrentColorSpace;\n    cmsProfileClassSignature ClassSig;\n    cmsUInt32Number  i, Intent;\n\n    if (nProfiles == 0) return NULL;\n\n    Result = cmsPipelineAlloc(ContextID, 0, 0);\n    if (Result == NULL) return NULL;\n\n    CurrentColorSpace = cmsGetColorSpace(hProfiles[0]);\n\n    for (i=0; i < nProfiles; i++) {\n\n        cmsBool  lIsDeviceLink, lIsInput;\n\n        hProfile      = hProfiles[i];\n        ClassSig      = cmsGetDeviceClass(hProfile);\n        lIsDeviceLink = (ClassSig == cmsSigLinkClass || ClassSig == cmsSigAbstractClass );\n\n        if ((i == 0) && !lIsDeviceLink) {\n            lIsInput = TRUE;\n        }\n        else {\n        lIsInput      = (CurrentColorSpace != cmsSigXYZData) &&\n                        (CurrentColorSpace != cmsSigLabData);\n        }\n\n        Intent        = TheIntents[i];\n\n        if (lIsInput || lIsDeviceLink) {\n\n            ColorSpaceIn    = cmsGetColorSpace(hProfile);\n            ColorSpaceOut   = cmsGetPCS(hProfile);\n        }\n        else {\n\n            ColorSpaceIn    = cmsGetPCS(hProfile);\n            ColorSpaceOut   = cmsGetColorSpace(hProfile);\n        }\n\n        if (!ColorSpaceIsCompatible(ColorSpaceIn, CurrentColorSpace)) {\n\n            cmsSignalError(ContextID, cmsERROR_COLORSPACE_CHECK, \"ColorSpace mismatch\");\n            goto Error;\n        }\n\n        if (lIsDeviceLink || ((ClassSig == cmsSigNamedColorClass) && (nProfiles == 1))) {\n\n            Lut = _cmsReadDevicelinkLUT(hProfile, Intent);\n            if (Lut == NULL) goto Error;\n\n             if (ClassSig == cmsSigAbstractClass && i > 0) {\n                if (!ComputeConversion(i, hProfiles, Intent, BPC[i], AdaptationStates[i], &m, &off)) goto Error;\n             }\n             else {\n                _cmsMAT3identity(&m);\n                _cmsVEC3init(&off, 0, 0, 0);\n             }\n\n\n            if (!AddConversion(Result, CurrentColorSpace, ColorSpaceIn, &m, &off)) goto Error;\n\n        }\n        else {\n\n            if (lIsInput) {\n                Lut = _cmsReadInputLUT(hProfile, Intent);\n                if (Lut == NULL) goto Error;\n            }\n            else {\n\n                Lut = _cmsReadOutputLUT(hProfile, Intent);\n                if (Lut == NULL) goto Error;\n\n\n                if (!ComputeConversion(i, hProfiles, Intent, BPC[i], AdaptationStates[i], &m, &off)) goto Error;\n                if (!AddConversion(Result, CurrentColorSpace, ColorSpaceIn, &m, &off)) goto Error;\n\n            }\n        }\n\n         if (!cmsPipelineCat(Result, Lut))\n             goto Error;\n\n         cmsPipelineFree(Lut);\n        Lut = NULL;\n \n         CurrentColorSpace = ColorSpaceOut;\n    }\n\n    return Result;\n \n Error:\n \n    if (Lut != NULL) cmsPipelineFree(Lut);\n     if (Result != NULL) cmsPipelineFree(Result);\n     return NULL;\n \n    cmsUNUSED_PARAMETER(dwFlags);\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 142238,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "void WebRuntimeFeatures::enableSpeechSynthesis(bool enable)\n{\n    RuntimeEnabledFeatures::setSpeechSynthesisEnabled(enable);\n}\n",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 143589,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "void WebRuntimeFeatures::enableFileSystem(bool enable)\n{\n    RuntimeEnabledFeatures::setFileSystemEnabled(enable);\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 143590,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "int virtio_load(VirtIODevice *vdev, QEMUFile *f)\n{\n    int i, ret;\n    uint32_t num;\n    uint32_t features;\n    uint32_t supported_features;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->load_config) {\n        ret = k->load_config(qbus->parent, f);\n        if (ret)\n            return ret;\n    }\n\n     qemu_get_8s(f, &vdev->status);\n     qemu_get_8s(f, &vdev->isr);\n     qemu_get_be16s(f, &vdev->queue_sel);\n     qemu_get_be32s(f, &features);\n \n     if (virtio_set_features(vdev, features) < 0) {\n        return -1;\n    }\n    vdev->config_len = qemu_get_be32(f);\n    qemu_get_buffer(f, vdev->config, vdev->config_len);\n\n    num = qemu_get_be32(f);\n\n    if (num > VIRTIO_PCI_QUEUE_MAX) {\n        error_report(\"Invalid number of PCI queues: 0x%x\", num);\n        return -1;\n    }\n\n    for (i = 0; i < num; i++) {\n        vdev->vq[i].vring.num = qemu_get_be32(f);\n        if (k->has_variable_vring_alignment) {\n            vdev->vq[i].vring.align = qemu_get_be32(f);\n        }\n        vdev->vq[i].pa = qemu_get_be64(f);\n        qemu_get_be16s(f, &vdev->vq[i].last_avail_idx);\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n\n        if (vdev->vq[i].pa) {\n            uint16_t nheads;\n            virtqueue_init(&vdev->vq[i]);\n            nheads = vring_avail_idx(&vdev->vq[i]) - vdev->vq[i].last_avail_idx;\n            /* Check it isn't doing very strange things with descriptor numbers. */\n            if (nheads > vdev->vq[i].vring.num) {\n                error_report(\"VQ %d size 0x%x Guest index 0x%x \"\n                             \"inconsistent with Host index 0x%x: delta 0x%x\",\n                             i, vdev->vq[i].vring.num,\n                             vring_avail_idx(&vdev->vq[i]),\n                             vdev->vq[i].last_avail_idx, nheads);\n                return -1;\n            }\n        } else if (vdev->vq[i].last_avail_idx) {\n            error_report(\"VQ %d address 0x0 \"\n                         \"inconsistent with Host index 0x%x\",\n                         i, vdev->vq[i].last_avail_idx);\n                return -1;\n\t}\n        if (k->load_queue) {\n            ret = k->load_queue(qbus->parent, i, f);\n            if (ret)\n                return ret;\n        }\n    }\n\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n    return 0;\n}\n",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 144385,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "int virtio_load(VirtIODevice *vdev, QEMUFile *f)\n{\n    int i, ret;\n    uint32_t num;\n    uint32_t features;\n    uint32_t supported_features;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->load_config) {\n        ret = k->load_config(qbus->parent, f);\n        if (ret)\n            return ret;\n    }\n\n     qemu_get_8s(f, &vdev->status);\n     qemu_get_8s(f, &vdev->isr);\n     qemu_get_be16s(f, &vdev->queue_sel);\n    if (vdev->queue_sel >= VIRTIO_PCI_QUEUE_MAX) {\n        return -1;\n    }\n     qemu_get_be32s(f, &features);\n \n     if (virtio_set_features(vdev, features) < 0) {\n        return -1;\n    }\n    vdev->config_len = qemu_get_be32(f);\n    qemu_get_buffer(f, vdev->config, vdev->config_len);\n\n    num = qemu_get_be32(f);\n\n    if (num > VIRTIO_PCI_QUEUE_MAX) {\n        error_report(\"Invalid number of PCI queues: 0x%x\", num);\n        return -1;\n    }\n\n    for (i = 0; i < num; i++) {\n        vdev->vq[i].vring.num = qemu_get_be32(f);\n        if (k->has_variable_vring_alignment) {\n            vdev->vq[i].vring.align = qemu_get_be32(f);\n        }\n        vdev->vq[i].pa = qemu_get_be64(f);\n        qemu_get_be16s(f, &vdev->vq[i].last_avail_idx);\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n\n        if (vdev->vq[i].pa) {\n            uint16_t nheads;\n            virtqueue_init(&vdev->vq[i]);\n            nheads = vring_avail_idx(&vdev->vq[i]) - vdev->vq[i].last_avail_idx;\n            /* Check it isn't doing very strange things with descriptor numbers. */\n            if (nheads > vdev->vq[i].vring.num) {\n                error_report(\"VQ %d size 0x%x Guest index 0x%x \"\n                             \"inconsistent with Host index 0x%x: delta 0x%x\",\n                             i, vdev->vq[i].vring.num,\n                             vring_avail_idx(&vdev->vq[i]),\n                             vdev->vq[i].last_avail_idx, nheads);\n                return -1;\n            }\n        } else if (vdev->vq[i].last_avail_idx) {\n            error_report(\"VQ %d address 0x0 \"\n                         \"inconsistent with Host index 0x%x\",\n                         i, vdev->vq[i].last_avail_idx);\n                return -1;\n\t}\n        if (k->load_queue) {\n            ret = k->load_queue(qbus->parent, i, f);\n            if (ret)\n                return ret;\n        }\n    }\n\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n    return 0;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 144386,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static target_ulong disas_insn(CPUX86State *env, DisasContext *s,\n                               target_ulong pc_start)\n{\n    int b, prefixes;\n    int shift;\n    TCGMemOp ot, aflag, dflag;\n    int modrm, reg, rm, mod, op, opreg, val;\n    target_ulong next_eip, tval;\n    int rex_w, rex_r;\n\n    s->pc_start = s->pc = pc_start;\n    prefixes = 0;\n    s->override = -1;\n    rex_w = -1;\n    rex_r = 0;\n#ifdef TARGET_X86_64\n    s->rex_x = 0;\n    s->rex_b = 0;\n    x86_64_hregs = 0;\n#endif\n    s->rip_offset = 0; /* for relative ip address */\n     s->vex_l = 0;\n     s->vex_v = 0;\n  next_byte:\n     b = cpu_ldub_code(env, s->pc);\n     s->pc++;\n     /* Collect prefixes.  */\n    switch (b) {\n    case 0xf3:\n        prefixes |= PREFIX_REPZ;\n        goto next_byte;\n    case 0xf2:\n        prefixes |= PREFIX_REPNZ;\n        goto next_byte;\n    case 0xf0:\n        prefixes |= PREFIX_LOCK;\n        goto next_byte;\n    case 0x2e:\n        s->override = R_CS;\n        goto next_byte;\n    case 0x36:\n        s->override = R_SS;\n        goto next_byte;\n    case 0x3e:\n        s->override = R_DS;\n        goto next_byte;\n    case 0x26:\n        s->override = R_ES;\n        goto next_byte;\n    case 0x64:\n        s->override = R_FS;\n        goto next_byte;\n    case 0x65:\n        s->override = R_GS;\n        goto next_byte;\n    case 0x66:\n        prefixes |= PREFIX_DATA;\n        goto next_byte;\n    case 0x67:\n        prefixes |= PREFIX_ADR;\n        goto next_byte;\n#ifdef TARGET_X86_64\n    case 0x40 ... 0x4f:\n        if (CODE64(s)) {\n            /* REX prefix */\n            rex_w = (b >> 3) & 1;\n            rex_r = (b & 0x4) << 1;\n            s->rex_x = (b & 0x2) << 2;\n            REX_B(s) = (b & 0x1) << 3;\n            x86_64_hregs = 1; /* select uniform byte register addressing */\n            goto next_byte;\n        }\n        break;\n#endif\n    case 0xc5: /* 2-byte VEX */\n    case 0xc4: /* 3-byte VEX */\n        /* VEX prefixes cannot be used except in 32-bit mode.\n           Otherwise the instruction is LES or LDS.  */\n        if (s->code32 && !s->vm86) {\n            static const int pp_prefix[4] = {\n                0, PREFIX_DATA, PREFIX_REPZ, PREFIX_REPNZ\n            };\n            int vex3, vex2 = cpu_ldub_code(env, s->pc);\n\n            if (!CODE64(s) && (vex2 & 0xc0) != 0xc0) {\n                /* 4.1.4.6: In 32-bit mode, bits [7:6] must be 11b,\n                   otherwise the instruction is LES or LDS.  */\n                break;\n            }\n            s->pc++;\n\n            /* 4.1.1-4.1.3: No preceding lock, 66, f2, f3, or rex prefixes. */\n            if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ\n                            | PREFIX_LOCK | PREFIX_DATA)) {\n                goto illegal_op;\n            }\n#ifdef TARGET_X86_64\n            if (x86_64_hregs) {\n                goto illegal_op;\n            }\n#endif\n            rex_r = (~vex2 >> 4) & 8;\n            if (b == 0xc5) {\n                vex3 = vex2;\n                b = cpu_ldub_code(env, s->pc++);\n            } else {\n#ifdef TARGET_X86_64\n                s->rex_x = (~vex2 >> 3) & 8;\n                s->rex_b = (~vex2 >> 2) & 8;\n#endif\n                vex3 = cpu_ldub_code(env, s->pc++);\n                rex_w = (vex3 >> 7) & 1;\n                switch (vex2 & 0x1f) {\n                case 0x01: /* Implied 0f leading opcode bytes.  */\n                    b = cpu_ldub_code(env, s->pc++) | 0x100;\n                    break;\n                case 0x02: /* Implied 0f 38 leading opcode bytes.  */\n                    b = 0x138;\n                    break;\n                case 0x03: /* Implied 0f 3a leading opcode bytes.  */\n                    b = 0x13a;\n                    break;\n                default:   /* Reserved for future use.  */\n                    goto unknown_op;\n                }\n            }\n            s->vex_v = (~vex3 >> 3) & 0xf;\n            s->vex_l = (vex3 >> 2) & 1;\n            prefixes |= pp_prefix[vex3 & 3] | PREFIX_VEX;\n        }\n        break;\n    }\n\n    /* Post-process prefixes.  */\n    if (CODE64(s)) {\n        /* In 64-bit mode, the default data size is 32-bit.  Select 64-bit\n           data with rex_w, and 16-bit data with 0x66; rex_w takes precedence\n           over 0x66 if both are present.  */\n        dflag = (rex_w > 0 ? MO_64 : prefixes & PREFIX_DATA ? MO_16 : MO_32);\n        /* In 64-bit mode, 0x67 selects 32-bit addressing.  */\n        aflag = (prefixes & PREFIX_ADR ? MO_32 : MO_64);\n    } else {\n        /* In 16/32-bit mode, 0x66 selects the opposite data size.  */\n        if (s->code32 ^ ((prefixes & PREFIX_DATA) != 0)) {\n            dflag = MO_32;\n        } else {\n            dflag = MO_16;\n        }\n        /* In 16/32-bit mode, 0x67 selects the opposite addressing.  */\n        if (s->code32 ^ ((prefixes & PREFIX_ADR) != 0)) {\n            aflag = MO_32;\n        }  else {\n            aflag = MO_16;\n        }\n    }\n\n    s->prefix = prefixes;\n    s->aflag = aflag;\n    s->dflag = dflag;\n\n    /* now check op code */\n reswitch:\n    switch(b) {\n    case 0x0f:\n        /**************************/\n        /* extended op code */\n        b = cpu_ldub_code(env, s->pc++) | 0x100;\n        goto reswitch;\n\n        /**************************/\n        /* arith & logic */\n    case 0x00 ... 0x05:\n    case 0x08 ... 0x0d:\n    case 0x10 ... 0x15:\n    case 0x18 ... 0x1d:\n    case 0x20 ... 0x25:\n    case 0x28 ... 0x2d:\n    case 0x30 ... 0x35:\n    case 0x38 ... 0x3d:\n        {\n            int op, f, val;\n            op = (b >> 3) & 7;\n            f = (b >> 1) & 3;\n\n            ot = mo_b_d(b, dflag);\n\n            switch(f) {\n            case 0: /* OP Ev, Gv */\n                modrm = cpu_ldub_code(env, s->pc++);\n                reg = ((modrm >> 3) & 7) | rex_r;\n                mod = (modrm >> 6) & 3;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    opreg = OR_TMP0;\n                } else if (op == OP_XORL && rm == reg) {\n                xor_zero:\n                    /* xor reg, reg optimisation */\n                    set_cc_op(s, CC_OP_CLR);\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                    break;\n                } else {\n                    opreg = rm;\n                }\n                gen_op_mov_v_reg(ot, cpu_T1, reg);\n                gen_op(s, op, ot, opreg);\n                break;\n            case 1: /* OP Gv, Ev */\n                modrm = cpu_ldub_code(env, s->pc++);\n                mod = (modrm >> 6) & 3;\n                reg = ((modrm >> 3) & 7) | rex_r;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                } else if (op == OP_XORL && rm == reg) {\n                    goto xor_zero;\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T1, rm);\n                }\n                gen_op(s, op, ot, reg);\n                break;\n            case 2: /* OP A, Iv */\n                val = insn_get(env, s, ot);\n                tcg_gen_movi_tl(cpu_T1, val);\n                gen_op(s, op, ot, OR_EAX);\n                break;\n            }\n        }\n        break;\n\n    case 0x82:\n        if (CODE64(s))\n            goto illegal_op;\n    case 0x80: /* GRP1 */\n    case 0x81:\n    case 0x83:\n        {\n            int val;\n\n            ot = mo_b_d(b, dflag);\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (b == 0x83)\n                    s->rip_offset = 1;\n                else\n                    s->rip_offset = insn_const_size(ot);\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = rm;\n            }\n\n            switch(b) {\n            default:\n            case 0x80:\n            case 0x81:\n            case 0x82:\n                val = insn_get(env, s, ot);\n                break;\n            case 0x83:\n                val = (int8_t)insn_get(env, s, MO_8);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op(s, op, ot, opreg);\n        }\n        break;\n\n        /**************************/\n        /* inc, dec, and other misc arith */\n    case 0x40 ... 0x47: /* inc Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), 1);\n        break;\n    case 0x48 ... 0x4f: /* dec Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), -1);\n        break;\n    case 0xf6: /* GRP3 */\n    case 0xf7:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (mod != 3) {\n            if (op == 0) {\n                s->rip_offset = insn_const_size(ot);\n            }\n            gen_lea_modrm(env, s, modrm);\n            /* For those below that handle locked memory, don't load here.  */\n            if (!(s->prefix & PREFIX_LOCK)\n                || op != 2) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* test */\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op_testl_T0_T1_cc();\n            set_cc_op(s, CC_OP_LOGICB + ot);\n            break;\n        case 2: /* not */\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                tcg_gen_movi_tl(cpu_T0, ~0);\n                tcg_gen_atomic_xor_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n            } else {\n                tcg_gen_not_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            break;\n        case 3: /* neg */\n            if (s->prefix & PREFIX_LOCK) {\n                TCGLabel *label1;\n                TCGv a0, t0, t1, t2;\n\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                a0 = tcg_temp_local_new();\n                t0 = tcg_temp_local_new();\n                label1 = gen_new_label();\n\n                tcg_gen_mov_tl(a0, cpu_A0);\n                tcg_gen_mov_tl(t0, cpu_T0);\n\n                gen_set_label(label1);\n                t1 = tcg_temp_new();\n                t2 = tcg_temp_new();\n                tcg_gen_mov_tl(t2, t0);\n                tcg_gen_neg_tl(t1, t0);\n                tcg_gen_atomic_cmpxchg_tl(t0, a0, t0, t1,\n                                          s->mem_index, ot | MO_LE);\n                tcg_temp_free(t1);\n                tcg_gen_brcond_tl(TCG_COND_NE, t0, t2, label1);\n\n                tcg_temp_free(t2);\n                tcg_temp_free(a0);\n                tcg_gen_mov_tl(cpu_T0, t0);\n                tcg_temp_free(t0);\n            } else {\n                tcg_gen_neg_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            gen_op_update_neg_cc();\n            set_cc_op(s, CC_OP_SUBB + ot);\n            break;\n        case 4: /* mul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_andi_tl(cpu_cc_src, cpu_T0, 0xff00);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_mulu2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_mulu2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 5: /* imul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_muls2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sari_tl(cpu_cc_src, cpu_regs[R_EAX], 63);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 6: /* div */\n            switch(ot) {\n            case MO_8:\n                gen_helper_divb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_divw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_divl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_divq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        case 7: /* idiv */\n            switch(ot) {\n            case MO_8:\n                gen_helper_idivb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_idivw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_idivl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_idivq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0xfe: /* GRP4 */\n    case 0xff: /* GRP5 */\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (op >= 2 && b == 0xfe) {\n            goto unknown_op;\n        }\n        if (CODE64(s)) {\n            if (op == 2 || op == 4) {\n                /* operand size for jumps is 64 bit */\n                ot = MO_64;\n            } else if (op == 3 || op == 5) {\n                ot = dflag != MO_16 ? MO_32 + (rex_w == 1) : MO_16;\n            } else if (op == 6) {\n                /* default push size is 64 bit */\n                ot = mo_pushpop(s, dflag);\n            }\n        }\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            if (op >= 2 && op != 3 && op != 5)\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* inc Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, 1);\n            break;\n        case 1: /* dec Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, -1);\n            break;\n        case 2: /* call Ev */\n            /* XXX: optimize if memory (no 'and' is necessary) */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            next_eip = s->pc - s->cs_base;\n            tcg_gen_movi_tl(cpu_T1, next_eip);\n            gen_push_v(s, cpu_T1);\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 3: /* lcall Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_lcall:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                           tcg_const_i32(dflag - 1),\n                                           tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_real(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                      tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            }\n            gen_eob(s);\n            break;\n        case 4: /* jmp Ev */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 5: /* ljmp Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_ljmp:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ljmp_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                          tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                gen_op_movl_seg_T0_vm(R_CS);\n                gen_op_jmp_v(cpu_T1);\n            }\n            gen_eob(s);\n            break;\n        case 6: /* push Ev */\n            gen_push_v(s, cpu_T0);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x84: /* test Ev, Gv */\n    case 0x85:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0xa8: /* test eAX, Iv */\n    case 0xa9:\n        ot = mo_b_d(b, dflag);\n        val = insn_get(env, s, ot);\n\n        gen_op_mov_v_reg(ot, cpu_T0, OR_EAX);\n        tcg_gen_movi_tl(cpu_T1, val);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0x98: /* CWDE/CBW */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, R_EAX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, R_EAX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_8, cpu_T0, R_EAX);\n            tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x99: /* CDQ/CWD */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_64, cpu_T0, R_EAX);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 63);\n            gen_op_mov_reg_v(MO_64, R_EDX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 31);\n            gen_op_mov_reg_v(MO_32, R_EDX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 15);\n            gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x1af: /* imul Gv, Ev */\n    case 0x69: /* imul Gv, Ev, I */\n    case 0x6b:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (b == 0x69)\n            s->rip_offset = insn_const_size(ot);\n        else if (b == 0x6b)\n            s->rip_offset = 1;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        if (b == 0x69) {\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else if (b == 0x6b) {\n            val = (int8_t)insn_get(env, s, MO_8);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T1, reg);\n        }\n        switch (ot) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            tcg_gen_muls2_i64(cpu_regs[reg], cpu_T1, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sari_tl(cpu_cc_src, cpu_cc_dst, 63);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_T1);\n            break;\n#endif\n        case MO_32:\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n            tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n            tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                              cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n            break;\n        default:\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n            /* XXX: use 32 bit mul which could be faster */\n            tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        }\n        set_cc_op(s, CC_OP_MULB + ot);\n        break;\n    case 0x1c0:\n    case 0x1c1: /* xadd Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            if (s->prefix & PREFIX_LOCK) {\n                tcg_gen_atomic_fetch_add_tl(cpu_T1, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            } else {\n                gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s, CC_OP_ADDB + ot);\n        break;\n    case 0x1b0:\n    case 0x1b1: /* cmpxchg Ev, Gv */\n        {\n            TCGv oldv, newv, cmpv;\n\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            oldv = tcg_temp_new();\n            newv = tcg_temp_new();\n            cmpv = tcg_temp_new();\n            gen_op_mov_v_reg(ot, newv, reg);\n            tcg_gen_mov_tl(cmpv, cpu_regs[R_EAX]);\n\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_atomic_cmpxchg_tl(oldv, cpu_A0, cmpv, newv,\n                                          s->mem_index, ot | MO_LE);\n                gen_op_mov_reg_v(ot, R_EAX, oldv);\n            } else {\n                if (mod == 3) {\n                    rm = (modrm & 7) | REX_B(s);\n                    gen_op_mov_v_reg(ot, oldv, rm);\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, oldv, cpu_A0);\n                    rm = 0; /* avoid warning */\n                }\n                gen_extu(ot, oldv);\n                gen_extu(ot, cmpv);\n                /* store value = (old == cmp ? new : old);  */\n                tcg_gen_movcond_tl(TCG_COND_EQ, newv, oldv, cmpv, newv, oldv);\n                if (mod == 3) {\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                    gen_op_mov_reg_v(ot, rm, newv);\n                } else {\n                    /* Perform an unconditional store cycle like physical cpu;\n                       must be before changing accumulator to ensure\n                       idempotency if the store faults and the instruction\n                       is restarted */\n                    gen_op_st_v(s, ot, newv, cpu_A0);\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                }\n            }\n            tcg_gen_mov_tl(cpu_cc_src, oldv);\n            tcg_gen_mov_tl(cpu_cc_srcT, cmpv);\n            tcg_gen_sub_tl(cpu_cc_dst, cmpv, oldv);\n            set_cc_op(s, CC_OP_SUBB + ot);\n            tcg_temp_free(oldv);\n            tcg_temp_free(newv);\n            tcg_temp_free(cmpv);\n        }\n        break;\n    case 0x1c7: /* cmpxchg8b */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if ((mod == 3) || ((modrm & 0x38) != 0x8))\n            goto illegal_op;\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            if (!(s->cpuid_ext_features & CPUID_EXT_CX16))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg16b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg16b_unlocked(cpu_env, cpu_A0);\n            }\n        } else\n#endif        \n        {\n            if (!(s->cpuid_features & CPUID_CX8))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg8b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg8b_unlocked(cpu_env, cpu_A0);\n            }\n        }\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n\n        /**************************/\n        /* push/pop */\n    case 0x50 ... 0x57: /* push */\n        gen_op_mov_v_reg(MO_32, cpu_T0, (b & 7) | REX_B(s));\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x58 ... 0x5f: /* pop */\n        ot = gen_pop_T0(s);\n        /* NOTE: order is important for pop %sp */\n        gen_pop_update(s, ot);\n        gen_op_mov_reg_v(ot, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0x60: /* pusha */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_pusha(s);\n        break;\n    case 0x61: /* popa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_popa(s);\n        break;\n    case 0x68: /* push Iv */\n    case 0x6a:\n        ot = mo_pushpop(s, dflag);\n        if (b == 0x68)\n            val = insn_get(env, s, ot);\n        else\n            val = (int8_t)insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x8f: /* pop Ev */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        ot = gen_pop_T0(s);\n        if (mod == 3) {\n            /* NOTE: order is important for pop %sp */\n            gen_pop_update(s, ot);\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            /* NOTE: order is important too for MMU exceptions */\n            s->popl_esp_hack = 1 << ot;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            s->popl_esp_hack = 0;\n            gen_pop_update(s, ot);\n        }\n        break;\n    case 0xc8: /* enter */\n        {\n            int level;\n            val = cpu_lduw_code(env, s->pc);\n            s->pc += 2;\n            level = cpu_ldub_code(env, s->pc++);\n            gen_enter(s, val, level);\n        }\n        break;\n    case 0xc9: /* leave */\n        gen_leave(s);\n        break;\n    case 0x06: /* push es */\n    case 0x0e: /* push cs */\n    case 0x16: /* push ss */\n    case 0x1e: /* push ds */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_op_movl_T0_seg(b >> 3);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x1a0: /* push fs */\n    case 0x1a8: /* push gs */\n        gen_op_movl_T0_seg((b >> 3) & 7);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x07: /* pop es */\n    case 0x17: /* pop ss */\n    case 0x1f: /* pop ds */\n        if (CODE64(s))\n            goto illegal_op;\n        reg = b >> 3;\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, reg);\n        gen_pop_update(s, ot);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x1a1: /* pop fs */\n    case 0x1a9: /* pop gs */\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, (b >> 3) & 7);\n        gen_pop_update(s, ot);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /**************************/\n        /* mov */\n    case 0x88:\n    case 0x89: /* mov Gv, Ev */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0xc6:\n    case 0xc7: /* mov Ev, Iv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod != 3) {\n            s->rip_offset = insn_const_size(ot);\n            gen_lea_modrm(env, s, modrm);\n        }\n        val = insn_get(env, s, ot);\n        tcg_gen_movi_tl(cpu_T0, val);\n        if (mod != 3) {\n            gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_reg_v(ot, (modrm & 7) | REX_B(s), cpu_T0);\n        }\n        break;\n    case 0x8a:\n    case 0x8b: /* mov Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n    case 0x8e: /* mov seg, Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        if (reg >= 6 || reg == R_CS)\n            goto illegal_op;\n        gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n        gen_movl_seg_T0(s, reg);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x8c: /* mov Gv, seg */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (reg >= 6)\n            goto illegal_op;\n        gen_op_movl_T0_seg(reg);\n        ot = mod == 3 ? dflag : MO_16;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n        break;\n\n    case 0x1b6: /* movzbS Gv, Eb */\n    case 0x1b7: /* movzwS Gv, Eb */\n    case 0x1be: /* movsbS Gv, Eb */\n    case 0x1bf: /* movswS Gv, Eb */\n        {\n            TCGMemOp d_ot;\n            TCGMemOp s_ot;\n\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n            /* ot is the size of source */\n            ot = (b & 1) + MO_8;\n            /* s_ot is the sign+size of source */\n            s_ot = b & 8 ? MO_SIGN | ot : ot;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                if (s_ot == MO_SB && byte_reg_is_xH(rm)) {\n                    tcg_gen_sextract_tl(cpu_T0, cpu_regs[rm - 4], 8, 8);\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    switch (s_ot) {\n                    case MO_UB:\n                        tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_SB:\n                        tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_UW:\n                        tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                        break;\n                    default:\n                    case MO_SW:\n                        tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                        break;\n                    }\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, s_ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        }\n        break;\n\n    case 0x8d: /* lea */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            TCGv ea = gen_lea_modrm_1(a);\n            gen_lea_v_seg(s, s->aflag, ea, -1, -1);\n            gen_op_mov_reg_v(dflag, reg, cpu_A0);\n        }\n        break;\n\n    case 0xa0: /* mov EAX, Ov */\n    case 0xa1:\n    case 0xa2: /* mov Ov, EAX */\n    case 0xa3:\n        {\n            target_ulong offset_addr;\n\n            ot = mo_b_d(b, dflag);\n            switch (s->aflag) {\n#ifdef TARGET_X86_64\n            case MO_64:\n                offset_addr = cpu_ldq_code(env, s->pc);\n                s->pc += 8;\n                break;\n#endif\n            default:\n                offset_addr = insn_get(env, s, s->aflag);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_A0, offset_addr);\n            gen_add_A0_ds_seg(s);\n            if ((b & 2) == 0) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(ot, R_EAX, cpu_T0);\n            } else {\n                gen_op_mov_v_reg(ot, cpu_T0, R_EAX);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n        }\n        break;\n    case 0xd7: /* xlat */\n        tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EBX]);\n        tcg_gen_ext8u_tl(cpu_T0, cpu_regs[R_EAX]);\n        tcg_gen_add_tl(cpu_A0, cpu_A0, cpu_T0);\n        gen_extu(s->aflag, cpu_A0);\n        gen_add_A0_ds_seg(s);\n        gen_op_ld_v(s, MO_8, cpu_T0, cpu_A0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xb0 ... 0xb7: /* mov R, Ib */\n        val = insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_op_mov_reg_v(MO_8, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0xb8 ... 0xbf: /* mov R, Iv */\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            uint64_t tmp;\n            /* 64 bit case */\n            tmp = cpu_ldq_code(env, s->pc);\n            s->pc += 8;\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, tmp);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            ot = dflag;\n            val = insn_get(env, s, ot);\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, val);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n        break;\n\n    case 0x91 ... 0x97: /* xchg R, EAX */\n    do_xchg_reg_eax:\n        ot = dflag;\n        reg = (b & 7) | REX_B(s);\n        rm = R_EAX;\n        goto do_xchg_reg;\n    case 0x86:\n    case 0x87: /* xchg Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n        do_xchg_reg:\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            /* for xchg, lock is implicit */\n            tcg_gen_atomic_xchg_tl(cpu_T1, cpu_A0, cpu_T0,\n                                   s->mem_index, ot | MO_LE);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        break;\n    case 0xc4: /* les Gv */\n        /* In CODE64 this is VEX3; see above.  */\n        op = R_ES;\n        goto do_lxx;\n    case 0xc5: /* lds Gv */\n        /* In CODE64 this is VEX2; see above.  */\n        op = R_DS;\n        goto do_lxx;\n    case 0x1b2: /* lss Gv */\n        op = R_SS;\n        goto do_lxx;\n    case 0x1b4: /* lfs Gv */\n        op = R_FS;\n        goto do_lxx;\n    case 0x1b5: /* lgs Gv */\n        op = R_GS;\n    do_lxx:\n        ot = dflag != MO_16 ? MO_32 : MO_16;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_lea_modrm(env, s, modrm);\n        gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n        gen_add_A0_im(s, 1 << ot);\n        /* load the segment first to handle exceptions properly */\n        gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        gen_movl_seg_T0(s, op);\n        /* then put the data */\n        gen_op_mov_reg_v(ot, reg, cpu_T1);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /************************/\n        /* shifts */\n    case 0xc0:\n    case 0xc1:\n        /* shift Ev,Ib */\n        shift = 2;\n    grp2:\n        {\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (shift == 2) {\n                    s->rip_offset = 1;\n                }\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = (modrm & 7) | REX_B(s);\n            }\n\n            /* simpler op */\n            if (shift == 0) {\n                gen_shift(s, op, ot, opreg, OR_ECX);\n            } else {\n                if (shift == 2) {\n                    shift = cpu_ldub_code(env, s->pc++);\n                }\n                gen_shifti(s, op, ot, opreg, shift);\n            }\n        }\n        break;\n    case 0xd0:\n    case 0xd1:\n        /* shift Ev,1 */\n        shift = 1;\n        goto grp2;\n    case 0xd2:\n    case 0xd3:\n        /* shift Ev,cl */\n        shift = 0;\n        goto grp2;\n\n    case 0x1a4: /* shld imm */\n        op = 0;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1a5: /* shld cl */\n        op = 0;\n        shift = 0;\n        goto do_shiftd;\n    case 0x1ac: /* shrd imm */\n        op = 1;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1ad: /* shrd cl */\n        op = 1;\n        shift = 0;\n    do_shiftd:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            opreg = OR_TMP0;\n        } else {\n            opreg = rm;\n        }\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n\n        if (shift) {\n            TCGv imm = tcg_const_tl(cpu_ldub_code(env, s->pc++));\n            gen_shiftd_rm_T1(s, ot, opreg, op, imm);\n            tcg_temp_free(imm);\n        } else {\n            gen_shiftd_rm_T1(s, ot, opreg, op, cpu_regs[R_ECX]);\n        }\n        break;\n\n        /************************/\n        /* floats */\n    case 0xd8 ... 0xdf:\n        if (s->flags & (HF_EM_MASK | HF_TS_MASK)) {\n            /* if CR0.EM or CR0.TS are set, generate an FPU exception */\n            /* XXX: what to do if illegal op ? */\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n            break;\n        }\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = modrm & 7;\n        op = ((b & 7) << 3) | ((modrm >> 3) & 7);\n        if (mod != 3) {\n            /* memory op */\n            gen_lea_modrm(env, s, modrm);\n            switch(op) {\n            case 0x00 ... 0x07: /* fxxxs */\n            case 0x10 ... 0x17: /* fixxxl */\n            case 0x20 ... 0x27: /* fxxxl */\n            case 0x30 ... 0x37: /* fixxx */\n                {\n                    int op1;\n                    op1 = op & 7;\n\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_FT0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n\n                    gen_helper_fp_arith_ST0_FT0(op1);\n                    if (op1 == 3) {\n                        /* fcomp needs pop */\n                        gen_helper_fpop(cpu_env);\n                    }\n                }\n                break;\n            case 0x08: /* flds */\n            case 0x0a: /* fsts */\n            case 0x0b: /* fstps */\n            case 0x18 ... 0x1b: /* fildl, fisttpl, fistl, fistpl */\n            case 0x28 ... 0x2b: /* fldl, fisttpll, fstl, fstpl */\n            case 0x38 ... 0x3b: /* filds, fisttps, fists, fistps */\n                switch(op & 7) {\n                case 0:\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_ST0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n                    break;\n                case 1:\n                    /* XXX: the corresponding CPUID bit must be tested ! */\n                    switch(op >> 4) {\n                    case 1:\n                        gen_helper_fisttl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fisttll_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fistt_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    switch(op >> 4) {\n                    case 0:\n                        gen_helper_fsts_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 1:\n                        gen_helper_fistl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fstl_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fist_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    if ((op & 7) == 3)\n                        gen_helper_fpop(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0c: /* fldenv mem */\n                gen_helper_fldenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0d: /* fldcw mem */\n                tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                gen_helper_fldcw(cpu_env, cpu_tmp2_i32);\n                break;\n            case 0x0e: /* fnstenv mem */\n                gen_helper_fstenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0f: /* fnstcw mem */\n                gen_helper_fnstcw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x1d: /* fldt mem */\n                gen_helper_fldt_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x1f: /* fstpt mem */\n                gen_helper_fstt_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* frstor mem */\n                gen_helper_frstor(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2e: /* fnsave mem */\n                gen_helper_fsave(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2f: /* fnstsw mem */\n                gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x3c: /* fbld */\n                gen_helper_fbld_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x3e: /* fbstp */\n                gen_helper_fbst_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3d: /* fildll */\n                tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fildll_ST0(cpu_env, cpu_tmp1_i64);\n                break;\n            case 0x3f: /* fistpll */\n                gen_helper_fistll_ST0(cpu_tmp1_i64, cpu_env);\n                tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fpop(cpu_env);\n                break;\n            default:\n                goto unknown_op;\n            }\n        } else {\n            /* register float ops */\n            opreg = rm;\n\n            switch(op) {\n            case 0x08: /* fld sti */\n                gen_helper_fpush(cpu_env);\n                gen_helper_fmov_ST0_STN(cpu_env,\n                                        tcg_const_i32((opreg + 1) & 7));\n                break;\n            case 0x09: /* fxchg sti */\n            case 0x29: /* fxchg4 sti, undocumented op */\n            case 0x39: /* fxchg7 sti, undocumented op */\n                gen_helper_fxchg_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x0a: /* grp d9/2 */\n                switch(rm) {\n                case 0: /* fnop */\n                    /* check exceptions (FreeBSD FPU probe) */\n                    gen_helper_fwait(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0c: /* grp d9/4 */\n                switch(rm) {\n                case 0: /* fchs */\n                    gen_helper_fchs_ST0(cpu_env);\n                    break;\n                case 1: /* fabs */\n                    gen_helper_fabs_ST0(cpu_env);\n                    break;\n                case 4: /* ftst */\n                    gen_helper_fldz_FT0(cpu_env);\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    break;\n                case 5: /* fxam */\n                    gen_helper_fxam_ST0(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0d: /* grp d9/5 */\n                {\n                    switch(rm) {\n                    case 0:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fld1_ST0(cpu_env);\n                        break;\n                    case 1:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2t_ST0(cpu_env);\n                        break;\n                    case 2:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2e_ST0(cpu_env);\n                        break;\n                    case 3:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldpi_ST0(cpu_env);\n                        break;\n                    case 4:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldlg2_ST0(cpu_env);\n                        break;\n                    case 5:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldln2_ST0(cpu_env);\n                        break;\n                    case 6:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldz_ST0(cpu_env);\n                        break;\n                    default:\n                        goto unknown_op;\n                    }\n                }\n                break;\n            case 0x0e: /* grp d9/6 */\n                switch(rm) {\n                case 0: /* f2xm1 */\n                    gen_helper_f2xm1(cpu_env);\n                    break;\n                case 1: /* fyl2x */\n                    gen_helper_fyl2x(cpu_env);\n                    break;\n                case 2: /* fptan */\n                    gen_helper_fptan(cpu_env);\n                    break;\n                case 3: /* fpatan */\n                    gen_helper_fpatan(cpu_env);\n                    break;\n                case 4: /* fxtract */\n                    gen_helper_fxtract(cpu_env);\n                    break;\n                case 5: /* fprem1 */\n                    gen_helper_fprem1(cpu_env);\n                    break;\n                case 6: /* fdecstp */\n                    gen_helper_fdecstp(cpu_env);\n                    break;\n                default:\n                case 7: /* fincstp */\n                    gen_helper_fincstp(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0f: /* grp d9/7 */\n                switch(rm) {\n                case 0: /* fprem */\n                    gen_helper_fprem(cpu_env);\n                    break;\n                case 1: /* fyl2xp1 */\n                    gen_helper_fyl2xp1(cpu_env);\n                    break;\n                case 2: /* fsqrt */\n                    gen_helper_fsqrt(cpu_env);\n                    break;\n                case 3: /* fsincos */\n                    gen_helper_fsincos(cpu_env);\n                    break;\n                case 5: /* fscale */\n                    gen_helper_fscale(cpu_env);\n                    break;\n                case 4: /* frndint */\n                    gen_helper_frndint(cpu_env);\n                    break;\n                case 6: /* fsin */\n                    gen_helper_fsin(cpu_env);\n                    break;\n                default:\n                case 7: /* fcos */\n                    gen_helper_fcos(cpu_env);\n                    break;\n                }\n                break;\n            case 0x00: case 0x01: case 0x04 ... 0x07: /* fxxx st, sti */\n            case 0x20: case 0x21: case 0x24 ... 0x27: /* fxxx sti, st */\n            case 0x30: case 0x31: case 0x34 ... 0x37: /* fxxxp sti, st */\n                {\n                    int op1;\n\n                    op1 = op & 7;\n                    if (op >= 0x20) {\n                        gen_helper_fp_arith_STN_ST0(op1, opreg);\n                        if (op >= 0x30)\n                            gen_helper_fpop(cpu_env);\n                    } else {\n                        gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                        gen_helper_fp_arith_ST0_FT0(op1);\n                    }\n                }\n                break;\n            case 0x02: /* fcom */\n            case 0x22: /* fcom2, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                break;\n            case 0x03: /* fcomp */\n            case 0x23: /* fcomp3, undocumented op */\n            case 0x32: /* fcomp5, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x15: /* da/5 */\n                switch(rm) {\n                case 1: /* fucompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fucom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1c:\n                switch(rm) {\n                case 0: /* feni (287 only, just do nop here) */\n                    break;\n                case 1: /* fdisi (287 only, just do nop here) */\n                    break;\n                case 2: /* fclex */\n                    gen_helper_fclex(cpu_env);\n                    break;\n                case 3: /* fninit */\n                    gen_helper_fninit(cpu_env);\n                    break;\n                case 4: /* fsetpm (287 only, just do nop here) */\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1d: /* fucomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x1e: /* fcomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x28: /* ffree sti */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2a: /* fst sti */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2b: /* fstp sti */\n            case 0x0b: /* fstp1 sti, undocumented op */\n            case 0x3a: /* fstp8 sti, undocumented op */\n            case 0x3b: /* fstp9 sti, undocumented op */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* fucom st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                break;\n            case 0x2d: /* fucomp st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x33: /* de/3 */\n                switch(rm) {\n                case 1: /* fcompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x38: /* ffreep sti, undocumented op */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3c: /* df/4 */\n                switch(rm) {\n                case 0:\n                    gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                    tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n                    gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x3d: /* fucomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x3e: /* fcomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x10 ... 0x13: /* fcmovxx */\n            case 0x18 ... 0x1b:\n                {\n                    int op1;\n                    TCGLabel *l1;\n                    static const uint8_t fcmov_cc[8] = {\n                        (JCC_B << 1),\n                        (JCC_Z << 1),\n                        (JCC_BE << 1),\n                        (JCC_P << 1),\n                    };\n\n                    if (!(s->cpuid_features & CPUID_CMOV)) {\n                        goto illegal_op;\n                    }\n                    op1 = fcmov_cc[op & 3] | (((op >> 3) & 1) ^ 1);\n                    l1 = gen_new_label();\n                    gen_jcc1_noeob(s, op1, l1);\n                    gen_helper_fmov_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                    gen_set_label(l1);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n        /************************/\n        /* string ops */\n\n    case 0xa4: /* movsS */\n    case 0xa5:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_movs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_movs(s, ot);\n        }\n        break;\n\n    case 0xaa: /* stosS */\n    case 0xab:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_stos(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_stos(s, ot);\n        }\n        break;\n    case 0xac: /* lodsS */\n    case 0xad:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_lods(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_lods(s, ot);\n        }\n        break;\n    case 0xae: /* scasS */\n    case 0xaf:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_scas(s, ot);\n        }\n        break;\n\n    case 0xa6: /* cmpsS */\n    case 0xa7:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_cmps(s, ot);\n        }\n        break;\n    case 0x6c: /* insS */\n    case 0x6d:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base, \n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_ins(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_ins(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n    case 0x6e: /* outsS */\n    case 0x6f:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_outs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_outs(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n\n        /************************/\n        /* port I/O */\n\n    case 0xe4:\n    case 0xe5:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xe6:\n    case 0xe7:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xec:\n    case 0xed:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xee:\n    case 0xef:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n\n        /************************/\n        /* control */\n    case 0xc2: /* ret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n        ot = gen_pop_T0(s);\n        gen_stack_update(s, val + (1 << ot));\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xc3: /* ret */\n        ot = gen_pop_T0(s);\n        gen_pop_update(s, ot);\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xca: /* lret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n    do_lret:\n        if (s->pe && !s->vm86) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_lret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(val));\n        } else {\n            gen_stack_A0(s);\n            /* pop offset */\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            /* NOTE: keeping EIP updated is not a problem in case of\n               exception */\n            gen_op_jmp_v(cpu_T0);\n            /* pop selector */\n            gen_add_A0_im(s, 1 << dflag);\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            gen_op_movl_seg_T0_vm(R_CS);\n            /* add stack offset */\n            gen_stack_update(s, val + (2 << dflag));\n        }\n        gen_eob(s);\n        break;\n    case 0xcb: /* lret */\n        val = 0;\n        goto do_lret;\n    case 0xcf: /* iret */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_IRET);\n        if (!s->pe) {\n            /* real mode */\n            gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n            set_cc_op(s, CC_OP_EFLAGS);\n        } else if (s->vm86) {\n            if (s->iopl != 3) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n        } else {\n            gen_helper_iret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            set_cc_op(s, CC_OP_EFLAGS);\n        }\n        gen_eob(s);\n        break;\n    case 0xe8: /* call im */\n        {\n            if (dflag != MO_16) {\n                tval = (int32_t)insn_get(env, s, MO_32);\n            } else {\n                tval = (int16_t)insn_get(env, s, MO_16);\n            }\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            } else if (!CODE64(s)) {\n                tval &= 0xffffffff;\n            }\n            tcg_gen_movi_tl(cpu_T0, next_eip);\n            gen_push_v(s, cpu_T0);\n            gen_bnd_jmp(s);\n            gen_jmp(s, tval);\n        }\n        break;\n    case 0x9a: /* lcall im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_lcall;\n    case 0xe9: /* jmp im */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        } else if (!CODE64(s)) {\n            tval &= 0xffffffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jmp(s, tval);\n        break;\n    case 0xea: /* ljmp im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_ljmp;\n    case 0xeb: /* jmp Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_jmp(s, tval);\n        break;\n    case 0x70 ... 0x7f: /* jcc Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        goto do_jcc;\n    case 0x180 ... 0x18f: /* jcc Jv */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n    do_jcc:\n        next_eip = s->pc - s->cs_base;\n        tval += next_eip;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jcc(s, b, tval, next_eip);\n        break;\n\n    case 0x190 ... 0x19f: /* setcc Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_setcc1(s, b, cpu_T0);\n        gen_ldst_modrm(env, s, modrm, MO_8, OR_TMP0, 1);\n        break;\n    case 0x140 ... 0x14f: /* cmov Gv, Ev */\n        if (!(s->cpuid_features & CPUID_CMOV)) {\n            goto illegal_op;\n        }\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_cmovcc1(env, s, ot, b, modrm, reg);\n        break;\n\n        /************************/\n        /* flags */\n    case 0x9c: /* pushf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_PUSHF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_helper_read_eflags(cpu_T0, cpu_env);\n            gen_push_v(s, cpu_T0);\n        }\n        break;\n    case 0x9d: /* popf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_POPF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            ot = gen_pop_T0(s);\n            if (s->cpl == 0) {\n                if (dflag != MO_16) {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK |\n                                                           IOPL_MASK)));\n                } else {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK | IOPL_MASK)\n                                                          & 0xffff));\n                }\n            } else {\n                if (s->cpl <= s->iopl) {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)\n                                                              & 0xffff));\n                    }\n                } else {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)\n                                                         & 0xffff));\n                    }\n                }\n            }\n            gen_pop_update(s, ot);\n            set_cc_op(s, CC_OP_EFLAGS);\n            /* abort translation because TF/AC flag may change */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    case 0x9e: /* sahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_op_mov_v_reg(MO_8, cpu_T0, R_AH);\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, CC_O);\n        tcg_gen_andi_tl(cpu_T0, cpu_T0, CC_S | CC_Z | CC_A | CC_P | CC_C);\n        tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, cpu_T0);\n        break;\n    case 0x9f: /* lahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_compute_eflags(s);\n        /* Note: gen_compute_eflags() only gives the condition codes */\n        tcg_gen_ori_tl(cpu_T0, cpu_cc_src, 0x02);\n        gen_op_mov_reg_v(MO_8, R_AH, cpu_T0);\n        break;\n    case 0xf5: /* cmc */\n        gen_compute_eflags(s);\n        tcg_gen_xori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xf8: /* clc */\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_C);\n        break;\n    case 0xf9: /* stc */\n        gen_compute_eflags(s);\n        tcg_gen_ori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xfc: /* cld */\n        tcg_gen_movi_i32(cpu_tmp2_i32, 1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n    case 0xfd: /* std */\n        tcg_gen_movi_i32(cpu_tmp2_i32, -1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n\n        /************************/\n        /* bit operations */\n    case 0x1ba: /* bt/bts/btr/btc Gv, im */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        op = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        if (mod != 3) {\n            s->rip_offset = 1;\n            gen_lea_modrm(env, s, modrm);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n        /* load shift */\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T1, val);\n        if (op < 4)\n            goto unknown_op;\n        op -= 4;\n        goto bt_op;\n    case 0x1a3: /* bt Gv, Ev */\n        op = 0;\n        goto do_btx;\n    case 0x1ab: /* bts */\n        op = 1;\n        goto do_btx;\n    case 0x1b3: /* btr */\n        op = 2;\n        goto do_btx;\n    case 0x1bb: /* btc */\n        op = 3;\n    do_btx:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        gen_op_mov_v_reg(MO_32, cpu_T1, reg);\n        if (mod != 3) {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            /* specific case: we need to add a displacement */\n            gen_exts(ot, cpu_T1);\n            tcg_gen_sari_tl(cpu_tmp0, cpu_T1, 3 + ot);\n            tcg_gen_shli_tl(cpu_tmp0, cpu_tmp0, ot);\n            tcg_gen_add_tl(cpu_A0, gen_lea_modrm_1(a), cpu_tmp0);\n            gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n    bt_op:\n        tcg_gen_andi_tl(cpu_T1, cpu_T1, (1 << (3 + ot)) - 1);\n        tcg_gen_movi_tl(cpu_tmp0, 1);\n        tcg_gen_shl_tl(cpu_tmp0, cpu_tmp0, cpu_T1);\n        if (s->prefix & PREFIX_LOCK) {\n            switch (op) {\n            case 0: /* bt */\n                /* Needs no atomic ops; we surpressed the normal\n                   memory load for LOCK above so do it now.  */\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                break;\n            case 1: /* bts */\n                tcg_gen_atomic_fetch_or_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                           s->mem_index, ot | MO_LE);\n                break;\n            case 2: /* btr */\n                tcg_gen_not_tl(cpu_tmp0, cpu_tmp0);\n                tcg_gen_atomic_fetch_and_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_atomic_fetch_xor_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            }\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n        } else {\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n            switch (op) {\n            case 0: /* bt */\n                /* Data already loaded; nothing to do.  */\n                break;\n            case 1: /* bts */\n                tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            case 2: /* btr */\n                tcg_gen_andc_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            }\n            if (op != 0) {\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n        }\n\n        /* Delay all CC updates until after the store above.  Note that\n           C is the result of the test, Z is unchanged, and the others\n           are all undefined.  */\n        switch (s->cc_op) {\n        case CC_OP_MULB ... CC_OP_MULQ:\n        case CC_OP_ADDB ... CC_OP_ADDQ:\n        case CC_OP_ADCB ... CC_OP_ADCQ:\n        case CC_OP_SUBB ... CC_OP_SUBQ:\n        case CC_OP_SBBB ... CC_OP_SBBQ:\n        case CC_OP_LOGICB ... CC_OP_LOGICQ:\n        case CC_OP_INCB ... CC_OP_INCQ:\n        case CC_OP_DECB ... CC_OP_DECQ:\n        case CC_OP_SHLB ... CC_OP_SHLQ:\n        case CC_OP_SARB ... CC_OP_SARQ:\n        case CC_OP_BMILGB ... CC_OP_BMILGQ:\n            /* Z was going to be computed from the non-zero status of CC_DST.\n               We can get that same Z value (and the new C value) by leaving\n               CC_DST alone, setting CC_SRC, and using a CC_OP_SAR of the\n               same width.  */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_tmp4);\n            set_cc_op(s, ((s->cc_op - CC_OP_MULB) & 3) + CC_OP_SARB);\n            break;\n        default:\n            /* Otherwise, generate EFLAGS and replace the C bit.  */\n            gen_compute_eflags(s);\n            tcg_gen_deposit_tl(cpu_cc_src, cpu_cc_src, cpu_tmp4,\n                               ctz32(CC_C), 1);\n            break;\n        }\n        break;\n    case 0x1bc: /* bsf / tzcnt */\n    case 0x1bd: /* bsr / lzcnt */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n\n        /* Note that lzcnt and tzcnt are in different extensions.  */\n        if ((prefixes & PREFIX_REPZ)\n            && (b & 1\n                ? s->cpuid_ext3_features & CPUID_EXT3_ABM\n                : s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)) {\n            int size = 8 << ot;\n            /* For lzcnt/tzcnt, C bit is defined related to the input. */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n            if (b & 1) {\n                /* For lzcnt, reduce the target_ulong result by the\n                   number of zeros that we expect to find at the top.  */\n                tcg_gen_clzi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS);\n                tcg_gen_subi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - size);\n            } else {\n                /* For tzcnt, a zero input must return the operand size.  */\n                tcg_gen_ctzi_tl(cpu_T0, cpu_T0, size);\n            }\n            /* For lzcnt/tzcnt, Z bit is defined related to the result.  */\n            gen_op_update1_cc();\n            set_cc_op(s, CC_OP_BMILGB + ot);\n        } else {\n            /* For bsr/bsf, only the Z bit is defined and it is related\n               to the input and not the result.  */\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            set_cc_op(s, CC_OP_LOGICB + ot);\n\n            /* ??? The manual says that the output is undefined when the\n               input is zero, but real hardware leaves it unchanged, and\n               real programs appear to depend on that.  Accomplish this\n               by passing the output as the value to return upon zero.  */\n            if (b & 1) {\n                /* For bsr, return the bit index of the first 1 bit,\n                   not the count of leading zeros.  */\n                tcg_gen_xori_tl(cpu_T1, cpu_regs[reg], TARGET_LONG_BITS - 1);\n                tcg_gen_clz_tl(cpu_T0, cpu_T0, cpu_T1);\n                tcg_gen_xori_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - 1);\n            } else {\n                tcg_gen_ctz_tl(cpu_T0, cpu_T0, cpu_regs[reg]);\n            }\n        }\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n        /************************/\n        /* bcd */\n    case 0x27: /* daa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_daa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x2f: /* das */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_das(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x37: /* aaa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aaa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x3f: /* aas */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aas(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0xd4: /* aam */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        if (val == 0) {\n            gen_exception(s, EXCP00_DIVZ, pc_start - s->cs_base);\n        } else {\n            gen_helper_aam(cpu_env, tcg_const_i32(val));\n            set_cc_op(s, CC_OP_LOGICB);\n        }\n        break;\n    case 0xd5: /* aad */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        gen_helper_aad(cpu_env, tcg_const_i32(val));\n        set_cc_op(s, CC_OP_LOGICB);\n        break;\n        /************************/\n        /* misc */\n    case 0x90: /* nop */\n        /* XXX: correct lock test for all insn */\n        if (prefixes & PREFIX_LOCK) {\n            goto illegal_op;\n        }\n        /* If REX_B is set, then this is xchg eax, r8d, not a nop.  */\n        if (REX_B(s)) {\n            goto do_xchg_reg_eax;\n        }\n        if (prefixes & PREFIX_REPZ) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_pause(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x9b: /* fwait */\n        if ((s->flags & (HF_MP_MASK | HF_TS_MASK)) ==\n            (HF_MP_MASK | HF_TS_MASK)) {\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n        } else {\n            gen_helper_fwait(cpu_env);\n        }\n        break;\n    case 0xcc: /* int3 */\n        gen_interrupt(s, EXCP03_INT3, pc_start - s->cs_base, s->pc - s->cs_base);\n        break;\n    case 0xcd: /* int N */\n        val = cpu_ldub_code(env, s->pc++);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_interrupt(s, val, pc_start - s->cs_base, s->pc - s->cs_base);\n        }\n        break;\n    case 0xce: /* into */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_into(cpu_env, tcg_const_i32(s->pc - pc_start));\n        break;\n#ifdef WANT_ICEBP\n    case 0xf1: /* icebp (undocumented, exits to external debugger) */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_ICEBP);\n#if 1\n        gen_debug(s, pc_start - s->cs_base);\n#else\n        /* start debug */\n        tb_flush(CPU(x86_env_get_cpu(env)));\n        qemu_set_log(CPU_LOG_INT | CPU_LOG_TB_IN_ASM);\n#endif\n        break;\n#endif\n    case 0xfa: /* cli */\n        if (!s->vm86) {\n            if (s->cpl <= s->iopl) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        } else {\n            if (s->iopl == 3) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        }\n        break;\n    case 0xfb: /* sti */\n        if (s->vm86 ? s->iopl == 3 : s->cpl <= s->iopl) {\n            gen_helper_sti(cpu_env);\n            /* interruptions are enabled only the first insn after sti */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob_inhibit_irq(s, true);\n        } else {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        }\n        break;\n    case 0x62: /* bound */\n        if (CODE64(s))\n            goto illegal_op;\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        gen_lea_modrm(env, s, modrm);\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        if (ot == MO_16) {\n            gen_helper_boundw(cpu_env, cpu_A0, cpu_tmp2_i32);\n        } else {\n            gen_helper_boundl(cpu_env, cpu_A0, cpu_tmp2_i32);\n        }\n        break;\n    case 0x1c8 ... 0x1cf: /* bswap reg */\n        reg = (b & 7) | REX_B(s);\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            gen_op_mov_v_reg(MO_64, cpu_T0, reg);\n            tcg_gen_bswap64_i64(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            gen_op_mov_v_reg(MO_32, cpu_T0, reg);\n            tcg_gen_ext32u_tl(cpu_T0, cpu_T0);\n            tcg_gen_bswap32_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, reg, cpu_T0);\n        }\n        break;\n    case 0xd6: /* salc */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_compute_eflags_c(s, cpu_T0);\n        tcg_gen_neg_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xe0: /* loopnz */\n    case 0xe1: /* loopz */\n    case 0xe2: /* loop */\n    case 0xe3: /* jecxz */\n        {\n            TCGLabel *l1, *l2, *l3;\n\n            tval = (int8_t)insn_get(env, s, MO_8);\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            }\n\n            l1 = gen_new_label();\n            l2 = gen_new_label();\n            l3 = gen_new_label();\n            b &= 3;\n            switch(b) {\n            case 0: /* loopnz */\n            case 1: /* loopz */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jz_ecx(s->aflag, l3);\n                gen_jcc1(s, (JCC_Z << 1) | (b ^ 1), l1);\n                break;\n            case 2: /* loop */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jnz_ecx(s->aflag, l1);\n                break;\n            default:\n            case 3: /* jcxz */\n                gen_op_jz_ecx(s->aflag, l1);\n                break;\n            }\n\n            gen_set_label(l3);\n            gen_jmp_im(next_eip);\n            tcg_gen_br(l2);\n\n            gen_set_label(l1);\n            gen_jmp_im(tval);\n            gen_set_label(l2);\n            gen_eob(s);\n        }\n        break;\n    case 0x130: /* wrmsr */\n    case 0x132: /* rdmsr */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (b & 2) {\n                gen_helper_rdmsr(cpu_env);\n            } else {\n                gen_helper_wrmsr(cpu_env);\n            }\n        }\n        break;\n    case 0x131: /* rdtsc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        gen_helper_rdtsc(cpu_env);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0x133: /* rdpmc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_rdpmc(cpu_env);\n        break;\n    case 0x134: /* sysenter */\n        /* For Intel SYSENTER is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysenter(cpu_env);\n            gen_eob(s);\n        }\n        break;\n    case 0x135: /* sysexit */\n        /* For Intel SYSEXIT is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysexit(cpu_env, tcg_const_i32(dflag - 1));\n            gen_eob(s);\n        }\n        break;\n#ifdef TARGET_X86_64\n    case 0x105: /* syscall */\n        /* XXX: is it usable in real mode ? */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_syscall(cpu_env, tcg_const_i32(s->pc - pc_start));\n        /* TF handling for the syscall insn is different. The TF bit is  checked\n           after the syscall insn completes. This allows #DB to not be\n           generated after one has entered CPL0 if TF is set in FMASK.  */\n        gen_eob_worker(s, false, true);\n        break;\n    case 0x107: /* sysret */\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysret(cpu_env, tcg_const_i32(dflag - 1));\n            /* condition codes are modified only in long mode */\n            if (s->lma) {\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n            /* TF handling for the sysret insn is different. The TF bit is\n               checked after the sysret insn completes. This allows #DB to be\n               generated \"as if\" the syscall insn in userspace has just\n               completed.  */\n            gen_eob_worker(s, false, true);\n        }\n        break;\n#endif\n    case 0x1a2: /* cpuid */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_cpuid(cpu_env);\n        break;\n    case 0xf4: /* hlt */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_hlt(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x100:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* sldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, ldt.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 2: /* lldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lldt(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 1: /* str */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, tr.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 3: /* ltr */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ltr(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 4: /* verr */\n        case 5: /* verw */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_update_cc_op(s);\n            if (op == 4) {\n                gen_helper_verr(cpu_env, cpu_T0);\n            } else {\n                gen_helper_verw(cpu_env, cpu_T0);\n            }\n            set_cc_op(s, CC_OP_EFLAGS);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x101:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* sgdt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0,\n                             cpu_env, offsetof(CPUX86State, gdt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xc8: /* monitor */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EAX]);\n            gen_extu(s->aflag, cpu_A0);\n            gen_add_A0_ds_seg(s);\n            gen_helper_monitor(cpu_env, cpu_A0);\n            break;\n\n        case 0xc9: /* mwait */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_mwait(cpu_env, tcg_const_i32(s->pc - pc_start));\n            gen_eob(s);\n            break;\n\n        case 0xca: /* clac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_clac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xcb: /* stac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_stac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* sidt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xd0: /* xgetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xgetbv(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n\n        case 0xd1: /* xsetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xsetbv(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            /* End TB because translation flags may change.  */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xd8: /* VMRUN */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmrun(cpu_env, tcg_const_i32(s->aflag - 1),\n                             tcg_const_i32(s->pc - pc_start));\n            tcg_gen_exit_tb(0);\n            s->is_jmp = DISAS_TB_JUMP;\n            break;\n\n        case 0xd9: /* VMMCALL */\n            if (!(s->flags & HF_SVME_MASK)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmmcall(cpu_env);\n            break;\n\n        case 0xda: /* VMLOAD */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmload(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdb: /* VMSAVE */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmsave(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdc: /* STGI */\n            if ((!(s->flags & HF_SVME_MASK)\n                   && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_stgi(cpu_env);\n            break;\n\n        case 0xdd: /* CLGI */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_clgi(cpu_env);\n            break;\n\n        case 0xde: /* SKINIT */\n            if ((!(s->flags & HF_SVME_MASK)\n                 && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_skinit(cpu_env);\n            break;\n\n        case 0xdf: /* INVLPGA */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_invlpga(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* lgdt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, gdt.limit));\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* lidt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, idt.limit));\n            break;\n\n        CASE_MODRM_OP(4): /* smsw */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_CR0);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, cr[0]));\n            if (CODE64(s)) {\n                mod = (modrm >> 6) & 3;\n                ot = (mod != 3 ? MO_16 : s->dflag);\n            } else {\n                ot = MO_16;\n            }\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 0xee: /* rdpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_rdpkru(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n        case 0xef: /* wrpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_wrpkru(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            break;\n        CASE_MODRM_OP(6): /* lmsw */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_helper_lmsw(cpu_env, cpu_T0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* invlpg */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_invlpg(cpu_env, cpu_A0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xf8: /* swapgs */\n#ifdef TARGET_X86_64\n            if (CODE64(s)) {\n                if (s->cpl != 0) {\n                    gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                } else {\n                    tcg_gen_mov_tl(cpu_T0, cpu_seg_base[R_GS]);\n                    tcg_gen_ld_tl(cpu_seg_base[R_GS], cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                    tcg_gen_st_tl(cpu_T0, cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                }\n                break;\n            }\n#endif\n            goto illegal_op;\n\n        case 0xf9: /* rdtscp */\n            if (!(s->cpuid_ext2_features & CPUID_EXT2_RDTSCP)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_start();\n            }\n            gen_helper_rdtscp(cpu_env);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_end();\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x108: /* invd */\n    case 0x109: /* wbinvd */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, (b & 2) ? SVM_EXIT_INVD : SVM_EXIT_WBINVD);\n            /* nothing to do */\n        }\n        break;\n    case 0x63: /* arpl or movslS (x86_64) */\n#ifdef TARGET_X86_64\n        if (CODE64(s)) {\n            int d_ot;\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                gen_op_mov_v_reg(MO_32, cpu_T0, rm);\n                /* sign extend */\n                if (d_ot == MO_64) {\n                    tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, MO_32 | MO_SIGN, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        } else\n#endif\n        {\n            TCGLabel *label1;\n            TCGv t0, t1, t2, a0;\n\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            t0 = tcg_temp_local_new();\n            t1 = tcg_temp_local_new();\n            t2 = tcg_temp_local_new();\n            ot = MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = (modrm >> 3) & 7;\n            mod = (modrm >> 6) & 3;\n            rm = modrm & 7;\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, ot, t0, cpu_A0);\n                a0 = tcg_temp_local_new();\n                tcg_gen_mov_tl(a0, cpu_A0);\n            } else {\n                gen_op_mov_v_reg(ot, t0, rm);\n                TCGV_UNUSED(a0);\n            }\n            gen_op_mov_v_reg(ot, t1, reg);\n            tcg_gen_andi_tl(cpu_tmp0, t0, 3);\n            tcg_gen_andi_tl(t1, t1, 3);\n            tcg_gen_movi_tl(t2, 0);\n            label1 = gen_new_label();\n            tcg_gen_brcond_tl(TCG_COND_GE, cpu_tmp0, t1, label1);\n            tcg_gen_andi_tl(t0, t0, ~3);\n            tcg_gen_or_tl(t0, t0, t1);\n            tcg_gen_movi_tl(t2, CC_Z);\n            gen_set_label(label1);\n            if (mod != 3) {\n                gen_op_st_v(s, ot, t0, a0);\n                tcg_temp_free(a0);\n           } else {\n                gen_op_mov_reg_v(ot, rm, t0);\n            }\n            gen_compute_eflags(s);\n            tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_Z);\n            tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, t2);\n            tcg_temp_free(t0);\n            tcg_temp_free(t1);\n            tcg_temp_free(t2);\n        }\n        break;\n    case 0x102: /* lar */\n    case 0x103: /* lsl */\n        {\n            TCGLabel *label1;\n            TCGv t0;\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            ot = dflag != MO_16 ? MO_32 : MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            t0 = tcg_temp_local_new();\n            gen_update_cc_op(s);\n            if (b == 0x102) {\n                gen_helper_lar(t0, cpu_env, cpu_T0);\n            } else {\n                gen_helper_lsl(t0, cpu_env, cpu_T0);\n            }\n            tcg_gen_andi_tl(cpu_tmp0, cpu_cc_src, CC_Z);\n            label1 = gen_new_label();\n            tcg_gen_brcondi_tl(TCG_COND_EQ, cpu_tmp0, 0, label1);\n            gen_op_mov_reg_v(ot, reg, t0);\n            gen_set_label(label1);\n            set_cc_op(s, CC_OP_EFLAGS);\n            tcg_temp_free(t0);\n        }\n        break;\n    case 0x118:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* prefetchnta */\n        case 1: /* prefetchnt0 */\n        case 2: /* prefetchnt0 */\n        case 3: /* prefetchnt0 */\n            if (mod == 3)\n                goto illegal_op;\n            gen_nop_modrm(env, s, modrm);\n            /* nothing more to do */\n            break;\n        default: /* nop (multi byte) */\n            gen_nop_modrm(env, s, modrm);\n            break;\n        }\n        break;\n    case 0x11a:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (prefixes & PREFIX_REPZ) {\n                /* bndcl */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_LTU, cpu_bndl[reg]);\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcu */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                TCGv_i64 notu = tcg_temp_new_i64();\n                tcg_gen_not_i64(notu, cpu_bndu[reg]);\n                gen_bndck(env, s, modrm, TCG_COND_GTU, notu);\n                tcg_temp_free_i64(notu);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- from reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg], cpu_bndl[reg2]);\n                        tcg_gen_mov_i64(cpu_bndu[reg], cpu_bndu[reg2]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                    /* bnd registers are now in-use */\n                    gen_set_hflag(s, HF_MPX_IU_MASK);\n                }\n            } else if (mod != 3) {\n                /* bndldx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndldx64(cpu_bndl[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ld_i64(cpu_bndu[reg], cpu_env,\n                                   offsetof(CPUX86State, mmx_t0.MMX_Q(0)));\n                } else {\n                    gen_helper_bndldx32(cpu_bndu[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndu[reg]);\n                    tcg_gen_shri_i64(cpu_bndu[reg], cpu_bndu[reg], 32);\n                }\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x11b:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (mod != 3 && (prefixes & PREFIX_REPZ)) {\n                /* bndmk */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (a.base >= 0) {\n                    tcg_gen_extu_tl_i64(cpu_bndl[reg], cpu_regs[a.base]);\n                    if (!CODE64(s)) {\n                        tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndl[reg]);\n                    }\n                } else if (a.base == -1) {\n                    /* no base register has lower bound of 0 */\n                    tcg_gen_movi_i64(cpu_bndl[reg], 0);\n                } else {\n                    /* rip-relative generates #ud */\n                    goto illegal_op;\n                }\n                tcg_gen_not_tl(cpu_A0, gen_lea_modrm_1(a));\n                if (!CODE64(s)) {\n                    tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n                }\n                tcg_gen_extu_tl_i64(cpu_bndu[reg], cpu_A0);\n                /* bnd registers are now in-use */\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n                break;\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcn */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_GTU, cpu_bndu[reg]);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- to reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg2], cpu_bndl[reg]);\n                        tcg_gen_mov_i64(cpu_bndu[reg2], cpu_bndu[reg]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                }\n            } else if (mod != 3) {\n                /* bndstx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndstx64(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                } else {\n                    gen_helper_bndstx32(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                }\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x119: case 0x11c ... 0x11f: /* nop (multi byte) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x120: /* mov reg, crN */\n    case 0x122: /* mov crN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if ((prefixes & PREFIX_LOCK) && (reg == 0) &&\n                (s->cpuid_ext3_features & CPUID_EXT3_CR8LEG)) {\n                reg = 8;\n            }\n            switch(reg) {\n            case 0:\n            case 2:\n            case 3:\n            case 4:\n            case 8:\n                gen_update_cc_op(s);\n                gen_jmp_im(pc_start - s->cs_base);\n                if (b & 2) {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    gen_helper_write_crN(cpu_env, tcg_const_i32(reg),\n                                         cpu_T0);\n                    gen_jmp_im(s->pc - s->cs_base);\n                    gen_eob(s);\n                } else {\n                    gen_helper_read_crN(cpu_T0, cpu_env, tcg_const_i32(reg));\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n    case 0x121: /* mov reg, drN */\n    case 0x123: /* mov drN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if (reg >= 8) {\n                goto illegal_op;\n            }\n            if (b & 2) {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_DR0 + reg);\n                gen_op_mov_v_reg(ot, cpu_T0, rm);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_set_dr(cpu_env, cpu_tmp2_i32, cpu_T0);\n                gen_jmp_im(s->pc - s->cs_base);\n                gen_eob(s);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_DR0 + reg);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_get_dr(cpu_T0, cpu_env, cpu_tmp2_i32);\n                gen_op_mov_reg_v(ot, rm, cpu_T0);\n            }\n        }\n        break;\n    case 0x106: /* clts */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_helper_clts(cpu_env);\n            /* abort block because static cpu state changed */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    /* MMX/3DNow!/SSE/SSE2/SSE3/SSSE3/SSE4 support */\n    case 0x1c3: /* MOVNTI reg, mem */\n        if (!(s->cpuid_features & CPUID_SSE2))\n            goto illegal_op;\n        ot = mo_64_32(dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0x1ae:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* fxsave */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxsave(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* fxrstor */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxrstor(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* ldmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0, s->mem_index, MO_LEUL);\n            gen_helper_ldmxcsr(cpu_env, cpu_tmp2_i32);\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* stmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, mxcsr));\n            gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(4): /* xsave */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xsave(cpu_env, cpu_A0, cpu_tmp1_i64);\n            break;\n\n        CASE_MODRM_MEM_OP(5): /* xrstor */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xrstor(cpu_env, cpu_A0, cpu_tmp1_i64);\n            /* XRSTOR is how MPX is enabled, which changes how\n               we translate.  Thus we need to end the TB.  */\n            gen_update_cc_op(s);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(6): /* xsaveopt / clwb */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clwb */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLWB)) {\n                    goto illegal_op;\n                }\n                gen_nop_modrm(env, s, modrm);\n            } else {\n                /* xsaveopt */\n                if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                    || (s->cpuid_xsave_features & CPUID_XSAVE_XSAVEOPT) == 0\n                    || (prefixes & (PREFIX_REPZ | PREFIX_REPNZ))) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                      cpu_regs[R_EDX]);\n                gen_helper_xsaveopt(cpu_env, cpu_A0, cpu_tmp1_i64);\n            }\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* clflush / clflushopt */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clflushopt */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLFLUSHOPT)) {\n                    goto illegal_op;\n                }\n            } else {\n                /* clflush */\n                if ((s->prefix & (PREFIX_REPZ | PREFIX_REPNZ))\n                    || !(s->cpuid_features & CPUID_CLFLUSH)) {\n                    goto illegal_op;\n                }\n            }\n            gen_nop_modrm(env, s, modrm);\n            break;\n\n        case 0xc0 ... 0xc7: /* rdfsbase (f3 0f ae /0) */\n        case 0xc8 ... 0xc8: /* rdgsbase (f3 0f ae /1) */\n        case 0xd0 ... 0xd7: /* wrfsbase (f3 0f ae /2) */\n        case 0xd8 ... 0xd8: /* wrgsbase (f3 0f ae /3) */\n            if (CODE64(s)\n                && (prefixes & PREFIX_REPZ)\n                && !(prefixes & PREFIX_LOCK)\n                && (s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_FSGSBASE)) {\n                TCGv base, treg, src, dst;\n\n                /* Preserve hflags bits by testing CR4 at runtime.  */\n                tcg_gen_movi_i32(cpu_tmp2_i32, CR4_FSGSBASE_MASK);\n                gen_helper_cr4_testbit(cpu_env, cpu_tmp2_i32);\n\n                base = cpu_seg_base[modrm & 8 ? R_GS : R_FS];\n                treg = cpu_regs[(modrm & 7) | REX_B(s)];\n\n                if (modrm & 0x10) {\n                    /* wr*base */\n                    dst = base, src = treg;\n                } else {\n                    /* rd*base */\n                    dst = treg, src = base;\n                }\n\n                if (s->dflag == MO_32) {\n                    tcg_gen_ext32u_tl(dst, src);\n                } else {\n                    tcg_gen_mov_tl(dst, src);\n                }\n                break;\n            }\n            goto unknown_op;\n\n        case 0xf8: /* sfence / pcommit */\n            if (prefixes & PREFIX_DATA) {\n                /* pcommit */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_PCOMMIT)\n                    || (prefixes & PREFIX_LOCK)) {\n                    goto illegal_op;\n                }\n                break;\n            }\n            /* fallthru */\n        case 0xf9 ... 0xff: /* sfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ST_ST | TCG_BAR_SC);\n            break;\n        case 0xe8 ... 0xef: /* lfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_LD_LD | TCG_BAR_SC);\n            break;\n        case 0xf0 ... 0xf7: /* mfence */\n            if (!(s->cpuid_features & CPUID_SSE2)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ALL | TCG_BAR_SC);\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x10d: /* 3DNow! prefetch(w) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x1aa: /* rsm */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_RSM);\n        if (!(s->flags & HF_SMM_MASK))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(s->pc - s->cs_base);\n        gen_helper_rsm(cpu_env);\n        gen_eob(s);\n        break;\n    case 0x1b8: /* SSE4.2 popcnt */\n        if ((prefixes & (PREFIX_REPZ | PREFIX_LOCK | PREFIX_REPNZ)) !=\n             PREFIX_REPZ)\n            goto illegal_op;\n        if (!(s->cpuid_ext_features & CPUID_EXT_POPCNT))\n            goto illegal_op;\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        if (s->prefix & PREFIX_DATA) {\n            ot = MO_16;\n        } else {\n            ot = mo_64_32(dflag);\n        }\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n        tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n        tcg_gen_ctpop_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n\n        set_cc_op(s, CC_OP_POPCNT);\n        break;\n    case 0x10e ... 0x10f:\n        /* 3DNow! instructions, ignore prefixes */\n        s->prefix &= ~(PREFIX_REPZ | PREFIX_REPNZ | PREFIX_DATA);\n    case 0x110 ... 0x117:\n    case 0x128 ... 0x12f:\n    case 0x138 ... 0x13a:\n    case 0x150 ... 0x179:\n    case 0x17c ... 0x17f:\n    case 0x1c2:\n    case 0x1c4 ... 0x1c6:\n    case 0x1d0 ... 0x1fe:\n        gen_sse(env, s, b, pc_start, rex_r);\n        break;\n    default:\n        goto unknown_op;\n    }\n    return s->pc;\n illegal_op:\n    gen_illegal_opcode(s);\n    return s->pc;\n unknown_op:\n    gen_unknown_opcode(env, s);\n    return s->pc;\n}\n",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 144425,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static target_ulong disas_insn(CPUX86State *env, DisasContext *s,\n                               target_ulong pc_start)\n{\n    int b, prefixes;\n    int shift;\n    TCGMemOp ot, aflag, dflag;\n    int modrm, reg, rm, mod, op, opreg, val;\n    target_ulong next_eip, tval;\n    int rex_w, rex_r;\n\n    s->pc_start = s->pc = pc_start;\n    prefixes = 0;\n    s->override = -1;\n    rex_w = -1;\n    rex_r = 0;\n#ifdef TARGET_X86_64\n    s->rex_x = 0;\n    s->rex_b = 0;\n    x86_64_hregs = 0;\n#endif\n    s->rip_offset = 0; /* for relative ip address */\n     s->vex_l = 0;\n     s->vex_v = 0;\n  next_byte:\n    /* x86 has an upper limit of 15 bytes for an instruction. Since we\n     * do not want to decode and generate IR for an illegal\n     * instruction, the following check limits the instruction size to\n     * 25 bytes: 14 prefix + 1 opc + 6 (modrm+sib+ofs) + 4 imm */\n    if (s->pc - pc_start > 14) {\n        goto illegal_op;\n    }\n     b = cpu_ldub_code(env, s->pc);\n     s->pc++;\n     /* Collect prefixes.  */\n    switch (b) {\n    case 0xf3:\n        prefixes |= PREFIX_REPZ;\n        goto next_byte;\n    case 0xf2:\n        prefixes |= PREFIX_REPNZ;\n        goto next_byte;\n    case 0xf0:\n        prefixes |= PREFIX_LOCK;\n        goto next_byte;\n    case 0x2e:\n        s->override = R_CS;\n        goto next_byte;\n    case 0x36:\n        s->override = R_SS;\n        goto next_byte;\n    case 0x3e:\n        s->override = R_DS;\n        goto next_byte;\n    case 0x26:\n        s->override = R_ES;\n        goto next_byte;\n    case 0x64:\n        s->override = R_FS;\n        goto next_byte;\n    case 0x65:\n        s->override = R_GS;\n        goto next_byte;\n    case 0x66:\n        prefixes |= PREFIX_DATA;\n        goto next_byte;\n    case 0x67:\n        prefixes |= PREFIX_ADR;\n        goto next_byte;\n#ifdef TARGET_X86_64\n    case 0x40 ... 0x4f:\n        if (CODE64(s)) {\n            /* REX prefix */\n            rex_w = (b >> 3) & 1;\n            rex_r = (b & 0x4) << 1;\n            s->rex_x = (b & 0x2) << 2;\n            REX_B(s) = (b & 0x1) << 3;\n            x86_64_hregs = 1; /* select uniform byte register addressing */\n            goto next_byte;\n        }\n        break;\n#endif\n    case 0xc5: /* 2-byte VEX */\n    case 0xc4: /* 3-byte VEX */\n        /* VEX prefixes cannot be used except in 32-bit mode.\n           Otherwise the instruction is LES or LDS.  */\n        if (s->code32 && !s->vm86) {\n            static const int pp_prefix[4] = {\n                0, PREFIX_DATA, PREFIX_REPZ, PREFIX_REPNZ\n            };\n            int vex3, vex2 = cpu_ldub_code(env, s->pc);\n\n            if (!CODE64(s) && (vex2 & 0xc0) != 0xc0) {\n                /* 4.1.4.6: In 32-bit mode, bits [7:6] must be 11b,\n                   otherwise the instruction is LES or LDS.  */\n                break;\n            }\n            s->pc++;\n\n            /* 4.1.1-4.1.3: No preceding lock, 66, f2, f3, or rex prefixes. */\n            if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ\n                            | PREFIX_LOCK | PREFIX_DATA)) {\n                goto illegal_op;\n            }\n#ifdef TARGET_X86_64\n            if (x86_64_hregs) {\n                goto illegal_op;\n            }\n#endif\n            rex_r = (~vex2 >> 4) & 8;\n            if (b == 0xc5) {\n                vex3 = vex2;\n                b = cpu_ldub_code(env, s->pc++);\n            } else {\n#ifdef TARGET_X86_64\n                s->rex_x = (~vex2 >> 3) & 8;\n                s->rex_b = (~vex2 >> 2) & 8;\n#endif\n                vex3 = cpu_ldub_code(env, s->pc++);\n                rex_w = (vex3 >> 7) & 1;\n                switch (vex2 & 0x1f) {\n                case 0x01: /* Implied 0f leading opcode bytes.  */\n                    b = cpu_ldub_code(env, s->pc++) | 0x100;\n                    break;\n                case 0x02: /* Implied 0f 38 leading opcode bytes.  */\n                    b = 0x138;\n                    break;\n                case 0x03: /* Implied 0f 3a leading opcode bytes.  */\n                    b = 0x13a;\n                    break;\n                default:   /* Reserved for future use.  */\n                    goto unknown_op;\n                }\n            }\n            s->vex_v = (~vex3 >> 3) & 0xf;\n            s->vex_l = (vex3 >> 2) & 1;\n            prefixes |= pp_prefix[vex3 & 3] | PREFIX_VEX;\n        }\n        break;\n    }\n\n    /* Post-process prefixes.  */\n    if (CODE64(s)) {\n        /* In 64-bit mode, the default data size is 32-bit.  Select 64-bit\n           data with rex_w, and 16-bit data with 0x66; rex_w takes precedence\n           over 0x66 if both are present.  */\n        dflag = (rex_w > 0 ? MO_64 : prefixes & PREFIX_DATA ? MO_16 : MO_32);\n        /* In 64-bit mode, 0x67 selects 32-bit addressing.  */\n        aflag = (prefixes & PREFIX_ADR ? MO_32 : MO_64);\n    } else {\n        /* In 16/32-bit mode, 0x66 selects the opposite data size.  */\n        if (s->code32 ^ ((prefixes & PREFIX_DATA) != 0)) {\n            dflag = MO_32;\n        } else {\n            dflag = MO_16;\n        }\n        /* In 16/32-bit mode, 0x67 selects the opposite addressing.  */\n        if (s->code32 ^ ((prefixes & PREFIX_ADR) != 0)) {\n            aflag = MO_32;\n        }  else {\n            aflag = MO_16;\n        }\n    }\n\n    s->prefix = prefixes;\n    s->aflag = aflag;\n    s->dflag = dflag;\n\n    /* now check op code */\n reswitch:\n    switch(b) {\n    case 0x0f:\n        /**************************/\n        /* extended op code */\n        b = cpu_ldub_code(env, s->pc++) | 0x100;\n        goto reswitch;\n\n        /**************************/\n        /* arith & logic */\n    case 0x00 ... 0x05:\n    case 0x08 ... 0x0d:\n    case 0x10 ... 0x15:\n    case 0x18 ... 0x1d:\n    case 0x20 ... 0x25:\n    case 0x28 ... 0x2d:\n    case 0x30 ... 0x35:\n    case 0x38 ... 0x3d:\n        {\n            int op, f, val;\n            op = (b >> 3) & 7;\n            f = (b >> 1) & 3;\n\n            ot = mo_b_d(b, dflag);\n\n            switch(f) {\n            case 0: /* OP Ev, Gv */\n                modrm = cpu_ldub_code(env, s->pc++);\n                reg = ((modrm >> 3) & 7) | rex_r;\n                mod = (modrm >> 6) & 3;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    opreg = OR_TMP0;\n                } else if (op == OP_XORL && rm == reg) {\n                xor_zero:\n                    /* xor reg, reg optimisation */\n                    set_cc_op(s, CC_OP_CLR);\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                    gen_op_mov_reg_v(ot, reg, cpu_T0);\n                    break;\n                } else {\n                    opreg = rm;\n                }\n                gen_op_mov_v_reg(ot, cpu_T1, reg);\n                gen_op(s, op, ot, opreg);\n                break;\n            case 1: /* OP Gv, Ev */\n                modrm = cpu_ldub_code(env, s->pc++);\n                mod = (modrm >> 6) & 3;\n                reg = ((modrm >> 3) & 7) | rex_r;\n                rm = (modrm & 7) | REX_B(s);\n                if (mod != 3) {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                } else if (op == OP_XORL && rm == reg) {\n                    goto xor_zero;\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T1, rm);\n                }\n                gen_op(s, op, ot, reg);\n                break;\n            case 2: /* OP A, Iv */\n                val = insn_get(env, s, ot);\n                tcg_gen_movi_tl(cpu_T1, val);\n                gen_op(s, op, ot, OR_EAX);\n                break;\n            }\n        }\n        break;\n\n    case 0x82:\n        if (CODE64(s))\n            goto illegal_op;\n    case 0x80: /* GRP1 */\n    case 0x81:\n    case 0x83:\n        {\n            int val;\n\n            ot = mo_b_d(b, dflag);\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (b == 0x83)\n                    s->rip_offset = 1;\n                else\n                    s->rip_offset = insn_const_size(ot);\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = rm;\n            }\n\n            switch(b) {\n            default:\n            case 0x80:\n            case 0x81:\n            case 0x82:\n                val = insn_get(env, s, ot);\n                break;\n            case 0x83:\n                val = (int8_t)insn_get(env, s, MO_8);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op(s, op, ot, opreg);\n        }\n        break;\n\n        /**************************/\n        /* inc, dec, and other misc arith */\n    case 0x40 ... 0x47: /* inc Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), 1);\n        break;\n    case 0x48 ... 0x4f: /* dec Gv */\n        ot = dflag;\n        gen_inc(s, ot, OR_EAX + (b & 7), -1);\n        break;\n    case 0xf6: /* GRP3 */\n    case 0xf7:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (mod != 3) {\n            if (op == 0) {\n                s->rip_offset = insn_const_size(ot);\n            }\n            gen_lea_modrm(env, s, modrm);\n            /* For those below that handle locked memory, don't load here.  */\n            if (!(s->prefix & PREFIX_LOCK)\n                || op != 2) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* test */\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n            gen_op_testl_T0_T1_cc();\n            set_cc_op(s, CC_OP_LOGICB + ot);\n            break;\n        case 2: /* not */\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                tcg_gen_movi_tl(cpu_T0, ~0);\n                tcg_gen_atomic_xor_fetch_tl(cpu_T0, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n            } else {\n                tcg_gen_not_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            break;\n        case 3: /* neg */\n            if (s->prefix & PREFIX_LOCK) {\n                TCGLabel *label1;\n                TCGv a0, t0, t1, t2;\n\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                a0 = tcg_temp_local_new();\n                t0 = tcg_temp_local_new();\n                label1 = gen_new_label();\n\n                tcg_gen_mov_tl(a0, cpu_A0);\n                tcg_gen_mov_tl(t0, cpu_T0);\n\n                gen_set_label(label1);\n                t1 = tcg_temp_new();\n                t2 = tcg_temp_new();\n                tcg_gen_mov_tl(t2, t0);\n                tcg_gen_neg_tl(t1, t0);\n                tcg_gen_atomic_cmpxchg_tl(t0, a0, t0, t1,\n                                          s->mem_index, ot | MO_LE);\n                tcg_temp_free(t1);\n                tcg_gen_brcond_tl(TCG_COND_NE, t0, t2, label1);\n\n                tcg_temp_free(t2);\n                tcg_temp_free(a0);\n                tcg_gen_mov_tl(cpu_T0, t0);\n                tcg_temp_free(t0);\n            } else {\n                tcg_gen_neg_tl(cpu_T0, cpu_T0);\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n            gen_op_update_neg_cc();\n            set_cc_op(s, CC_OP_SUBB + ot);\n            break;\n        case 4: /* mul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_andi_tl(cpu_cc_src, cpu_T0, 0xff00);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16u_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_mulu2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_mulu2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 5: /* imul */\n            switch(ot) {\n            case MO_8:\n                gen_op_mov_v_reg(MO_8, cpu_T1, R_EAX);\n                tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext8s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                set_cc_op(s, CC_OP_MULB);\n                break;\n            case MO_16:\n                gen_op_mov_v_reg(MO_16, cpu_T1, R_EAX);\n                tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n                /* XXX: use 32 bit mul which could be faster */\n                tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n                tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n                tcg_gen_shri_tl(cpu_T0, cpu_T0, 16);\n                gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n                set_cc_op(s, CC_OP_MULW);\n                break;\n            default:\n            case MO_32:\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_regs[R_EAX]);\n                tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                                  cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EAX], cpu_tmp2_i32);\n                tcg_gen_extu_i32_tl(cpu_regs[R_EDX], cpu_tmp3_i32);\n                tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n                tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n                set_cc_op(s, CC_OP_MULL);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                tcg_gen_muls2_i64(cpu_regs[R_EAX], cpu_regs[R_EDX],\n                                  cpu_T0, cpu_regs[R_EAX]);\n                tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[R_EAX]);\n                tcg_gen_sari_tl(cpu_cc_src, cpu_regs[R_EAX], 63);\n                tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_regs[R_EDX]);\n                set_cc_op(s, CC_OP_MULQ);\n                break;\n#endif\n            }\n            break;\n        case 6: /* div */\n            switch(ot) {\n            case MO_8:\n                gen_helper_divb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_divw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_divl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_divq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        case 7: /* idiv */\n            switch(ot) {\n            case MO_8:\n                gen_helper_idivb_AL(cpu_env, cpu_T0);\n                break;\n            case MO_16:\n                gen_helper_idivw_AX(cpu_env, cpu_T0);\n                break;\n            default:\n            case MO_32:\n                gen_helper_idivl_EAX(cpu_env, cpu_T0);\n                break;\n#ifdef TARGET_X86_64\n            case MO_64:\n                gen_helper_idivq_EAX(cpu_env, cpu_T0);\n                break;\n#endif\n            }\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0xfe: /* GRP4 */\n    case 0xff: /* GRP5 */\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        op = (modrm >> 3) & 7;\n        if (op >= 2 && b == 0xfe) {\n            goto unknown_op;\n        }\n        if (CODE64(s)) {\n            if (op == 2 || op == 4) {\n                /* operand size for jumps is 64 bit */\n                ot = MO_64;\n            } else if (op == 3 || op == 5) {\n                ot = dflag != MO_16 ? MO_32 + (rex_w == 1) : MO_16;\n            } else if (op == 6) {\n                /* default push size is 64 bit */\n                ot = mo_pushpop(s, dflag);\n            }\n        }\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            if (op >= 2 && op != 3 && op != 5)\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n\n        switch(op) {\n        case 0: /* inc Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, 1);\n            break;\n        case 1: /* dec Ev */\n            if (mod != 3)\n                opreg = OR_TMP0;\n            else\n                opreg = rm;\n            gen_inc(s, ot, opreg, -1);\n            break;\n        case 2: /* call Ev */\n            /* XXX: optimize if memory (no 'and' is necessary) */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            next_eip = s->pc - s->cs_base;\n            tcg_gen_movi_tl(cpu_T1, next_eip);\n            gen_push_v(s, cpu_T1);\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 3: /* lcall Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_lcall:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                           tcg_const_i32(dflag - 1),\n                                           tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lcall_real(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                      tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            }\n            gen_eob(s);\n            break;\n        case 4: /* jmp Ev */\n            if (dflag == MO_16) {\n                tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n            }\n            gen_op_jmp_v(cpu_T0);\n            gen_bnd_jmp(s);\n            gen_eob(s);\n            break;\n        case 5: /* ljmp Ev */\n            gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 1 << ot);\n            gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        do_ljmp:\n            if (s->pe && !s->vm86) {\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ljmp_protected(cpu_env, cpu_tmp2_i32, cpu_T1,\n                                          tcg_const_tl(s->pc - s->cs_base));\n            } else {\n                gen_op_movl_seg_T0_vm(R_CS);\n                gen_op_jmp_v(cpu_T1);\n            }\n            gen_eob(s);\n            break;\n        case 6: /* push Ev */\n            gen_push_v(s, cpu_T0);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x84: /* test Ev, Gv */\n    case 0x85:\n        ot = mo_b_d(b, dflag);\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0xa8: /* test eAX, Iv */\n    case 0xa9:\n        ot = mo_b_d(b, dflag);\n        val = insn_get(env, s, ot);\n\n        gen_op_mov_v_reg(ot, cpu_T0, OR_EAX);\n        tcg_gen_movi_tl(cpu_T1, val);\n        gen_op_testl_T0_T1_cc();\n        set_cc_op(s, CC_OP_LOGICB + ot);\n        break;\n\n    case 0x98: /* CWDE/CBW */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, R_EAX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, R_EAX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_8, cpu_T0, R_EAX);\n            tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x99: /* CDQ/CWD */\n        switch (dflag) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            gen_op_mov_v_reg(MO_64, cpu_T0, R_EAX);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 63);\n            gen_op_mov_reg_v(MO_64, R_EDX, cpu_T0);\n            break;\n#endif\n        case MO_32:\n            gen_op_mov_v_reg(MO_32, cpu_T0, R_EAX);\n            tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 31);\n            gen_op_mov_reg_v(MO_32, R_EDX, cpu_T0);\n            break;\n        case MO_16:\n            gen_op_mov_v_reg(MO_16, cpu_T0, R_EAX);\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_sari_tl(cpu_T0, cpu_T0, 15);\n            gen_op_mov_reg_v(MO_16, R_EDX, cpu_T0);\n            break;\n        default:\n            tcg_abort();\n        }\n        break;\n    case 0x1af: /* imul Gv, Ev */\n    case 0x69: /* imul Gv, Ev, I */\n    case 0x6b:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (b == 0x69)\n            s->rip_offset = insn_const_size(ot);\n        else if (b == 0x6b)\n            s->rip_offset = 1;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        if (b == 0x69) {\n            val = insn_get(env, s, ot);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else if (b == 0x6b) {\n            val = (int8_t)insn_get(env, s, MO_8);\n            tcg_gen_movi_tl(cpu_T1, val);\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T1, reg);\n        }\n        switch (ot) {\n#ifdef TARGET_X86_64\n        case MO_64:\n            tcg_gen_muls2_i64(cpu_regs[reg], cpu_T1, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sari_tl(cpu_cc_src, cpu_cc_dst, 63);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_cc_src, cpu_T1);\n            break;\n#endif\n        case MO_32:\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n            tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n            tcg_gen_muls2_i32(cpu_tmp2_i32, cpu_tmp3_i32,\n                              cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_regs[reg], cpu_tmp2_i32);\n            tcg_gen_sari_i32(cpu_tmp2_i32, cpu_tmp2_i32, 31);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_regs[reg]);\n            tcg_gen_sub_i32(cpu_tmp2_i32, cpu_tmp2_i32, cpu_tmp3_i32);\n            tcg_gen_extu_i32_tl(cpu_cc_src, cpu_tmp2_i32);\n            break;\n        default:\n            tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_T1, cpu_T1);\n            /* XXX: use 32 bit mul which could be faster */\n            tcg_gen_mul_tl(cpu_T0, cpu_T0, cpu_T1);\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            tcg_gen_ext16s_tl(cpu_tmp0, cpu_T0);\n            tcg_gen_sub_tl(cpu_cc_src, cpu_T0, cpu_tmp0);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n            break;\n        }\n        set_cc_op(s, CC_OP_MULB + ot);\n        break;\n    case 0x1c0:\n    case 0x1c1: /* xadd Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            if (s->prefix & PREFIX_LOCK) {\n                tcg_gen_atomic_fetch_add_tl(cpu_T1, cpu_A0, cpu_T0,\n                                            s->mem_index, ot | MO_LE);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n            } else {\n                gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n                tcg_gen_add_tl(cpu_T0, cpu_T0, cpu_T1);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        gen_op_update2_cc();\n        set_cc_op(s, CC_OP_ADDB + ot);\n        break;\n    case 0x1b0:\n    case 0x1b1: /* cmpxchg Ev, Gv */\n        {\n            TCGv oldv, newv, cmpv;\n\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            oldv = tcg_temp_new();\n            newv = tcg_temp_new();\n            cmpv = tcg_temp_new();\n            gen_op_mov_v_reg(ot, newv, reg);\n            tcg_gen_mov_tl(cmpv, cpu_regs[R_EAX]);\n\n            if (s->prefix & PREFIX_LOCK) {\n                if (mod == 3) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_atomic_cmpxchg_tl(oldv, cpu_A0, cmpv, newv,\n                                          s->mem_index, ot | MO_LE);\n                gen_op_mov_reg_v(ot, R_EAX, oldv);\n            } else {\n                if (mod == 3) {\n                    rm = (modrm & 7) | REX_B(s);\n                    gen_op_mov_v_reg(ot, oldv, rm);\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    gen_op_ld_v(s, ot, oldv, cpu_A0);\n                    rm = 0; /* avoid warning */\n                }\n                gen_extu(ot, oldv);\n                gen_extu(ot, cmpv);\n                /* store value = (old == cmp ? new : old);  */\n                tcg_gen_movcond_tl(TCG_COND_EQ, newv, oldv, cmpv, newv, oldv);\n                if (mod == 3) {\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                    gen_op_mov_reg_v(ot, rm, newv);\n                } else {\n                    /* Perform an unconditional store cycle like physical cpu;\n                       must be before changing accumulator to ensure\n                       idempotency if the store faults and the instruction\n                       is restarted */\n                    gen_op_st_v(s, ot, newv, cpu_A0);\n                    gen_op_mov_reg_v(ot, R_EAX, oldv);\n                }\n            }\n            tcg_gen_mov_tl(cpu_cc_src, oldv);\n            tcg_gen_mov_tl(cpu_cc_srcT, cmpv);\n            tcg_gen_sub_tl(cpu_cc_dst, cmpv, oldv);\n            set_cc_op(s, CC_OP_SUBB + ot);\n            tcg_temp_free(oldv);\n            tcg_temp_free(newv);\n            tcg_temp_free(cmpv);\n        }\n        break;\n    case 0x1c7: /* cmpxchg8b */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if ((mod == 3) || ((modrm & 0x38) != 0x8))\n            goto illegal_op;\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            if (!(s->cpuid_ext_features & CPUID_EXT_CX16))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg16b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg16b_unlocked(cpu_env, cpu_A0);\n            }\n        } else\n#endif        \n        {\n            if (!(s->cpuid_features & CPUID_CX8))\n                goto illegal_op;\n            gen_lea_modrm(env, s, modrm);\n            if ((s->prefix & PREFIX_LOCK) && parallel_cpus) {\n                gen_helper_cmpxchg8b(cpu_env, cpu_A0);\n            } else {\n                gen_helper_cmpxchg8b_unlocked(cpu_env, cpu_A0);\n            }\n        }\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n\n        /**************************/\n        /* push/pop */\n    case 0x50 ... 0x57: /* push */\n        gen_op_mov_v_reg(MO_32, cpu_T0, (b & 7) | REX_B(s));\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x58 ... 0x5f: /* pop */\n        ot = gen_pop_T0(s);\n        /* NOTE: order is important for pop %sp */\n        gen_pop_update(s, ot);\n        gen_op_mov_reg_v(ot, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0x60: /* pusha */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_pusha(s);\n        break;\n    case 0x61: /* popa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_popa(s);\n        break;\n    case 0x68: /* push Iv */\n    case 0x6a:\n        ot = mo_pushpop(s, dflag);\n        if (b == 0x68)\n            val = insn_get(env, s, ot);\n        else\n            val = (int8_t)insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x8f: /* pop Ev */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        ot = gen_pop_T0(s);\n        if (mod == 3) {\n            /* NOTE: order is important for pop %sp */\n            gen_pop_update(s, ot);\n            rm = (modrm & 7) | REX_B(s);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n        } else {\n            /* NOTE: order is important too for MMU exceptions */\n            s->popl_esp_hack = 1 << ot;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            s->popl_esp_hack = 0;\n            gen_pop_update(s, ot);\n        }\n        break;\n    case 0xc8: /* enter */\n        {\n            int level;\n            val = cpu_lduw_code(env, s->pc);\n            s->pc += 2;\n            level = cpu_ldub_code(env, s->pc++);\n            gen_enter(s, val, level);\n        }\n        break;\n    case 0xc9: /* leave */\n        gen_leave(s);\n        break;\n    case 0x06: /* push es */\n    case 0x0e: /* push cs */\n    case 0x16: /* push ss */\n    case 0x1e: /* push ds */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_op_movl_T0_seg(b >> 3);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x1a0: /* push fs */\n    case 0x1a8: /* push gs */\n        gen_op_movl_T0_seg((b >> 3) & 7);\n        gen_push_v(s, cpu_T0);\n        break;\n    case 0x07: /* pop es */\n    case 0x17: /* pop ss */\n    case 0x1f: /* pop ds */\n        if (CODE64(s))\n            goto illegal_op;\n        reg = b >> 3;\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, reg);\n        gen_pop_update(s, ot);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x1a1: /* pop fs */\n    case 0x1a9: /* pop gs */\n        ot = gen_pop_T0(s);\n        gen_movl_seg_T0(s, (b >> 3) & 7);\n        gen_pop_update(s, ot);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /**************************/\n        /* mov */\n    case 0x88:\n    case 0x89: /* mov Gv, Ev */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0xc6:\n    case 0xc7: /* mov Ev, Iv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod != 3) {\n            s->rip_offset = insn_const_size(ot);\n            gen_lea_modrm(env, s, modrm);\n        }\n        val = insn_get(env, s, ot);\n        tcg_gen_movi_tl(cpu_T0, val);\n        if (mod != 3) {\n            gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n        } else {\n            gen_op_mov_reg_v(ot, (modrm & 7) | REX_B(s), cpu_T0);\n        }\n        break;\n    case 0x8a:\n    case 0x8b: /* mov Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n    case 0x8e: /* mov seg, Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        if (reg >= 6 || reg == R_CS)\n            goto illegal_op;\n        gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n        gen_movl_seg_T0(s, reg);\n        /* Note that reg == R_SS in gen_movl_seg_T0 always sets is_jmp.  */\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            if (reg == R_SS) {\n                s->tf = 0;\n                gen_eob_inhibit_irq(s, true);\n            } else {\n                gen_eob(s);\n            }\n        }\n        break;\n    case 0x8c: /* mov Gv, seg */\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (reg >= 6)\n            goto illegal_op;\n        gen_op_movl_T0_seg(reg);\n        ot = mod == 3 ? dflag : MO_16;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n        break;\n\n    case 0x1b6: /* movzbS Gv, Eb */\n    case 0x1b7: /* movzwS Gv, Eb */\n    case 0x1be: /* movsbS Gv, Eb */\n    case 0x1bf: /* movswS Gv, Eb */\n        {\n            TCGMemOp d_ot;\n            TCGMemOp s_ot;\n\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n            /* ot is the size of source */\n            ot = (b & 1) + MO_8;\n            /* s_ot is the sign+size of source */\n            s_ot = b & 8 ? MO_SIGN | ot : ot;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                if (s_ot == MO_SB && byte_reg_is_xH(rm)) {\n                    tcg_gen_sextract_tl(cpu_T0, cpu_regs[rm - 4], 8, 8);\n                } else {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    switch (s_ot) {\n                    case MO_UB:\n                        tcg_gen_ext8u_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_SB:\n                        tcg_gen_ext8s_tl(cpu_T0, cpu_T0);\n                        break;\n                    case MO_UW:\n                        tcg_gen_ext16u_tl(cpu_T0, cpu_T0);\n                        break;\n                    default:\n                    case MO_SW:\n                        tcg_gen_ext16s_tl(cpu_T0, cpu_T0);\n                        break;\n                    }\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, s_ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        }\n        break;\n\n    case 0x8d: /* lea */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            TCGv ea = gen_lea_modrm_1(a);\n            gen_lea_v_seg(s, s->aflag, ea, -1, -1);\n            gen_op_mov_reg_v(dflag, reg, cpu_A0);\n        }\n        break;\n\n    case 0xa0: /* mov EAX, Ov */\n    case 0xa1:\n    case 0xa2: /* mov Ov, EAX */\n    case 0xa3:\n        {\n            target_ulong offset_addr;\n\n            ot = mo_b_d(b, dflag);\n            switch (s->aflag) {\n#ifdef TARGET_X86_64\n            case MO_64:\n                offset_addr = cpu_ldq_code(env, s->pc);\n                s->pc += 8;\n                break;\n#endif\n            default:\n                offset_addr = insn_get(env, s, s->aflag);\n                break;\n            }\n            tcg_gen_movi_tl(cpu_A0, offset_addr);\n            gen_add_A0_ds_seg(s);\n            if ((b & 2) == 0) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(ot, R_EAX, cpu_T0);\n            } else {\n                gen_op_mov_v_reg(ot, cpu_T0, R_EAX);\n                gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n            }\n        }\n        break;\n    case 0xd7: /* xlat */\n        tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EBX]);\n        tcg_gen_ext8u_tl(cpu_T0, cpu_regs[R_EAX]);\n        tcg_gen_add_tl(cpu_A0, cpu_A0, cpu_T0);\n        gen_extu(s->aflag, cpu_A0);\n        gen_add_A0_ds_seg(s);\n        gen_op_ld_v(s, MO_8, cpu_T0, cpu_A0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xb0 ... 0xb7: /* mov R, Ib */\n        val = insn_get(env, s, MO_8);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_op_mov_reg_v(MO_8, (b & 7) | REX_B(s), cpu_T0);\n        break;\n    case 0xb8 ... 0xbf: /* mov R, Iv */\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            uint64_t tmp;\n            /* 64 bit case */\n            tmp = cpu_ldq_code(env, s->pc);\n            s->pc += 8;\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, tmp);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            ot = dflag;\n            val = insn_get(env, s, ot);\n            reg = (b & 7) | REX_B(s);\n            tcg_gen_movi_tl(cpu_T0, val);\n            gen_op_mov_reg_v(ot, reg, cpu_T0);\n        }\n        break;\n\n    case 0x91 ... 0x97: /* xchg R, EAX */\n    do_xchg_reg_eax:\n        ot = dflag;\n        reg = (b & 7) | REX_B(s);\n        rm = R_EAX;\n        goto do_xchg_reg;\n    case 0x86:\n    case 0x87: /* xchg Ev, Gv */\n        ot = mo_b_d(b, dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3) {\n            rm = (modrm & 7) | REX_B(s);\n        do_xchg_reg:\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            gen_op_mov_v_reg(ot, cpu_T1, rm);\n            gen_op_mov_reg_v(ot, rm, cpu_T0);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        } else {\n            gen_lea_modrm(env, s, modrm);\n            gen_op_mov_v_reg(ot, cpu_T0, reg);\n            /* for xchg, lock is implicit */\n            tcg_gen_atomic_xchg_tl(cpu_T1, cpu_A0, cpu_T0,\n                                   s->mem_index, ot | MO_LE);\n            gen_op_mov_reg_v(ot, reg, cpu_T1);\n        }\n        break;\n    case 0xc4: /* les Gv */\n        /* In CODE64 this is VEX3; see above.  */\n        op = R_ES;\n        goto do_lxx;\n    case 0xc5: /* lds Gv */\n        /* In CODE64 this is VEX2; see above.  */\n        op = R_DS;\n        goto do_lxx;\n    case 0x1b2: /* lss Gv */\n        op = R_SS;\n        goto do_lxx;\n    case 0x1b4: /* lfs Gv */\n        op = R_FS;\n        goto do_lxx;\n    case 0x1b5: /* lgs Gv */\n        op = R_GS;\n    do_lxx:\n        ot = dflag != MO_16 ? MO_32 : MO_16;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_lea_modrm(env, s, modrm);\n        gen_op_ld_v(s, ot, cpu_T1, cpu_A0);\n        gen_add_A0_im(s, 1 << ot);\n        /* load the segment first to handle exceptions properly */\n        gen_op_ld_v(s, MO_16, cpu_T0, cpu_A0);\n        gen_movl_seg_T0(s, op);\n        /* then put the data */\n        gen_op_mov_reg_v(ot, reg, cpu_T1);\n        if (s->is_jmp) {\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n\n        /************************/\n        /* shifts */\n    case 0xc0:\n    case 0xc1:\n        /* shift Ev,Ib */\n        shift = 2;\n    grp2:\n        {\n            ot = mo_b_d(b, dflag);\n            modrm = cpu_ldub_code(env, s->pc++);\n            mod = (modrm >> 6) & 3;\n            op = (modrm >> 3) & 7;\n\n            if (mod != 3) {\n                if (shift == 2) {\n                    s->rip_offset = 1;\n                }\n                gen_lea_modrm(env, s, modrm);\n                opreg = OR_TMP0;\n            } else {\n                opreg = (modrm & 7) | REX_B(s);\n            }\n\n            /* simpler op */\n            if (shift == 0) {\n                gen_shift(s, op, ot, opreg, OR_ECX);\n            } else {\n                if (shift == 2) {\n                    shift = cpu_ldub_code(env, s->pc++);\n                }\n                gen_shifti(s, op, ot, opreg, shift);\n            }\n        }\n        break;\n    case 0xd0:\n    case 0xd1:\n        /* shift Ev,1 */\n        shift = 1;\n        goto grp2;\n    case 0xd2:\n    case 0xd3:\n        /* shift Ev,cl */\n        shift = 0;\n        goto grp2;\n\n    case 0x1a4: /* shld imm */\n        op = 0;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1a5: /* shld cl */\n        op = 0;\n        shift = 0;\n        goto do_shiftd;\n    case 0x1ac: /* shrd imm */\n        op = 1;\n        shift = 1;\n        goto do_shiftd;\n    case 0x1ad: /* shrd cl */\n        op = 1;\n        shift = 0;\n    do_shiftd:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        if (mod != 3) {\n            gen_lea_modrm(env, s, modrm);\n            opreg = OR_TMP0;\n        } else {\n            opreg = rm;\n        }\n        gen_op_mov_v_reg(ot, cpu_T1, reg);\n\n        if (shift) {\n            TCGv imm = tcg_const_tl(cpu_ldub_code(env, s->pc++));\n            gen_shiftd_rm_T1(s, ot, opreg, op, imm);\n            tcg_temp_free(imm);\n        } else {\n            gen_shiftd_rm_T1(s, ot, opreg, op, cpu_regs[R_ECX]);\n        }\n        break;\n\n        /************************/\n        /* floats */\n    case 0xd8 ... 0xdf:\n        if (s->flags & (HF_EM_MASK | HF_TS_MASK)) {\n            /* if CR0.EM or CR0.TS are set, generate an FPU exception */\n            /* XXX: what to do if illegal op ? */\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n            break;\n        }\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        rm = modrm & 7;\n        op = ((b & 7) << 3) | ((modrm >> 3) & 7);\n        if (mod != 3) {\n            /* memory op */\n            gen_lea_modrm(env, s, modrm);\n            switch(op) {\n            case 0x00 ... 0x07: /* fxxxs */\n            case 0x10 ... 0x17: /* fixxxl */\n            case 0x20 ... 0x27: /* fxxxl */\n            case 0x30 ... 0x37: /* fixxx */\n                {\n                    int op1;\n                    op1 = op & 7;\n\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_FT0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_FT0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n\n                    gen_helper_fp_arith_ST0_FT0(op1);\n                    if (op1 == 3) {\n                        /* fcomp needs pop */\n                        gen_helper_fpop(cpu_env);\n                    }\n                }\n                break;\n            case 0x08: /* flds */\n            case 0x0a: /* fsts */\n            case 0x0b: /* fstps */\n            case 0x18 ... 0x1b: /* fildl, fisttpl, fistl, fistpl */\n            case 0x28 ... 0x2b: /* fldl, fisttpll, fstl, fstpl */\n            case 0x38 ... 0x3b: /* filds, fisttps, fists, fistps */\n                switch(op & 7) {\n                case 0:\n                    switch(op >> 4) {\n                    case 0:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_flds_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 1:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    case 2:\n                        tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        gen_helper_fldl_ST0(cpu_env, cpu_tmp1_i64);\n                        break;\n                    case 3:\n                    default:\n                        tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LESW);\n                        gen_helper_fildl_ST0(cpu_env, cpu_tmp2_i32);\n                        break;\n                    }\n                    break;\n                case 1:\n                    /* XXX: the corresponding CPUID bit must be tested ! */\n                    switch(op >> 4) {\n                    case 1:\n                        gen_helper_fisttl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fisttll_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fistt_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    switch(op >> 4) {\n                    case 0:\n                        gen_helper_fsts_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 1:\n                        gen_helper_fistl_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        break;\n                    case 2:\n                        gen_helper_fstl_ST0(cpu_tmp1_i64, cpu_env);\n                        tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        break;\n                    case 3:\n                    default:\n                        gen_helper_fist_ST0(cpu_tmp2_i32, cpu_env);\n                        tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                            s->mem_index, MO_LEUW);\n                        break;\n                    }\n                    if ((op & 7) == 3)\n                        gen_helper_fpop(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0c: /* fldenv mem */\n                gen_helper_fldenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0d: /* fldcw mem */\n                tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                gen_helper_fldcw(cpu_env, cpu_tmp2_i32);\n                break;\n            case 0x0e: /* fnstenv mem */\n                gen_helper_fstenv(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x0f: /* fnstcw mem */\n                gen_helper_fnstcw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x1d: /* fldt mem */\n                gen_helper_fldt_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x1f: /* fstpt mem */\n                gen_helper_fstt_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* frstor mem */\n                gen_helper_frstor(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2e: /* fnsave mem */\n                gen_helper_fsave(cpu_env, cpu_A0, tcg_const_i32(dflag - 1));\n                break;\n            case 0x2f: /* fnstsw mem */\n                gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                tcg_gen_qemu_st_i32(cpu_tmp2_i32, cpu_A0,\n                                    s->mem_index, MO_LEUW);\n                break;\n            case 0x3c: /* fbld */\n                gen_helper_fbld_ST0(cpu_env, cpu_A0);\n                break;\n            case 0x3e: /* fbstp */\n                gen_helper_fbst_ST0(cpu_env, cpu_A0);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3d: /* fildll */\n                tcg_gen_qemu_ld_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fildll_ST0(cpu_env, cpu_tmp1_i64);\n                break;\n            case 0x3f: /* fistpll */\n                gen_helper_fistll_ST0(cpu_tmp1_i64, cpu_env);\n                tcg_gen_qemu_st_i64(cpu_tmp1_i64, cpu_A0, s->mem_index, MO_LEQ);\n                gen_helper_fpop(cpu_env);\n                break;\n            default:\n                goto unknown_op;\n            }\n        } else {\n            /* register float ops */\n            opreg = rm;\n\n            switch(op) {\n            case 0x08: /* fld sti */\n                gen_helper_fpush(cpu_env);\n                gen_helper_fmov_ST0_STN(cpu_env,\n                                        tcg_const_i32((opreg + 1) & 7));\n                break;\n            case 0x09: /* fxchg sti */\n            case 0x29: /* fxchg4 sti, undocumented op */\n            case 0x39: /* fxchg7 sti, undocumented op */\n                gen_helper_fxchg_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x0a: /* grp d9/2 */\n                switch(rm) {\n                case 0: /* fnop */\n                    /* check exceptions (FreeBSD FPU probe) */\n                    gen_helper_fwait(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0c: /* grp d9/4 */\n                switch(rm) {\n                case 0: /* fchs */\n                    gen_helper_fchs_ST0(cpu_env);\n                    break;\n                case 1: /* fabs */\n                    gen_helper_fabs_ST0(cpu_env);\n                    break;\n                case 4: /* ftst */\n                    gen_helper_fldz_FT0(cpu_env);\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    break;\n                case 5: /* fxam */\n                    gen_helper_fxam_ST0(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x0d: /* grp d9/5 */\n                {\n                    switch(rm) {\n                    case 0:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fld1_ST0(cpu_env);\n                        break;\n                    case 1:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2t_ST0(cpu_env);\n                        break;\n                    case 2:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldl2e_ST0(cpu_env);\n                        break;\n                    case 3:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldpi_ST0(cpu_env);\n                        break;\n                    case 4:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldlg2_ST0(cpu_env);\n                        break;\n                    case 5:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldln2_ST0(cpu_env);\n                        break;\n                    case 6:\n                        gen_helper_fpush(cpu_env);\n                        gen_helper_fldz_ST0(cpu_env);\n                        break;\n                    default:\n                        goto unknown_op;\n                    }\n                }\n                break;\n            case 0x0e: /* grp d9/6 */\n                switch(rm) {\n                case 0: /* f2xm1 */\n                    gen_helper_f2xm1(cpu_env);\n                    break;\n                case 1: /* fyl2x */\n                    gen_helper_fyl2x(cpu_env);\n                    break;\n                case 2: /* fptan */\n                    gen_helper_fptan(cpu_env);\n                    break;\n                case 3: /* fpatan */\n                    gen_helper_fpatan(cpu_env);\n                    break;\n                case 4: /* fxtract */\n                    gen_helper_fxtract(cpu_env);\n                    break;\n                case 5: /* fprem1 */\n                    gen_helper_fprem1(cpu_env);\n                    break;\n                case 6: /* fdecstp */\n                    gen_helper_fdecstp(cpu_env);\n                    break;\n                default:\n                case 7: /* fincstp */\n                    gen_helper_fincstp(cpu_env);\n                    break;\n                }\n                break;\n            case 0x0f: /* grp d9/7 */\n                switch(rm) {\n                case 0: /* fprem */\n                    gen_helper_fprem(cpu_env);\n                    break;\n                case 1: /* fyl2xp1 */\n                    gen_helper_fyl2xp1(cpu_env);\n                    break;\n                case 2: /* fsqrt */\n                    gen_helper_fsqrt(cpu_env);\n                    break;\n                case 3: /* fsincos */\n                    gen_helper_fsincos(cpu_env);\n                    break;\n                case 5: /* fscale */\n                    gen_helper_fscale(cpu_env);\n                    break;\n                case 4: /* frndint */\n                    gen_helper_frndint(cpu_env);\n                    break;\n                case 6: /* fsin */\n                    gen_helper_fsin(cpu_env);\n                    break;\n                default:\n                case 7: /* fcos */\n                    gen_helper_fcos(cpu_env);\n                    break;\n                }\n                break;\n            case 0x00: case 0x01: case 0x04 ... 0x07: /* fxxx st, sti */\n            case 0x20: case 0x21: case 0x24 ... 0x27: /* fxxx sti, st */\n            case 0x30: case 0x31: case 0x34 ... 0x37: /* fxxxp sti, st */\n                {\n                    int op1;\n\n                    op1 = op & 7;\n                    if (op >= 0x20) {\n                        gen_helper_fp_arith_STN_ST0(op1, opreg);\n                        if (op >= 0x30)\n                            gen_helper_fpop(cpu_env);\n                    } else {\n                        gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                        gen_helper_fp_arith_ST0_FT0(op1);\n                    }\n                }\n                break;\n            case 0x02: /* fcom */\n            case 0x22: /* fcom2, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                break;\n            case 0x03: /* fcomp */\n            case 0x23: /* fcomp3, undocumented op */\n            case 0x32: /* fcomp5, undocumented op */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x15: /* da/5 */\n                switch(rm) {\n                case 1: /* fucompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fucom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1c:\n                switch(rm) {\n                case 0: /* feni (287 only, just do nop here) */\n                    break;\n                case 1: /* fdisi (287 only, just do nop here) */\n                    break;\n                case 2: /* fclex */\n                    gen_helper_fclex(cpu_env);\n                    break;\n                case 3: /* fninit */\n                    gen_helper_fninit(cpu_env);\n                    break;\n                case 4: /* fsetpm (287 only, just do nop here) */\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x1d: /* fucomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x1e: /* fcomi */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x28: /* ffree sti */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2a: /* fst sti */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                break;\n            case 0x2b: /* fstp sti */\n            case 0x0b: /* fstp1 sti, undocumented op */\n            case 0x3a: /* fstp8 sti, undocumented op */\n            case 0x3b: /* fstp9 sti, undocumented op */\n                gen_helper_fmov_STN_ST0(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x2c: /* fucom st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                break;\n            case 0x2d: /* fucomp st(i) */\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucom_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x33: /* de/3 */\n                switch(rm) {\n                case 1: /* fcompp */\n                    gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(1));\n                    gen_helper_fcom_ST0_FT0(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    gen_helper_fpop(cpu_env);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x38: /* ffreep sti, undocumented op */\n                gen_helper_ffree_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fpop(cpu_env);\n                break;\n            case 0x3c: /* df/4 */\n                switch(rm) {\n                case 0:\n                    gen_helper_fnstsw(cpu_tmp2_i32, cpu_env);\n                    tcg_gen_extu_i32_tl(cpu_T0, cpu_tmp2_i32);\n                    gen_op_mov_reg_v(MO_16, R_EAX, cpu_T0);\n                    break;\n                default:\n                    goto unknown_op;\n                }\n                break;\n            case 0x3d: /* fucomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fucomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x3e: /* fcomip */\n                if (!(s->cpuid_features & CPUID_CMOV)) {\n                    goto illegal_op;\n                }\n                gen_update_cc_op(s);\n                gen_helper_fmov_FT0_STN(cpu_env, tcg_const_i32(opreg));\n                gen_helper_fcomi_ST0_FT0(cpu_env);\n                gen_helper_fpop(cpu_env);\n                set_cc_op(s, CC_OP_EFLAGS);\n                break;\n            case 0x10 ... 0x13: /* fcmovxx */\n            case 0x18 ... 0x1b:\n                {\n                    int op1;\n                    TCGLabel *l1;\n                    static const uint8_t fcmov_cc[8] = {\n                        (JCC_B << 1),\n                        (JCC_Z << 1),\n                        (JCC_BE << 1),\n                        (JCC_P << 1),\n                    };\n\n                    if (!(s->cpuid_features & CPUID_CMOV)) {\n                        goto illegal_op;\n                    }\n                    op1 = fcmov_cc[op & 3] | (((op >> 3) & 1) ^ 1);\n                    l1 = gen_new_label();\n                    gen_jcc1_noeob(s, op1, l1);\n                    gen_helper_fmov_ST0_STN(cpu_env, tcg_const_i32(opreg));\n                    gen_set_label(l1);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n        /************************/\n        /* string ops */\n\n    case 0xa4: /* movsS */\n    case 0xa5:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_movs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_movs(s, ot);\n        }\n        break;\n\n    case 0xaa: /* stosS */\n    case 0xab:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_stos(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_stos(s, ot);\n        }\n        break;\n    case 0xac: /* lodsS */\n    case 0xad:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_lods(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_lods(s, ot);\n        }\n        break;\n    case 0xae: /* scasS */\n    case 0xaf:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_scas(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_scas(s, ot);\n        }\n        break;\n\n    case 0xa6: /* cmpsS */\n    case 0xa7:\n        ot = mo_b_d(b, dflag);\n        if (prefixes & PREFIX_REPNZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 1);\n        } else if (prefixes & PREFIX_REPZ) {\n            gen_repz_cmps(s, ot, pc_start - s->cs_base, s->pc - s->cs_base, 0);\n        } else {\n            gen_cmps(s, ot);\n        }\n        break;\n    case 0x6c: /* insS */\n    case 0x6d:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base, \n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_ins(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_ins(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n    case 0x6e: /* outsS */\n    case 0x6f:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes) | 4);\n        if (prefixes & (PREFIX_REPZ | PREFIX_REPNZ)) {\n            gen_repz_outs(s, ot, pc_start - s->cs_base, s->pc - s->cs_base);\n        } else {\n            gen_outs(s, ot);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n        }\n        break;\n\n        /************************/\n        /* port I/O */\n\n    case 0xe4:\n    case 0xe5:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xe6:\n    case 0xe7:\n        ot = mo_b_d32(b, dflag);\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T0, val);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_movi_i32(cpu_tmp2_i32, val);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xec:\n    case 0xed:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     SVM_IOIO_TYPE_MASK | svm_is_rep(prefixes));\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        gen_helper_in_func(ot, cpu_T1, cpu_tmp2_i32);\n        gen_op_mov_reg_v(ot, R_EAX, cpu_T1);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0xee:\n    case 0xef:\n        ot = mo_b_d32(b, dflag);\n        tcg_gen_ext16u_tl(cpu_T0, cpu_regs[R_EDX]);\n        gen_check_io(s, ot, pc_start - s->cs_base,\n                     svm_is_rep(prefixes));\n        gen_op_mov_v_reg(ot, cpu_T1, R_EAX);\n\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        tcg_gen_trunc_tl_i32(cpu_tmp3_i32, cpu_T1);\n        gen_helper_out_func(ot, cpu_tmp2_i32, cpu_tmp3_i32);\n        gen_bpt_io(s, cpu_tmp2_i32, ot);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n\n        /************************/\n        /* control */\n    case 0xc2: /* ret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n        ot = gen_pop_T0(s);\n        gen_stack_update(s, val + (1 << ot));\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xc3: /* ret */\n        ot = gen_pop_T0(s);\n        gen_pop_update(s, ot);\n        /* Note that gen_pop_T0 uses a zero-extending load.  */\n        gen_op_jmp_v(cpu_T0);\n        gen_bnd_jmp(s);\n        gen_eob(s);\n        break;\n    case 0xca: /* lret im */\n        val = cpu_ldsw_code(env, s->pc);\n        s->pc += 2;\n    do_lret:\n        if (s->pe && !s->vm86) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_lret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(val));\n        } else {\n            gen_stack_A0(s);\n            /* pop offset */\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            /* NOTE: keeping EIP updated is not a problem in case of\n               exception */\n            gen_op_jmp_v(cpu_T0);\n            /* pop selector */\n            gen_add_A0_im(s, 1 << dflag);\n            gen_op_ld_v(s, dflag, cpu_T0, cpu_A0);\n            gen_op_movl_seg_T0_vm(R_CS);\n            /* add stack offset */\n            gen_stack_update(s, val + (2 << dflag));\n        }\n        gen_eob(s);\n        break;\n    case 0xcb: /* lret */\n        val = 0;\n        goto do_lret;\n    case 0xcf: /* iret */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_IRET);\n        if (!s->pe) {\n            /* real mode */\n            gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n            set_cc_op(s, CC_OP_EFLAGS);\n        } else if (s->vm86) {\n            if (s->iopl != 3) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_helper_iret_real(cpu_env, tcg_const_i32(dflag - 1));\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n        } else {\n            gen_helper_iret_protected(cpu_env, tcg_const_i32(dflag - 1),\n                                      tcg_const_i32(s->pc - s->cs_base));\n            set_cc_op(s, CC_OP_EFLAGS);\n        }\n        gen_eob(s);\n        break;\n    case 0xe8: /* call im */\n        {\n            if (dflag != MO_16) {\n                tval = (int32_t)insn_get(env, s, MO_32);\n            } else {\n                tval = (int16_t)insn_get(env, s, MO_16);\n            }\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            } else if (!CODE64(s)) {\n                tval &= 0xffffffff;\n            }\n            tcg_gen_movi_tl(cpu_T0, next_eip);\n            gen_push_v(s, cpu_T0);\n            gen_bnd_jmp(s);\n            gen_jmp(s, tval);\n        }\n        break;\n    case 0x9a: /* lcall im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_lcall;\n    case 0xe9: /* jmp im */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        } else if (!CODE64(s)) {\n            tval &= 0xffffffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jmp(s, tval);\n        break;\n    case 0xea: /* ljmp im */\n        {\n            unsigned int selector, offset;\n\n            if (CODE64(s))\n                goto illegal_op;\n            ot = dflag;\n            offset = insn_get(env, s, ot);\n            selector = insn_get(env, s, MO_16);\n\n            tcg_gen_movi_tl(cpu_T0, selector);\n            tcg_gen_movi_tl(cpu_T1, offset);\n        }\n        goto do_ljmp;\n    case 0xeb: /* jmp Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        tval += s->pc - s->cs_base;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_jmp(s, tval);\n        break;\n    case 0x70 ... 0x7f: /* jcc Jb */\n        tval = (int8_t)insn_get(env, s, MO_8);\n        goto do_jcc;\n    case 0x180 ... 0x18f: /* jcc Jv */\n        if (dflag != MO_16) {\n            tval = (int32_t)insn_get(env, s, MO_32);\n        } else {\n            tval = (int16_t)insn_get(env, s, MO_16);\n        }\n    do_jcc:\n        next_eip = s->pc - s->cs_base;\n        tval += next_eip;\n        if (dflag == MO_16) {\n            tval &= 0xffff;\n        }\n        gen_bnd_jmp(s);\n        gen_jcc(s, b, tval, next_eip);\n        break;\n\n    case 0x190 ... 0x19f: /* setcc Gv */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_setcc1(s, b, cpu_T0);\n        gen_ldst_modrm(env, s, modrm, MO_8, OR_TMP0, 1);\n        break;\n    case 0x140 ... 0x14f: /* cmov Gv, Ev */\n        if (!(s->cpuid_features & CPUID_CMOV)) {\n            goto illegal_op;\n        }\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_cmovcc1(env, s, ot, b, modrm, reg);\n        break;\n\n        /************************/\n        /* flags */\n    case 0x9c: /* pushf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_PUSHF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_helper_read_eflags(cpu_T0, cpu_env);\n            gen_push_v(s, cpu_T0);\n        }\n        break;\n    case 0x9d: /* popf */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_POPF);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            ot = gen_pop_T0(s);\n            if (s->cpl == 0) {\n                if (dflag != MO_16) {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK |\n                                                           IOPL_MASK)));\n                } else {\n                    gen_helper_write_eflags(cpu_env, cpu_T0,\n                                            tcg_const_i32((TF_MASK | AC_MASK |\n                                                           ID_MASK | NT_MASK |\n                                                           IF_MASK | IOPL_MASK)\n                                                          & 0xffff));\n                }\n            } else {\n                if (s->cpl <= s->iopl) {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                                tcg_const_i32((TF_MASK |\n                                                               AC_MASK |\n                                                               ID_MASK |\n                                                               NT_MASK |\n                                                               IF_MASK)\n                                                              & 0xffff));\n                    }\n                } else {\n                    if (dflag != MO_16) {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)));\n                    } else {\n                        gen_helper_write_eflags(cpu_env, cpu_T0,\n                                           tcg_const_i32((TF_MASK | AC_MASK |\n                                                          ID_MASK | NT_MASK)\n                                                         & 0xffff));\n                    }\n                }\n            }\n            gen_pop_update(s, ot);\n            set_cc_op(s, CC_OP_EFLAGS);\n            /* abort translation because TF/AC flag may change */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    case 0x9e: /* sahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_op_mov_v_reg(MO_8, cpu_T0, R_AH);\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, CC_O);\n        tcg_gen_andi_tl(cpu_T0, cpu_T0, CC_S | CC_Z | CC_A | CC_P | CC_C);\n        tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, cpu_T0);\n        break;\n    case 0x9f: /* lahf */\n        if (CODE64(s) && !(s->cpuid_ext3_features & CPUID_EXT3_LAHF_LM))\n            goto illegal_op;\n        gen_compute_eflags(s);\n        /* Note: gen_compute_eflags() only gives the condition codes */\n        tcg_gen_ori_tl(cpu_T0, cpu_cc_src, 0x02);\n        gen_op_mov_reg_v(MO_8, R_AH, cpu_T0);\n        break;\n    case 0xf5: /* cmc */\n        gen_compute_eflags(s);\n        tcg_gen_xori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xf8: /* clc */\n        gen_compute_eflags(s);\n        tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_C);\n        break;\n    case 0xf9: /* stc */\n        gen_compute_eflags(s);\n        tcg_gen_ori_tl(cpu_cc_src, cpu_cc_src, CC_C);\n        break;\n    case 0xfc: /* cld */\n        tcg_gen_movi_i32(cpu_tmp2_i32, 1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n    case 0xfd: /* std */\n        tcg_gen_movi_i32(cpu_tmp2_i32, -1);\n        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, df));\n        break;\n\n        /************************/\n        /* bit operations */\n    case 0x1ba: /* bt/bts/btr/btc Gv, im */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        op = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        if (mod != 3) {\n            s->rip_offset = 1;\n            gen_lea_modrm(env, s, modrm);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n        /* load shift */\n        val = cpu_ldub_code(env, s->pc++);\n        tcg_gen_movi_tl(cpu_T1, val);\n        if (op < 4)\n            goto unknown_op;\n        op -= 4;\n        goto bt_op;\n    case 0x1a3: /* bt Gv, Ev */\n        op = 0;\n        goto do_btx;\n    case 0x1ab: /* bts */\n        op = 1;\n        goto do_btx;\n    case 0x1b3: /* btr */\n        op = 2;\n        goto do_btx;\n    case 0x1bb: /* btc */\n        op = 3;\n    do_btx:\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        mod = (modrm >> 6) & 3;\n        rm = (modrm & 7) | REX_B(s);\n        gen_op_mov_v_reg(MO_32, cpu_T1, reg);\n        if (mod != 3) {\n            AddressParts a = gen_lea_modrm_0(env, s, modrm);\n            /* specific case: we need to add a displacement */\n            gen_exts(ot, cpu_T1);\n            tcg_gen_sari_tl(cpu_tmp0, cpu_T1, 3 + ot);\n            tcg_gen_shli_tl(cpu_tmp0, cpu_tmp0, ot);\n            tcg_gen_add_tl(cpu_A0, gen_lea_modrm_1(a), cpu_tmp0);\n            gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n            if (!(s->prefix & PREFIX_LOCK)) {\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n            }\n        } else {\n            gen_op_mov_v_reg(ot, cpu_T0, rm);\n        }\n    bt_op:\n        tcg_gen_andi_tl(cpu_T1, cpu_T1, (1 << (3 + ot)) - 1);\n        tcg_gen_movi_tl(cpu_tmp0, 1);\n        tcg_gen_shl_tl(cpu_tmp0, cpu_tmp0, cpu_T1);\n        if (s->prefix & PREFIX_LOCK) {\n            switch (op) {\n            case 0: /* bt */\n                /* Needs no atomic ops; we surpressed the normal\n                   memory load for LOCK above so do it now.  */\n                gen_op_ld_v(s, ot, cpu_T0, cpu_A0);\n                break;\n            case 1: /* bts */\n                tcg_gen_atomic_fetch_or_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                           s->mem_index, ot | MO_LE);\n                break;\n            case 2: /* btr */\n                tcg_gen_not_tl(cpu_tmp0, cpu_tmp0);\n                tcg_gen_atomic_fetch_and_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_atomic_fetch_xor_tl(cpu_T0, cpu_A0, cpu_tmp0,\n                                            s->mem_index, ot | MO_LE);\n                break;\n            }\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n        } else {\n            tcg_gen_shr_tl(cpu_tmp4, cpu_T0, cpu_T1);\n            switch (op) {\n            case 0: /* bt */\n                /* Data already loaded; nothing to do.  */\n                break;\n            case 1: /* bts */\n                tcg_gen_or_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            case 2: /* btr */\n                tcg_gen_andc_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            default:\n            case 3: /* btc */\n                tcg_gen_xor_tl(cpu_T0, cpu_T0, cpu_tmp0);\n                break;\n            }\n            if (op != 0) {\n                if (mod != 3) {\n                    gen_op_st_v(s, ot, cpu_T0, cpu_A0);\n                } else {\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n            }\n        }\n\n        /* Delay all CC updates until after the store above.  Note that\n           C is the result of the test, Z is unchanged, and the others\n           are all undefined.  */\n        switch (s->cc_op) {\n        case CC_OP_MULB ... CC_OP_MULQ:\n        case CC_OP_ADDB ... CC_OP_ADDQ:\n        case CC_OP_ADCB ... CC_OP_ADCQ:\n        case CC_OP_SUBB ... CC_OP_SUBQ:\n        case CC_OP_SBBB ... CC_OP_SBBQ:\n        case CC_OP_LOGICB ... CC_OP_LOGICQ:\n        case CC_OP_INCB ... CC_OP_INCQ:\n        case CC_OP_DECB ... CC_OP_DECQ:\n        case CC_OP_SHLB ... CC_OP_SHLQ:\n        case CC_OP_SARB ... CC_OP_SARQ:\n        case CC_OP_BMILGB ... CC_OP_BMILGQ:\n            /* Z was going to be computed from the non-zero status of CC_DST.\n               We can get that same Z value (and the new C value) by leaving\n               CC_DST alone, setting CC_SRC, and using a CC_OP_SAR of the\n               same width.  */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_tmp4);\n            set_cc_op(s, ((s->cc_op - CC_OP_MULB) & 3) + CC_OP_SARB);\n            break;\n        default:\n            /* Otherwise, generate EFLAGS and replace the C bit.  */\n            gen_compute_eflags(s);\n            tcg_gen_deposit_tl(cpu_cc_src, cpu_cc_src, cpu_tmp4,\n                               ctz32(CC_C), 1);\n            break;\n        }\n        break;\n    case 0x1bc: /* bsf / tzcnt */\n    case 0x1bd: /* bsr / lzcnt */\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n\n        /* Note that lzcnt and tzcnt are in different extensions.  */\n        if ((prefixes & PREFIX_REPZ)\n            && (b & 1\n                ? s->cpuid_ext3_features & CPUID_EXT3_ABM\n                : s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_BMI1)) {\n            int size = 8 << ot;\n            /* For lzcnt/tzcnt, C bit is defined related to the input. */\n            tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n            if (b & 1) {\n                /* For lzcnt, reduce the target_ulong result by the\n                   number of zeros that we expect to find at the top.  */\n                tcg_gen_clzi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS);\n                tcg_gen_subi_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - size);\n            } else {\n                /* For tzcnt, a zero input must return the operand size.  */\n                tcg_gen_ctzi_tl(cpu_T0, cpu_T0, size);\n            }\n            /* For lzcnt/tzcnt, Z bit is defined related to the result.  */\n            gen_op_update1_cc();\n            set_cc_op(s, CC_OP_BMILGB + ot);\n        } else {\n            /* For bsr/bsf, only the Z bit is defined and it is related\n               to the input and not the result.  */\n            tcg_gen_mov_tl(cpu_cc_dst, cpu_T0);\n            set_cc_op(s, CC_OP_LOGICB + ot);\n\n            /* ??? The manual says that the output is undefined when the\n               input is zero, but real hardware leaves it unchanged, and\n               real programs appear to depend on that.  Accomplish this\n               by passing the output as the value to return upon zero.  */\n            if (b & 1) {\n                /* For bsr, return the bit index of the first 1 bit,\n                   not the count of leading zeros.  */\n                tcg_gen_xori_tl(cpu_T1, cpu_regs[reg], TARGET_LONG_BITS - 1);\n                tcg_gen_clz_tl(cpu_T0, cpu_T0, cpu_T1);\n                tcg_gen_xori_tl(cpu_T0, cpu_T0, TARGET_LONG_BITS - 1);\n            } else {\n                tcg_gen_ctz_tl(cpu_T0, cpu_T0, cpu_regs[reg]);\n            }\n        }\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n        break;\n        /************************/\n        /* bcd */\n    case 0x27: /* daa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_daa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x2f: /* das */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_das(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x37: /* aaa */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aaa(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0x3f: /* aas */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_helper_aas(cpu_env);\n        set_cc_op(s, CC_OP_EFLAGS);\n        break;\n    case 0xd4: /* aam */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        if (val == 0) {\n            gen_exception(s, EXCP00_DIVZ, pc_start - s->cs_base);\n        } else {\n            gen_helper_aam(cpu_env, tcg_const_i32(val));\n            set_cc_op(s, CC_OP_LOGICB);\n        }\n        break;\n    case 0xd5: /* aad */\n        if (CODE64(s))\n            goto illegal_op;\n        val = cpu_ldub_code(env, s->pc++);\n        gen_helper_aad(cpu_env, tcg_const_i32(val));\n        set_cc_op(s, CC_OP_LOGICB);\n        break;\n        /************************/\n        /* misc */\n    case 0x90: /* nop */\n        /* XXX: correct lock test for all insn */\n        if (prefixes & PREFIX_LOCK) {\n            goto illegal_op;\n        }\n        /* If REX_B is set, then this is xchg eax, r8d, not a nop.  */\n        if (REX_B(s)) {\n            goto do_xchg_reg_eax;\n        }\n        if (prefixes & PREFIX_REPZ) {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_pause(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x9b: /* fwait */\n        if ((s->flags & (HF_MP_MASK | HF_TS_MASK)) ==\n            (HF_MP_MASK | HF_TS_MASK)) {\n            gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n        } else {\n            gen_helper_fwait(cpu_env);\n        }\n        break;\n    case 0xcc: /* int3 */\n        gen_interrupt(s, EXCP03_INT3, pc_start - s->cs_base, s->pc - s->cs_base);\n        break;\n    case 0xcd: /* int N */\n        val = cpu_ldub_code(env, s->pc++);\n        if (s->vm86 && s->iopl != 3) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_interrupt(s, val, pc_start - s->cs_base, s->pc - s->cs_base);\n        }\n        break;\n    case 0xce: /* into */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_into(cpu_env, tcg_const_i32(s->pc - pc_start));\n        break;\n#ifdef WANT_ICEBP\n    case 0xf1: /* icebp (undocumented, exits to external debugger) */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_ICEBP);\n#if 1\n        gen_debug(s, pc_start - s->cs_base);\n#else\n        /* start debug */\n        tb_flush(CPU(x86_env_get_cpu(env)));\n        qemu_set_log(CPU_LOG_INT | CPU_LOG_TB_IN_ASM);\n#endif\n        break;\n#endif\n    case 0xfa: /* cli */\n        if (!s->vm86) {\n            if (s->cpl <= s->iopl) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        } else {\n            if (s->iopl == 3) {\n                gen_helper_cli(cpu_env);\n            } else {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            }\n        }\n        break;\n    case 0xfb: /* sti */\n        if (s->vm86 ? s->iopl == 3 : s->cpl <= s->iopl) {\n            gen_helper_sti(cpu_env);\n            /* interruptions are enabled only the first insn after sti */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob_inhibit_irq(s, true);\n        } else {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        }\n        break;\n    case 0x62: /* bound */\n        if (CODE64(s))\n            goto illegal_op;\n        ot = dflag;\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = (modrm >> 3) & 7;\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_op_mov_v_reg(ot, cpu_T0, reg);\n        gen_lea_modrm(env, s, modrm);\n        tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n        if (ot == MO_16) {\n            gen_helper_boundw(cpu_env, cpu_A0, cpu_tmp2_i32);\n        } else {\n            gen_helper_boundl(cpu_env, cpu_A0, cpu_tmp2_i32);\n        }\n        break;\n    case 0x1c8 ... 0x1cf: /* bswap reg */\n        reg = (b & 7) | REX_B(s);\n#ifdef TARGET_X86_64\n        if (dflag == MO_64) {\n            gen_op_mov_v_reg(MO_64, cpu_T0, reg);\n            tcg_gen_bswap64_i64(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_64, reg, cpu_T0);\n        } else\n#endif\n        {\n            gen_op_mov_v_reg(MO_32, cpu_T0, reg);\n            tcg_gen_ext32u_tl(cpu_T0, cpu_T0);\n            tcg_gen_bswap32_tl(cpu_T0, cpu_T0);\n            gen_op_mov_reg_v(MO_32, reg, cpu_T0);\n        }\n        break;\n    case 0xd6: /* salc */\n        if (CODE64(s))\n            goto illegal_op;\n        gen_compute_eflags_c(s, cpu_T0);\n        tcg_gen_neg_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(MO_8, R_EAX, cpu_T0);\n        break;\n    case 0xe0: /* loopnz */\n    case 0xe1: /* loopz */\n    case 0xe2: /* loop */\n    case 0xe3: /* jecxz */\n        {\n            TCGLabel *l1, *l2, *l3;\n\n            tval = (int8_t)insn_get(env, s, MO_8);\n            next_eip = s->pc - s->cs_base;\n            tval += next_eip;\n            if (dflag == MO_16) {\n                tval &= 0xffff;\n            }\n\n            l1 = gen_new_label();\n            l2 = gen_new_label();\n            l3 = gen_new_label();\n            b &= 3;\n            switch(b) {\n            case 0: /* loopnz */\n            case 1: /* loopz */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jz_ecx(s->aflag, l3);\n                gen_jcc1(s, (JCC_Z << 1) | (b ^ 1), l1);\n                break;\n            case 2: /* loop */\n                gen_op_add_reg_im(s->aflag, R_ECX, -1);\n                gen_op_jnz_ecx(s->aflag, l1);\n                break;\n            default:\n            case 3: /* jcxz */\n                gen_op_jz_ecx(s->aflag, l1);\n                break;\n            }\n\n            gen_set_label(l3);\n            gen_jmp_im(next_eip);\n            tcg_gen_br(l2);\n\n            gen_set_label(l1);\n            gen_jmp_im(tval);\n            gen_set_label(l2);\n            gen_eob(s);\n        }\n        break;\n    case 0x130: /* wrmsr */\n    case 0x132: /* rdmsr */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (b & 2) {\n                gen_helper_rdmsr(cpu_env);\n            } else {\n                gen_helper_wrmsr(cpu_env);\n            }\n        }\n        break;\n    case 0x131: /* rdtsc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_start();\n\t}\n        gen_helper_rdtsc(cpu_env);\n        if (s->tb->cflags & CF_USE_ICOUNT) {\n            gen_io_end();\n            gen_jmp(s, s->pc - s->cs_base);\n        }\n        break;\n    case 0x133: /* rdpmc */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_rdpmc(cpu_env);\n        break;\n    case 0x134: /* sysenter */\n        /* For Intel SYSENTER is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysenter(cpu_env);\n            gen_eob(s);\n        }\n        break;\n    case 0x135: /* sysexit */\n        /* For Intel SYSEXIT is valid on 64-bit */\n        if (CODE64(s) && env->cpuid_vendor1 != CPUID_VENDOR_INTEL_1)\n            goto illegal_op;\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysexit(cpu_env, tcg_const_i32(dflag - 1));\n            gen_eob(s);\n        }\n        break;\n#ifdef TARGET_X86_64\n    case 0x105: /* syscall */\n        /* XXX: is it usable in real mode ? */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_syscall(cpu_env, tcg_const_i32(s->pc - pc_start));\n        /* TF handling for the syscall insn is different. The TF bit is  checked\n           after the syscall insn completes. This allows #DB to not be\n           generated after one has entered CPL0 if TF is set in FMASK.  */\n        gen_eob_worker(s, false, true);\n        break;\n    case 0x107: /* sysret */\n        if (!s->pe) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_helper_sysret(cpu_env, tcg_const_i32(dflag - 1));\n            /* condition codes are modified only in long mode */\n            if (s->lma) {\n                set_cc_op(s, CC_OP_EFLAGS);\n            }\n            /* TF handling for the sysret insn is different. The TF bit is\n               checked after the sysret insn completes. This allows #DB to be\n               generated \"as if\" the syscall insn in userspace has just\n               completed.  */\n            gen_eob_worker(s, false, true);\n        }\n        break;\n#endif\n    case 0x1a2: /* cpuid */\n        gen_update_cc_op(s);\n        gen_jmp_im(pc_start - s->cs_base);\n        gen_helper_cpuid(cpu_env);\n        break;\n    case 0xf4: /* hlt */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_hlt(cpu_env, tcg_const_i32(s->pc - pc_start));\n            s->is_jmp = DISAS_TB_JUMP;\n        }\n        break;\n    case 0x100:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* sldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, ldt.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 2: /* lldt */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_LDTR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_lldt(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 1: /* str */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_READ);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env,\n                             offsetof(CPUX86State, tr.selector));\n            ot = mod == 3 ? dflag : MO_16;\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 3: /* ltr */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_TR_WRITE);\n                gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T0);\n                gen_helper_ltr(cpu_env, cpu_tmp2_i32);\n            }\n            break;\n        case 4: /* verr */\n        case 5: /* verw */\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_update_cc_op(s);\n            if (op == 4) {\n                gen_helper_verr(cpu_env, cpu_T0);\n            } else {\n                gen_helper_verw(cpu_env, cpu_T0);\n            }\n            set_cc_op(s, CC_OP_EFLAGS);\n            break;\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x101:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* sgdt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0,\n                             cpu_env, offsetof(CPUX86State, gdt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xc8: /* monitor */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            tcg_gen_mov_tl(cpu_A0, cpu_regs[R_EAX]);\n            gen_extu(s->aflag, cpu_A0);\n            gen_add_A0_ds_seg(s);\n            gen_helper_monitor(cpu_env, cpu_A0);\n            break;\n\n        case 0xc9: /* mwait */\n            if (!(s->cpuid_ext_features & CPUID_EXT_MONITOR) || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_mwait(cpu_env, tcg_const_i32(s->pc - pc_start));\n            gen_eob(s);\n            break;\n\n        case 0xca: /* clac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_clac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xcb: /* stac */\n            if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_SMAP)\n                || s->cpl != 0) {\n                goto illegal_op;\n            }\n            gen_helper_stac(cpu_env);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* sidt */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_READ);\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.limit));\n            gen_op_st_v(s, MO_16, cpu_T0, cpu_A0);\n            gen_add_A0_im(s, 2);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            gen_op_st_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            break;\n\n        case 0xd0: /* xgetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xgetbv(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n\n        case 0xd1: /* xsetbv */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (s->prefix & (PREFIX_LOCK | PREFIX_DATA\n                                 | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_xsetbv(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            /* End TB because translation flags may change.  */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xd8: /* VMRUN */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmrun(cpu_env, tcg_const_i32(s->aflag - 1),\n                             tcg_const_i32(s->pc - pc_start));\n            tcg_gen_exit_tb(0);\n            s->is_jmp = DISAS_TB_JUMP;\n            break;\n\n        case 0xd9: /* VMMCALL */\n            if (!(s->flags & HF_SVME_MASK)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmmcall(cpu_env);\n            break;\n\n        case 0xda: /* VMLOAD */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmload(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdb: /* VMSAVE */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_vmsave(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        case 0xdc: /* STGI */\n            if ((!(s->flags & HF_SVME_MASK)\n                   && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_stgi(cpu_env);\n            break;\n\n        case 0xdd: /* CLGI */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_clgi(cpu_env);\n            break;\n\n        case 0xde: /* SKINIT */\n            if ((!(s->flags & HF_SVME_MASK)\n                 && !(s->cpuid_ext3_features & CPUID_EXT3_SKINIT))\n                || !s->pe) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_skinit(cpu_env);\n            break;\n\n        case 0xdf: /* INVLPGA */\n            if (!(s->flags & HF_SVME_MASK) || !s->pe) {\n                goto illegal_op;\n            }\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_helper_invlpga(cpu_env, tcg_const_i32(s->aflag - 1));\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* lgdt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_GDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, gdt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, gdt.limit));\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* lidt */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_IDTR_WRITE);\n            gen_lea_modrm(env, s, modrm);\n            gen_op_ld_v(s, MO_16, cpu_T1, cpu_A0);\n            gen_add_A0_im(s, 2);\n            gen_op_ld_v(s, CODE64(s) + MO_32, cpu_T0, cpu_A0);\n            if (dflag == MO_16) {\n                tcg_gen_andi_tl(cpu_T0, cpu_T0, 0xffffff);\n            }\n            tcg_gen_st_tl(cpu_T0, cpu_env, offsetof(CPUX86State, idt.base));\n            tcg_gen_st32_tl(cpu_T1, cpu_env, offsetof(CPUX86State, idt.limit));\n            break;\n\n        CASE_MODRM_OP(4): /* smsw */\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_CR0);\n            tcg_gen_ld_tl(cpu_T0, cpu_env, offsetof(CPUX86State, cr[0]));\n            if (CODE64(s)) {\n                mod = (modrm >> 6) & 3;\n                ot = (mod != 3 ? MO_16 : s->dflag);\n            } else {\n                ot = MO_16;\n            }\n            gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 1);\n            break;\n        case 0xee: /* rdpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_rdpkru(cpu_tmp1_i64, cpu_env, cpu_tmp2_i32);\n            tcg_gen_extr_i64_tl(cpu_regs[R_EAX], cpu_regs[R_EDX], cpu_tmp1_i64);\n            break;\n        case 0xef: /* wrpkru */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_regs[R_ECX]);\n            gen_helper_wrpkru(cpu_env, cpu_tmp2_i32, cpu_tmp1_i64);\n            break;\n        CASE_MODRM_OP(6): /* lmsw */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            gen_helper_lmsw(cpu_env, cpu_T0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* invlpg */\n            if (s->cpl != 0) {\n                gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                break;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_invlpg(cpu_env, cpu_A0);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        case 0xf8: /* swapgs */\n#ifdef TARGET_X86_64\n            if (CODE64(s)) {\n                if (s->cpl != 0) {\n                    gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n                } else {\n                    tcg_gen_mov_tl(cpu_T0, cpu_seg_base[R_GS]);\n                    tcg_gen_ld_tl(cpu_seg_base[R_GS], cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                    tcg_gen_st_tl(cpu_T0, cpu_env,\n                                  offsetof(CPUX86State, kernelgsbase));\n                }\n                break;\n            }\n#endif\n            goto illegal_op;\n\n        case 0xf9: /* rdtscp */\n            if (!(s->cpuid_ext2_features & CPUID_EXT2_RDTSCP)) {\n                goto illegal_op;\n            }\n            gen_update_cc_op(s);\n            gen_jmp_im(pc_start - s->cs_base);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_start();\n            }\n            gen_helper_rdtscp(cpu_env);\n            if (s->tb->cflags & CF_USE_ICOUNT) {\n                gen_io_end();\n                gen_jmp(s, s->pc - s->cs_base);\n            }\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x108: /* invd */\n    case 0x109: /* wbinvd */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, (b & 2) ? SVM_EXIT_INVD : SVM_EXIT_WBINVD);\n            /* nothing to do */\n        }\n        break;\n    case 0x63: /* arpl or movslS (x86_64) */\n#ifdef TARGET_X86_64\n        if (CODE64(s)) {\n            int d_ot;\n            /* d_ot is the size of destination */\n            d_ot = dflag;\n\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            mod = (modrm >> 6) & 3;\n            rm = (modrm & 7) | REX_B(s);\n\n            if (mod == 3) {\n                gen_op_mov_v_reg(MO_32, cpu_T0, rm);\n                /* sign extend */\n                if (d_ot == MO_64) {\n                    tcg_gen_ext32s_tl(cpu_T0, cpu_T0);\n                }\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            } else {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, MO_32 | MO_SIGN, cpu_T0, cpu_A0);\n                gen_op_mov_reg_v(d_ot, reg, cpu_T0);\n            }\n        } else\n#endif\n        {\n            TCGLabel *label1;\n            TCGv t0, t1, t2, a0;\n\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            t0 = tcg_temp_local_new();\n            t1 = tcg_temp_local_new();\n            t2 = tcg_temp_local_new();\n            ot = MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = (modrm >> 3) & 7;\n            mod = (modrm >> 6) & 3;\n            rm = modrm & 7;\n            if (mod != 3) {\n                gen_lea_modrm(env, s, modrm);\n                gen_op_ld_v(s, ot, t0, cpu_A0);\n                a0 = tcg_temp_local_new();\n                tcg_gen_mov_tl(a0, cpu_A0);\n            } else {\n                gen_op_mov_v_reg(ot, t0, rm);\n                TCGV_UNUSED(a0);\n            }\n            gen_op_mov_v_reg(ot, t1, reg);\n            tcg_gen_andi_tl(cpu_tmp0, t0, 3);\n            tcg_gen_andi_tl(t1, t1, 3);\n            tcg_gen_movi_tl(t2, 0);\n            label1 = gen_new_label();\n            tcg_gen_brcond_tl(TCG_COND_GE, cpu_tmp0, t1, label1);\n            tcg_gen_andi_tl(t0, t0, ~3);\n            tcg_gen_or_tl(t0, t0, t1);\n            tcg_gen_movi_tl(t2, CC_Z);\n            gen_set_label(label1);\n            if (mod != 3) {\n                gen_op_st_v(s, ot, t0, a0);\n                tcg_temp_free(a0);\n           } else {\n                gen_op_mov_reg_v(ot, rm, t0);\n            }\n            gen_compute_eflags(s);\n            tcg_gen_andi_tl(cpu_cc_src, cpu_cc_src, ~CC_Z);\n            tcg_gen_or_tl(cpu_cc_src, cpu_cc_src, t2);\n            tcg_temp_free(t0);\n            tcg_temp_free(t1);\n            tcg_temp_free(t2);\n        }\n        break;\n    case 0x102: /* lar */\n    case 0x103: /* lsl */\n        {\n            TCGLabel *label1;\n            TCGv t0;\n            if (!s->pe || s->vm86)\n                goto illegal_op;\n            ot = dflag != MO_16 ? MO_32 : MO_16;\n            modrm = cpu_ldub_code(env, s->pc++);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            gen_ldst_modrm(env, s, modrm, MO_16, OR_TMP0, 0);\n            t0 = tcg_temp_local_new();\n            gen_update_cc_op(s);\n            if (b == 0x102) {\n                gen_helper_lar(t0, cpu_env, cpu_T0);\n            } else {\n                gen_helper_lsl(t0, cpu_env, cpu_T0);\n            }\n            tcg_gen_andi_tl(cpu_tmp0, cpu_cc_src, CC_Z);\n            label1 = gen_new_label();\n            tcg_gen_brcondi_tl(TCG_COND_EQ, cpu_tmp0, 0, label1);\n            gen_op_mov_reg_v(ot, reg, t0);\n            gen_set_label(label1);\n            set_cc_op(s, CC_OP_EFLAGS);\n            tcg_temp_free(t0);\n        }\n        break;\n    case 0x118:\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        op = (modrm >> 3) & 7;\n        switch(op) {\n        case 0: /* prefetchnta */\n        case 1: /* prefetchnt0 */\n        case 2: /* prefetchnt0 */\n        case 3: /* prefetchnt0 */\n            if (mod == 3)\n                goto illegal_op;\n            gen_nop_modrm(env, s, modrm);\n            /* nothing more to do */\n            break;\n        default: /* nop (multi byte) */\n            gen_nop_modrm(env, s, modrm);\n            break;\n        }\n        break;\n    case 0x11a:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (prefixes & PREFIX_REPZ) {\n                /* bndcl */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_LTU, cpu_bndl[reg]);\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcu */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                TCGv_i64 notu = tcg_temp_new_i64();\n                tcg_gen_not_i64(notu, cpu_bndu[reg]);\n                gen_bndck(env, s, modrm, TCG_COND_GTU, notu);\n                tcg_temp_free_i64(notu);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- from reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg], cpu_bndl[reg2]);\n                        tcg_gen_mov_i64(cpu_bndu[reg], cpu_bndu[reg2]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_ld_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_ld_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                    /* bnd registers are now in-use */\n                    gen_set_hflag(s, HF_MPX_IU_MASK);\n                }\n            } else if (mod != 3) {\n                /* bndldx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndldx64(cpu_bndl[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ld_i64(cpu_bndu[reg], cpu_env,\n                                   offsetof(CPUX86State, mmx_t0.MMX_Q(0)));\n                } else {\n                    gen_helper_bndldx32(cpu_bndu[reg], cpu_env, cpu_A0, cpu_T0);\n                    tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndu[reg]);\n                    tcg_gen_shri_i64(cpu_bndu[reg], cpu_bndu[reg], 32);\n                }\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x11b:\n        modrm = cpu_ldub_code(env, s->pc++);\n        if (s->flags & HF_MPX_EN_MASK) {\n            mod = (modrm >> 6) & 3;\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (mod != 3 && (prefixes & PREFIX_REPZ)) {\n                /* bndmk */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (a.base >= 0) {\n                    tcg_gen_extu_tl_i64(cpu_bndl[reg], cpu_regs[a.base]);\n                    if (!CODE64(s)) {\n                        tcg_gen_ext32u_i64(cpu_bndl[reg], cpu_bndl[reg]);\n                    }\n                } else if (a.base == -1) {\n                    /* no base register has lower bound of 0 */\n                    tcg_gen_movi_i64(cpu_bndl[reg], 0);\n                } else {\n                    /* rip-relative generates #ud */\n                    goto illegal_op;\n                }\n                tcg_gen_not_tl(cpu_A0, gen_lea_modrm_1(a));\n                if (!CODE64(s)) {\n                    tcg_gen_ext32u_tl(cpu_A0, cpu_A0);\n                }\n                tcg_gen_extu_tl_i64(cpu_bndu[reg], cpu_A0);\n                /* bnd registers are now in-use */\n                gen_set_hflag(s, HF_MPX_IU_MASK);\n                break;\n            } else if (prefixes & PREFIX_REPNZ) {\n                /* bndcn */\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                gen_bndck(env, s, modrm, TCG_COND_GTU, cpu_bndu[reg]);\n            } else if (prefixes & PREFIX_DATA) {\n                /* bndmov -- to reg/mem */\n                if (reg >= 4 || s->aflag == MO_16) {\n                    goto illegal_op;\n                }\n                if (mod == 3) {\n                    int reg2 = (modrm & 7) | REX_B(s);\n                    if (reg2 >= 4 || (prefixes & PREFIX_LOCK)) {\n                        goto illegal_op;\n                    }\n                    if (s->flags & HF_MPX_IU_MASK) {\n                        tcg_gen_mov_i64(cpu_bndl[reg2], cpu_bndl[reg]);\n                        tcg_gen_mov_i64(cpu_bndu[reg2], cpu_bndu[reg]);\n                    }\n                } else {\n                    gen_lea_modrm(env, s, modrm);\n                    if (CODE64(s)) {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 8);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEQ);\n                    } else {\n                        tcg_gen_qemu_st_i64(cpu_bndl[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                        tcg_gen_addi_tl(cpu_A0, cpu_A0, 4);\n                        tcg_gen_qemu_st_i64(cpu_bndu[reg], cpu_A0,\n                                            s->mem_index, MO_LEUL);\n                    }\n                }\n            } else if (mod != 3) {\n                /* bndstx */\n                AddressParts a = gen_lea_modrm_0(env, s, modrm);\n                if (reg >= 4\n                    || (prefixes & PREFIX_LOCK)\n                    || s->aflag == MO_16\n                    || a.base < -1) {\n                    goto illegal_op;\n                }\n                if (a.base >= 0) {\n                    tcg_gen_addi_tl(cpu_A0, cpu_regs[a.base], a.disp);\n                } else {\n                    tcg_gen_movi_tl(cpu_A0, 0);\n                }\n                gen_lea_v_seg(s, s->aflag, cpu_A0, a.def_seg, s->override);\n                if (a.index >= 0) {\n                    tcg_gen_mov_tl(cpu_T0, cpu_regs[a.index]);\n                } else {\n                    tcg_gen_movi_tl(cpu_T0, 0);\n                }\n                if (CODE64(s)) {\n                    gen_helper_bndstx64(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                } else {\n                    gen_helper_bndstx32(cpu_env, cpu_A0, cpu_T0,\n                                        cpu_bndl[reg], cpu_bndu[reg]);\n                }\n            }\n        }\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x119: case 0x11c ... 0x11f: /* nop (multi byte) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x120: /* mov reg, crN */\n    case 0x122: /* mov crN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if ((prefixes & PREFIX_LOCK) && (reg == 0) &&\n                (s->cpuid_ext3_features & CPUID_EXT3_CR8LEG)) {\n                reg = 8;\n            }\n            switch(reg) {\n            case 0:\n            case 2:\n            case 3:\n            case 4:\n            case 8:\n                gen_update_cc_op(s);\n                gen_jmp_im(pc_start - s->cs_base);\n                if (b & 2) {\n                    gen_op_mov_v_reg(ot, cpu_T0, rm);\n                    gen_helper_write_crN(cpu_env, tcg_const_i32(reg),\n                                         cpu_T0);\n                    gen_jmp_im(s->pc - s->cs_base);\n                    gen_eob(s);\n                } else {\n                    gen_helper_read_crN(cpu_T0, cpu_env, tcg_const_i32(reg));\n                    gen_op_mov_reg_v(ot, rm, cpu_T0);\n                }\n                break;\n            default:\n                goto unknown_op;\n            }\n        }\n        break;\n    case 0x121: /* mov reg, drN */\n    case 0x123: /* mov drN, reg */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            modrm = cpu_ldub_code(env, s->pc++);\n            /* Ignore the mod bits (assume (modrm&0xc0)==0xc0).\n             * AMD documentation (24594.pdf) and testing of\n             * intel 386 and 486 processors all show that the mod bits\n             * are assumed to be 1's, regardless of actual values.\n             */\n            rm = (modrm & 7) | REX_B(s);\n            reg = ((modrm >> 3) & 7) | rex_r;\n            if (CODE64(s))\n                ot = MO_64;\n            else\n                ot = MO_32;\n            if (reg >= 8) {\n                goto illegal_op;\n            }\n            if (b & 2) {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_DR0 + reg);\n                gen_op_mov_v_reg(ot, cpu_T0, rm);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_set_dr(cpu_env, cpu_tmp2_i32, cpu_T0);\n                gen_jmp_im(s->pc - s->cs_base);\n                gen_eob(s);\n            } else {\n                gen_svm_check_intercept(s, pc_start, SVM_EXIT_READ_DR0 + reg);\n                tcg_gen_movi_i32(cpu_tmp2_i32, reg);\n                gen_helper_get_dr(cpu_T0, cpu_env, cpu_tmp2_i32);\n                gen_op_mov_reg_v(ot, rm, cpu_T0);\n            }\n        }\n        break;\n    case 0x106: /* clts */\n        if (s->cpl != 0) {\n            gen_exception(s, EXCP0D_GPF, pc_start - s->cs_base);\n        } else {\n            gen_svm_check_intercept(s, pc_start, SVM_EXIT_WRITE_CR0);\n            gen_helper_clts(cpu_env);\n            /* abort block because static cpu state changed */\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n        }\n        break;\n    /* MMX/3DNow!/SSE/SSE2/SSE3/SSSE3/SSE4 support */\n    case 0x1c3: /* MOVNTI reg, mem */\n        if (!(s->cpuid_features & CPUID_SSE2))\n            goto illegal_op;\n        ot = mo_64_32(dflag);\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        reg = ((modrm >> 3) & 7) | rex_r;\n        /* generate a generic store */\n        gen_ldst_modrm(env, s, modrm, ot, reg, 1);\n        break;\n    case 0x1ae:\n        modrm = cpu_ldub_code(env, s->pc++);\n        switch (modrm) {\n        CASE_MODRM_MEM_OP(0): /* fxsave */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxsave(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(1): /* fxrstor */\n            if (!(s->cpuid_features & CPUID_FXSR)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            if ((s->flags & HF_EM_MASK) || (s->flags & HF_TS_MASK)) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            gen_helper_fxrstor(cpu_env, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(2): /* ldmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_qemu_ld_i32(cpu_tmp2_i32, cpu_A0, s->mem_index, MO_LEUL);\n            gen_helper_ldmxcsr(cpu_env, cpu_tmp2_i32);\n            break;\n\n        CASE_MODRM_MEM_OP(3): /* stmxcsr */\n            if ((s->flags & HF_EM_MASK) || !(s->flags & HF_OSFXSR_MASK)) {\n                goto illegal_op;\n            }\n            if (s->flags & HF_TS_MASK) {\n                gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);\n                break;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_ld32u_tl(cpu_T0, cpu_env, offsetof(CPUX86State, mxcsr));\n            gen_op_st_v(s, MO_32, cpu_T0, cpu_A0);\n            break;\n\n        CASE_MODRM_MEM_OP(4): /* xsave */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xsave(cpu_env, cpu_A0, cpu_tmp1_i64);\n            break;\n\n        CASE_MODRM_MEM_OP(5): /* xrstor */\n            if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                || (prefixes & (PREFIX_LOCK | PREFIX_DATA\n                                | PREFIX_REPZ | PREFIX_REPNZ))) {\n                goto illegal_op;\n            }\n            gen_lea_modrm(env, s, modrm);\n            tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                  cpu_regs[R_EDX]);\n            gen_helper_xrstor(cpu_env, cpu_A0, cpu_tmp1_i64);\n            /* XRSTOR is how MPX is enabled, which changes how\n               we translate.  Thus we need to end the TB.  */\n            gen_update_cc_op(s);\n            gen_jmp_im(s->pc - s->cs_base);\n            gen_eob(s);\n            break;\n\n        CASE_MODRM_MEM_OP(6): /* xsaveopt / clwb */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clwb */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLWB)) {\n                    goto illegal_op;\n                }\n                gen_nop_modrm(env, s, modrm);\n            } else {\n                /* xsaveopt */\n                if ((s->cpuid_ext_features & CPUID_EXT_XSAVE) == 0\n                    || (s->cpuid_xsave_features & CPUID_XSAVE_XSAVEOPT) == 0\n                    || (prefixes & (PREFIX_REPZ | PREFIX_REPNZ))) {\n                    goto illegal_op;\n                }\n                gen_lea_modrm(env, s, modrm);\n                tcg_gen_concat_tl_i64(cpu_tmp1_i64, cpu_regs[R_EAX],\n                                      cpu_regs[R_EDX]);\n                gen_helper_xsaveopt(cpu_env, cpu_A0, cpu_tmp1_i64);\n            }\n            break;\n\n        CASE_MODRM_MEM_OP(7): /* clflush / clflushopt */\n            if (prefixes & PREFIX_LOCK) {\n                goto illegal_op;\n            }\n            if (prefixes & PREFIX_DATA) {\n                /* clflushopt */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_CLFLUSHOPT)) {\n                    goto illegal_op;\n                }\n            } else {\n                /* clflush */\n                if ((s->prefix & (PREFIX_REPZ | PREFIX_REPNZ))\n                    || !(s->cpuid_features & CPUID_CLFLUSH)) {\n                    goto illegal_op;\n                }\n            }\n            gen_nop_modrm(env, s, modrm);\n            break;\n\n        case 0xc0 ... 0xc7: /* rdfsbase (f3 0f ae /0) */\n        case 0xc8 ... 0xc8: /* rdgsbase (f3 0f ae /1) */\n        case 0xd0 ... 0xd7: /* wrfsbase (f3 0f ae /2) */\n        case 0xd8 ... 0xd8: /* wrgsbase (f3 0f ae /3) */\n            if (CODE64(s)\n                && (prefixes & PREFIX_REPZ)\n                && !(prefixes & PREFIX_LOCK)\n                && (s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_FSGSBASE)) {\n                TCGv base, treg, src, dst;\n\n                /* Preserve hflags bits by testing CR4 at runtime.  */\n                tcg_gen_movi_i32(cpu_tmp2_i32, CR4_FSGSBASE_MASK);\n                gen_helper_cr4_testbit(cpu_env, cpu_tmp2_i32);\n\n                base = cpu_seg_base[modrm & 8 ? R_GS : R_FS];\n                treg = cpu_regs[(modrm & 7) | REX_B(s)];\n\n                if (modrm & 0x10) {\n                    /* wr*base */\n                    dst = base, src = treg;\n                } else {\n                    /* rd*base */\n                    dst = treg, src = base;\n                }\n\n                if (s->dflag == MO_32) {\n                    tcg_gen_ext32u_tl(dst, src);\n                } else {\n                    tcg_gen_mov_tl(dst, src);\n                }\n                break;\n            }\n            goto unknown_op;\n\n        case 0xf8: /* sfence / pcommit */\n            if (prefixes & PREFIX_DATA) {\n                /* pcommit */\n                if (!(s->cpuid_7_0_ebx_features & CPUID_7_0_EBX_PCOMMIT)\n                    || (prefixes & PREFIX_LOCK)) {\n                    goto illegal_op;\n                }\n                break;\n            }\n            /* fallthru */\n        case 0xf9 ... 0xff: /* sfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ST_ST | TCG_BAR_SC);\n            break;\n        case 0xe8 ... 0xef: /* lfence */\n            if (!(s->cpuid_features & CPUID_SSE)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_LD_LD | TCG_BAR_SC);\n            break;\n        case 0xf0 ... 0xf7: /* mfence */\n            if (!(s->cpuid_features & CPUID_SSE2)\n                || (prefixes & PREFIX_LOCK)) {\n                goto illegal_op;\n            }\n            tcg_gen_mb(TCG_MO_ALL | TCG_BAR_SC);\n            break;\n\n        default:\n            goto unknown_op;\n        }\n        break;\n\n    case 0x10d: /* 3DNow! prefetch(w) */\n        modrm = cpu_ldub_code(env, s->pc++);\n        mod = (modrm >> 6) & 3;\n        if (mod == 3)\n            goto illegal_op;\n        gen_nop_modrm(env, s, modrm);\n        break;\n    case 0x1aa: /* rsm */\n        gen_svm_check_intercept(s, pc_start, SVM_EXIT_RSM);\n        if (!(s->flags & HF_SMM_MASK))\n            goto illegal_op;\n        gen_update_cc_op(s);\n        gen_jmp_im(s->pc - s->cs_base);\n        gen_helper_rsm(cpu_env);\n        gen_eob(s);\n        break;\n    case 0x1b8: /* SSE4.2 popcnt */\n        if ((prefixes & (PREFIX_REPZ | PREFIX_LOCK | PREFIX_REPNZ)) !=\n             PREFIX_REPZ)\n            goto illegal_op;\n        if (!(s->cpuid_ext_features & CPUID_EXT_POPCNT))\n            goto illegal_op;\n\n        modrm = cpu_ldub_code(env, s->pc++);\n        reg = ((modrm >> 3) & 7) | rex_r;\n\n        if (s->prefix & PREFIX_DATA) {\n            ot = MO_16;\n        } else {\n            ot = mo_64_32(dflag);\n        }\n\n        gen_ldst_modrm(env, s, modrm, ot, OR_TMP0, 0);\n        gen_extu(ot, cpu_T0);\n        tcg_gen_mov_tl(cpu_cc_src, cpu_T0);\n        tcg_gen_ctpop_tl(cpu_T0, cpu_T0);\n        gen_op_mov_reg_v(ot, reg, cpu_T0);\n\n        set_cc_op(s, CC_OP_POPCNT);\n        break;\n    case 0x10e ... 0x10f:\n        /* 3DNow! instructions, ignore prefixes */\n        s->prefix &= ~(PREFIX_REPZ | PREFIX_REPNZ | PREFIX_DATA);\n    case 0x110 ... 0x117:\n    case 0x128 ... 0x12f:\n    case 0x138 ... 0x13a:\n    case 0x150 ... 0x179:\n    case 0x17c ... 0x17f:\n    case 0x1c2:\n    case 0x1c4 ... 0x1c6:\n    case 0x1d0 ... 0x1fe:\n        gen_sse(env, s, b, pc_start, rex_r);\n        break;\n    default:\n        goto unknown_op;\n    }\n    return s->pc;\n illegal_op:\n    gen_illegal_opcode(s);\n    return s->pc;\n unknown_op:\n    gen_unknown_opcode(env, s);\n    return s->pc;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 144426,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "mrb_vm_exec(mrb_state *mrb, const struct RProc *proc, const mrb_code *pc)\n{\n  /* mrb_assert(MRB_PROC_CFUNC_P(proc)) */\n  const mrb_irep *irep = proc->body.irep;\n  const mrb_pool_value *pool = irep->pool;\n  const mrb_sym *syms = irep->syms;\n  mrb_code insn;\n  int ai = mrb_gc_arena_save(mrb);\n  struct mrb_jmpbuf *prev_jmp = mrb->jmp;\n  struct mrb_jmpbuf c_jmp;\n  uint32_t a;\n  uint16_t b;\n  uint16_t c;\n  mrb_sym mid;\n  const struct mrb_irep_catch_handler *ch;\n\n#ifdef DIRECT_THREADED\n  static const void * const optable[] = {\n#define OPCODE(x,_) &&L_OP_ ## x,\n#include \"mruby/ops.h\"\n#undef OPCODE\n  };\n#endif\n\n  mrb_bool exc_catched = FALSE;\nRETRY_TRY_BLOCK:\n\n  MRB_TRY(&c_jmp) {\n\n  if (exc_catched) {\n    exc_catched = FALSE;\n    mrb_gc_arena_restore(mrb, ai);\n    if (mrb->exc && mrb->exc->tt == MRB_TT_BREAK)\n      goto L_BREAK;\n    goto L_RAISE;\n  }\n  mrb->jmp = &c_jmp;\n  mrb_vm_ci_proc_set(mrb->c->ci, proc);\n\n#define regs (mrb->c->ci->stack)\n  INIT_DISPATCH {\n    CASE(OP_NOP, Z) {\n      /* do nothing */\n      NEXT;\n    }\n\n    CASE(OP_MOVE, BB) {\n      regs[a] = regs[b];\n      NEXT;\n    }\n\n    CASE(OP_LOADL, BB) {\n      switch (pool[b].tt) {   /* number */\n      case IREP_TT_INT32:\n        regs[a] = mrb_int_value(mrb, (mrb_int)pool[b].u.i32);\n        break;\n      case IREP_TT_INT64:\n#if defined(MRB_INT64)\n        regs[a] = mrb_int_value(mrb, (mrb_int)pool[b].u.i64);\n        break;\n#else\n#if defined(MRB_64BIT)\n        if (INT32_MIN <= pool[b].u.i64 && pool[b].u.i64 <= INT32_MAX) {\n          regs[a] = mrb_int_value(mrb, (mrb_int)pool[b].u.i64);\n          break;\n        }\n#endif\n        goto L_INT_OVERFLOW;\n#endif\n      case IREP_TT_BIGINT:\n        goto L_INT_OVERFLOW;\n#ifndef MRB_NO_FLOAT\n      case IREP_TT_FLOAT:\n        regs[a] = mrb_float_value(mrb, pool[b].u.f);\n        break;\n#endif\n      default:\n        /* should not happen (tt:string) */\n        regs[a] = mrb_nil_value();\n        break;\n      }\n      NEXT;\n    }\n\n    CASE(OP_LOADI, BB) {\n      SET_FIXNUM_VALUE(regs[a], b);\n      NEXT;\n    }\n\n    CASE(OP_LOADINEG, BB) {\n      SET_FIXNUM_VALUE(regs[a], -b);\n      NEXT;\n    }\n\n    CASE(OP_LOADI__1,B) goto L_LOADI;\n    CASE(OP_LOADI_0,B) goto L_LOADI;\n    CASE(OP_LOADI_1,B) goto L_LOADI;\n    CASE(OP_LOADI_2,B) goto L_LOADI;\n    CASE(OP_LOADI_3,B) goto L_LOADI;\n    CASE(OP_LOADI_4,B) goto L_LOADI;\n    CASE(OP_LOADI_5,B) goto L_LOADI;\n    CASE(OP_LOADI_6,B) goto L_LOADI;\n    CASE(OP_LOADI_7, B) {\n    L_LOADI:\n      SET_FIXNUM_VALUE(regs[a], (mrb_int)insn - (mrb_int)OP_LOADI_0);\n      NEXT;\n    }\n\n    CASE(OP_LOADI16, BS) {\n      SET_FIXNUM_VALUE(regs[a], (mrb_int)(int16_t)b);\n      NEXT;\n    }\n\n    CASE(OP_LOADI32, BSS) {\n      SET_INT_VALUE(mrb, regs[a], (int32_t)(((uint32_t)b<<16)+c));\n      NEXT;\n    }\n\n    CASE(OP_LOADSYM, BB) {\n      SET_SYM_VALUE(regs[a], syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_LOADNIL, B) {\n      SET_NIL_VALUE(regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_LOADSELF, B) {\n      regs[a] = regs[0];\n      NEXT;\n    }\n\n    CASE(OP_LOADT, B) {\n      SET_TRUE_VALUE(regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_LOADF, B) {\n      SET_FALSE_VALUE(regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETGV, BB) {\n      mrb_value val = mrb_gv_get(mrb, syms[b]);\n      regs[a] = val;\n      NEXT;\n    }\n\n    CASE(OP_SETGV, BB) {\n      mrb_gv_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETSV, BB) {\n      mrb_value val = mrb_vm_special_get(mrb, syms[b]);\n      regs[a] = val;\n      NEXT;\n    }\n\n    CASE(OP_SETSV, BB) {\n      mrb_vm_special_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETIV, BB) {\n      regs[a] = mrb_iv_get(mrb, regs[0], syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_SETIV, BB) {\n      mrb_iv_set(mrb, regs[0], syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETCV, BB) {\n      mrb_value val;\n      val = mrb_vm_cv_get(mrb, syms[b]);\n      regs[a] = val;\n      NEXT;\n    }\n\n    CASE(OP_SETCV, BB) {\n      mrb_vm_cv_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETIDX, B) {\n      mrb_value va = regs[a], vb = regs[a+1];\n      switch (mrb_type(va)) {\n      case MRB_TT_ARRAY:\n        if (!mrb_integer_p(vb)) goto getidx_fallback;\n        regs[a] = mrb_ary_entry(va, mrb_integer(vb));\n        break;\n      case MRB_TT_HASH:\n        regs[a] = mrb_hash_get(mrb, va, vb);\n        break;\n      case MRB_TT_STRING:\n        switch (mrb_type(vb)) {\n        case MRB_TT_INTEGER:\n        case MRB_TT_STRING:\n        case MRB_TT_RANGE:\n          regs[a] = mrb_str_aref(mrb, va, vb, mrb_undef_value());\n          break;\n        default:\n          goto getidx_fallback;\n        }\n        break;\n      default:\n      getidx_fallback:\n        mid = MRB_OPSYM(aref);\n        goto L_SEND_SYM;\n      }\n      NEXT;\n    }\n\n    CASE(OP_SETIDX, B) {\n      c = 2;\n      mid = MRB_OPSYM(aset);\n      SET_NIL_VALUE(regs[a+3]);\n      goto L_SENDB_SYM;\n    }\n\n    CASE(OP_GETCONST, BB) {\n      regs[a] = mrb_vm_const_get(mrb, syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_SETCONST, BB) {\n      mrb_vm_const_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETMCNST, BB) {\n      regs[a] = mrb_const_get(mrb, regs[a], syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_SETMCNST, BB) {\n      mrb_const_set(mrb, regs[a+1], syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETUPVAR, BBB) {\n      mrb_value *regs_a = regs + a;\n      struct REnv *e = uvenv(mrb, c);\n\n      if (e && b < MRB_ENV_LEN(e)) {\n        *regs_a = e->stack[b];\n      }\n      else {\n        *regs_a = mrb_nil_value();\n      }\n      NEXT;\n    }\n\n    CASE(OP_SETUPVAR, BBB) {\n      struct REnv *e = uvenv(mrb, c);\n\n      if (e) {\n        mrb_value *regs_a = regs + a;\n\n        if (b < MRB_ENV_LEN(e)) {\n          e->stack[b] = *regs_a;\n          mrb_write_barrier(mrb, (struct RBasic*)e);\n        }\n      }\n      NEXT;\n    }\n\n    CASE(OP_JMP, S) {\n      pc += (int16_t)a;\n      JUMP;\n    }\n    CASE(OP_JMPIF, BS) {\n      if (mrb_test(regs[a])) {\n        pc += (int16_t)b;\n        JUMP;\n      }\n      NEXT;\n    }\n    CASE(OP_JMPNOT, BS) {\n      if (!mrb_test(regs[a])) {\n        pc += (int16_t)b;\n        JUMP;\n      }\n      NEXT;\n    }\n    CASE(OP_JMPNIL, BS) {\n      if (mrb_nil_p(regs[a])) {\n        pc += (int16_t)b;\n        JUMP;\n      }\n      NEXT;\n    }\n\n    CASE(OP_JMPUW, S) {\n      a = (uint32_t)((pc - irep->iseq) + (int16_t)a);\n      CHECKPOINT_RESTORE(RBREAK_TAG_JUMP) {\n        struct RBreak *brk = (struct RBreak*)mrb->exc;\n        mrb_value target = mrb_break_value_get(brk);\n        mrb_assert(mrb_integer_p(target));\n        a = (uint32_t)mrb_integer(target);\n        mrb_assert(a >= 0 && a < irep->ilen);\n      }\n      CHECKPOINT_MAIN(RBREAK_TAG_JUMP) {\n        ch = catch_handler_find(mrb, mrb->c->ci, pc, MRB_CATCH_FILTER_ENSURE);\n        if (ch) {\n          /* avoiding a jump from a catch handler into the same handler */\n          if (a < mrb_irep_catch_handler_unpack(ch->begin) || a >= mrb_irep_catch_handler_unpack(ch->end)) {\n            THROW_TAGGED_BREAK(mrb, RBREAK_TAG_JUMP, proc, mrb_fixnum_value(a));\n          }\n        }\n      }\n      CHECKPOINT_END(RBREAK_TAG_JUMP);\n\n      mrb->exc = NULL; /* clear break object */\n      pc = irep->iseq + a;\n      JUMP;\n    }\n\n    CASE(OP_EXCEPT, B) {\n      mrb_value exc;\n\n      if (mrb->exc == NULL) {\n        exc = mrb_nil_value();\n      }\n      else {\n        switch (mrb->exc->tt) {\n        case MRB_TT_BREAK:\n        case MRB_TT_EXCEPTION:\n          exc = mrb_obj_value(mrb->exc);\n          break;\n        default:\n          mrb_assert(!\"bad mrb_type\");\n          exc = mrb_nil_value();\n          break;\n        }\n        mrb->exc = NULL;\n      }\n      regs[a] = exc;\n      NEXT;\n    }\n    CASE(OP_RESCUE, BB) {\n      mrb_value exc = regs[a];  /* exc on stack */\n      mrb_value e = regs[b];\n      struct RClass *ec;\n\n      switch (mrb_type(e)) {\n      case MRB_TT_CLASS:\n      case MRB_TT_MODULE:\n        break;\n      default:\n        {\n          mrb_value exc;\n\n          exc = mrb_exc_new_lit(mrb, E_TYPE_ERROR,\n                                    \"class or module required for rescue clause\");\n          mrb_exc_set(mrb, exc);\n          goto L_RAISE;\n        }\n      }\n      ec = mrb_class_ptr(e);\n      regs[b] = mrb_bool_value(mrb_obj_is_kind_of(mrb, exc, ec));\n      NEXT;\n    }\n\n    CASE(OP_RAISEIF, B) {\n      mrb_value exc = regs[a];\n      if (mrb_break_p(exc)) {\n        mrb->exc = mrb_obj_ptr(exc);\n        goto L_BREAK;\n      }\n      mrb_exc_set(mrb, exc);\n      if (mrb->exc) {\n        goto L_RAISE;\n      }\n      NEXT;\n    }\n\n    CASE(OP_SSEND, BBB) {\n      regs[a] = regs[0];\n      insn = OP_SEND;\n    }\n    goto L_SENDB;\n\n    CASE(OP_SSENDB, BBB) {\n      regs[a] = regs[0];\n    }\n    goto L_SENDB;\n\n    CASE(OP_SEND, BBB)\n    goto L_SENDB;\n\n    L_SEND_SYM:\n    c = 1;\n    /* push nil after arguments */\n    SET_NIL_VALUE(regs[a+2]);\n    goto L_SENDB_SYM;\n\n    CASE(OP_SENDB, BBB)\n    L_SENDB:\n    mid = syms[b];\n    L_SENDB_SYM:\n    {\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_method_t m;\n      struct RClass *cls;\n      mrb_value recv, blk;\n\n      ARGUMENT_NORMALIZE(a, &c, insn);\n\n      recv = regs[a];\n      cls = mrb_class(mrb, recv);\n      m = mrb_method_search_vm(mrb, &cls, mid);\n      if (MRB_METHOD_UNDEF_P(m)) {\n        m = prepare_missing(mrb, recv, mid, &cls, a, &c, blk, 0);\n        mid = MRB_SYM(method_missing);\n      }\n\n      /* push callinfo */\n      ci = cipush(mrb, a, 0, cls, NULL, mid, c);\n\n      if (MRB_METHOD_CFUNC_P(m)) {\n        if (MRB_METHOD_PROC_P(m)) {\n          struct RProc *p = MRB_METHOD_PROC(m);\n\n          mrb_vm_ci_proc_set(ci, p);\n          recv = p->body.func(mrb, recv);\n        }\n        else {\n          if (MRB_METHOD_NOARG_P(m)) {\n            check_method_noarg(mrb, ci);\n          }\n          recv = MRB_METHOD_FUNC(m)(mrb, recv);\n        }\n        mrb_gc_arena_shrink(mrb, ai);\n        if (mrb->exc) goto L_RAISE;\n        ci = mrb->c->ci;\n        if (mrb_proc_p(blk)) {\n          struct RProc *p = mrb_proc_ptr(blk);\n          if (p && !MRB_PROC_STRICT_P(p) && MRB_PROC_ENV(p) == mrb_vm_ci_env(&ci[-1])) {\n            p->flags |= MRB_PROC_ORPHAN;\n          }\n        }\n        if (!ci->u.target_class) { /* return from context modifying method (resume/yield) */\n          if (ci->cci == CINFO_RESUMED) {\n            mrb->jmp = prev_jmp;\n            return recv;\n          }\n          else {\n            mrb_assert(!MRB_PROC_CFUNC_P(ci[-1].proc));\n            proc = ci[-1].proc;\n            irep = proc->body.irep;\n            pool = irep->pool;\n            syms = irep->syms;\n          }\n        }\n        ci->stack[0] = recv;\n        /* pop stackpos */\n        ci = cipop(mrb);\n        pc = ci->pc;\n      }\n      else {\n        /* setup environment for calling method */\n        mrb_vm_ci_proc_set(ci, (proc = MRB_METHOD_PROC(m)));\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n        mrb_stack_extend(mrb, (irep->nregs < 4) ? 4 : irep->nregs);\n        pc = irep->iseq;\n      }\n    }\n    JUMP;\n\n    CASE(OP_CALL, Z) {\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_value recv = ci->stack[0];\n      struct RProc *m = mrb_proc_ptr(recv);\n\n      /* replace callinfo */\n      ci->u.target_class = MRB_PROC_TARGET_CLASS(m);\n      mrb_vm_ci_proc_set(ci, m);\n      if (MRB_PROC_ENV_P(m)) {\n        ci->mid = MRB_PROC_ENV(m)->mid;\n      }\n\n      /* prepare stack */\n      if (MRB_PROC_CFUNC_P(m)) {\n        recv = MRB_PROC_CFUNC(m)(mrb, recv);\n        mrb_gc_arena_shrink(mrb, ai);\n        if (mrb->exc) goto L_RAISE;\n        /* pop stackpos */\n        ci = cipop(mrb);\n        pc = ci->pc;\n        ci[1].stack[0] = recv;\n        irep = mrb->c->ci->proc->body.irep;\n      }\n      else {\n        /* setup environment for calling method */\n        proc = m;\n        irep = m->body.irep;\n        if (!irep) {\n          mrb->c->ci->stack[0] = mrb_nil_value();\n          a = 0;\n          c = OP_R_NORMAL;\n          goto L_OP_RETURN_BODY;\n        }\n        mrb_int nargs = mrb_ci_bidx(ci)+1;\n        if (nargs < irep->nregs) {\n          mrb_stack_extend(mrb, irep->nregs);\n          stack_clear(regs+nargs, irep->nregs-nargs);\n        }\n        if (MRB_PROC_ENV_P(m)) {\n          regs[0] = MRB_PROC_ENV(m)->stack[0];\n        }\n        pc = irep->iseq;\n      }\n      pool = irep->pool;\n      syms = irep->syms;\n      JUMP;\n    }\n\n    CASE(OP_SUPER, BB) {\n      mrb_method_t m;\n      struct RClass *cls;\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_value recv, blk;\n      const struct RProc *p = ci->proc;\n      mrb_sym mid = ci->mid;\n      struct RClass* target_class = MRB_PROC_TARGET_CLASS(p);\n\n      if (MRB_PROC_ENV_P(p) && p->e.env->mid && p->e.env->mid != mid) { /* alias support */\n        mid = p->e.env->mid;    /* restore old mid */\n      }\n\n      if (mid == 0 || !target_class) {\n        mrb_value exc = mrb_exc_new_lit(mrb, E_NOMETHOD_ERROR, \"super called outside of method\");\n        mrb_exc_set(mrb, exc);\n        goto L_RAISE;\n      }\n      if (target_class->flags & MRB_FL_CLASS_IS_PREPENDED) {\n        target_class = mrb_vm_ci_target_class(ci);\n      }\n      else if (target_class->tt == MRB_TT_MODULE) {\n        target_class = mrb_vm_ci_target_class(ci);\n        if (target_class->tt != MRB_TT_ICLASS) {\n          goto super_typeerror;\n        }\n      }\n      recv = regs[0];\n      if (!mrb_obj_is_kind_of(mrb, recv, target_class)) {\n      super_typeerror: ;\n        mrb_value exc = mrb_exc_new_lit(mrb, E_TYPE_ERROR,\n                                            \"self has wrong type to call super in this context\");\n        mrb_exc_set(mrb, exc);\n        goto L_RAISE;\n      }\n\n      ARGUMENT_NORMALIZE(a, &b, OP_SUPER);\n\n      cls = target_class->super;\n      m = mrb_method_search_vm(mrb, &cls, mid);\n      if (MRB_METHOD_UNDEF_P(m)) {\n        m = prepare_missing(mrb, recv, mid, &cls, a, &b, blk, 1);\n        mid = MRB_SYM(method_missing);\n      }\n\n      /* push callinfo */\n      ci = cipush(mrb, a, 0, cls, NULL, mid, b);\n\n      /* prepare stack */\n      ci->stack[0] = recv;\n\n      if (MRB_METHOD_CFUNC_P(m)) {\n        mrb_value v;\n\n        if (MRB_METHOD_PROC_P(m)) {\n          mrb_vm_ci_proc_set(ci, MRB_METHOD_PROC(m));\n        }\n        v = MRB_METHOD_CFUNC(m)(mrb, recv);\n        mrb_gc_arena_restore(mrb, ai);\n        if (mrb->exc) goto L_RAISE;\n        ci = mrb->c->ci;\n        mrb_assert(!mrb_break_p(v));\n        if (!mrb_vm_ci_target_class(ci)) { /* return from context modifying method (resume/yield) */\n          if (ci->cci == CINFO_RESUMED) {\n            mrb->jmp = prev_jmp;\n            return v;\n          }\n          else {\n            mrb_assert(!MRB_PROC_CFUNC_P(ci[-1].proc));\n            proc = ci[-1].proc;\n            irep = proc->body.irep;\n            pool = irep->pool;\n            syms = irep->syms;\n          }\n        }\n        mrb->c->ci->stack[0] = v;\n        ci = cipop(mrb);\n        pc = ci->pc;\n      }\n      else {\n        /* setup environment for calling method */\n        mrb_vm_ci_proc_set(ci, (proc = MRB_METHOD_PROC(m)));\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n        mrb_stack_extend(mrb, (irep->nregs < 4) ? 4 : irep->nregs);\n        pc = irep->iseq;\n      }\n      JUMP;\n    }\n\n    CASE(OP_ARGARY, BS) {\n      mrb_int m1 = (b>>11)&0x3f;\n      mrb_int r  = (b>>10)&0x1;\n      mrb_int m2 = (b>>5)&0x1f;\n      mrb_int kd = (b>>4)&0x1;\n      mrb_int lv = (b>>0)&0xf;\n      mrb_value *stack;\n\n      if (mrb->c->ci->mid == 0 || mrb_vm_ci_target_class(mrb->c->ci) == NULL) {\n        mrb_value exc;\n\n      L_NOSUPER:\n        exc = mrb_exc_new_lit(mrb, E_NOMETHOD_ERROR, \"super called outside of method\");\n        mrb_exc_set(mrb, exc);\n        goto L_RAISE;\n      }\n      if (lv == 0) stack = regs + 1;\n      else {\n        struct REnv *e = uvenv(mrb, lv-1);\n        if (!e) goto L_NOSUPER;\n        if (MRB_ENV_LEN(e) <= m1+r+m2+1)\n          goto L_NOSUPER;\n        stack = e->stack + 1;\n      }\n      if (r == 0) {\n        regs[a] = mrb_ary_new_from_values(mrb, m1+m2, stack);\n      }\n      else {\n        mrb_value *pp = NULL;\n        struct RArray *rest;\n        mrb_int len = 0;\n\n        if (mrb_array_p(stack[m1])) {\n          struct RArray *ary = mrb_ary_ptr(stack[m1]);\n\n          pp = ARY_PTR(ary);\n          len = ARY_LEN(ary);\n        }\n        regs[a] = mrb_ary_new_capa(mrb, m1+len+m2);\n        rest = mrb_ary_ptr(regs[a]);\n        if (m1 > 0) {\n          stack_copy(ARY_PTR(rest), stack, m1);\n        }\n        if (len > 0) {\n          stack_copy(ARY_PTR(rest)+m1, pp, len);\n        }\n        if (m2 > 0) {\n          stack_copy(ARY_PTR(rest)+m1+len, stack+m1+1, m2);\n        }\n        ARY_SET_LEN(rest, m1+len+m2);\n      }\n      if (kd) {\n        regs[a+1] = stack[m1+r+m2];\n        regs[a+2] = stack[m1+r+m2+1];\n      }\n      else {\n        regs[a+1] = stack[m1+r+m2];\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_ENTER, W) {\n      mrb_int m1 = MRB_ASPEC_REQ(a);\n      mrb_int o  = MRB_ASPEC_OPT(a);\n      mrb_int r  = MRB_ASPEC_REST(a);\n      mrb_int m2 = MRB_ASPEC_POST(a);\n      mrb_int kd = (MRB_ASPEC_KEY(a) > 0 || MRB_ASPEC_KDICT(a))? 1 : 0;\n      /* unused\n      int b  = MRB_ASPEC_BLOCK(a);\n      */\n      mrb_int const len = m1 + o + r + m2;\n\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_int argc = ci->n;\n      mrb_value *argv = regs+1;\n      mrb_value * const argv0 = argv;\n      mrb_int const kw_pos = len + kd;    /* where kwhash should be */\n      mrb_int const blk_pos = kw_pos + 1; /* where block should be */\n      mrb_value blk = regs[mrb_ci_bidx(ci)];\n      mrb_value kdict = mrb_nil_value();\n\n      /* keyword arguments */\n      if (ci->nk > 0) {\n        mrb_int kidx = mrb_ci_kidx(ci);\n        kdict = regs[kidx];\n        if (!mrb_hash_p(kdict) || mrb_hash_size(mrb, kdict) == 0) {\n          kdict = mrb_nil_value();\n          ci->nk = 0;\n        }\n      }\n      if (!kd && !mrb_nil_p(kdict)) {\n        if (argc < 14) {\n          ci->n++;\n          argc++;    /* include kdict in normal arguments */\n        }\n        else if (argc == 14) {\n          /* pack arguments and kdict */\n          regs[1] = mrb_ary_new_from_values(mrb, argc+1, &regs[1]);\n          argc = ci->n = 15;\n        }\n        else {/* argc == 15 */\n          /* push kdict to packed arguments */\n          mrb_ary_push(mrb, regs[1], regs[2]);\n        }\n        ci->nk = 0;\n      }\n      if (kd && MRB_ASPEC_KEY(a) > 0 && mrb_hash_p(kdict)) {\n        kdict = mrb_hash_dup(mrb, kdict);\n      }\n\n      /* arguments is passed with Array */\n      if (argc == 15) {\n        struct RArray *ary = mrb_ary_ptr(regs[1]);\n        argv = ARY_PTR(ary);\n        argc = (int)ARY_LEN(ary);\n        mrb_gc_protect(mrb, regs[1]);\n      }\n\n      /* strict argument check */\n      if (ci->proc && MRB_PROC_STRICT_P(ci->proc)) {\n        if (argc < m1 + m2 || (r == 0 && argc > len)) {\n          argnum_error(mrb, m1+m2);\n          goto L_RAISE;\n        }\n      }\n      /* extract first argument array to arguments */\n      else if (len > 1 && argc == 1 && mrb_array_p(argv[0])) {\n        mrb_gc_protect(mrb, argv[0]);\n        argc = (int)RARRAY_LEN(argv[0]);\n        argv = RARRAY_PTR(argv[0]);\n      }\n\n      /* rest arguments */\n      mrb_value rest = mrb_nil_value();\n      if (argc < len) {\n        mrb_int mlen = m2;\n        if (argc < m1+m2) {\n          mlen = m1 < argc ? argc - m1 : 0;\n        }\n\n        /* copy mandatory and optional arguments */\n        if (argv0 != argv && argv) {\n          value_move(&regs[1], argv, argc-mlen); /* m1 + o */\n        }\n        if (argc < m1) {\n          stack_clear(&regs[argc+1], m1-argc);\n        }\n        /* copy post mandatory arguments */\n        if (mlen) {\n          value_move(&regs[len-m2+1], &argv[argc-mlen], mlen);\n        }\n        if (mlen < m2) {\n          stack_clear(&regs[len-m2+mlen+1], m2-mlen);\n        }\n        /* initialize rest arguments with empty Array */\n        if (r) {\n          rest = mrb_ary_new_capa(mrb, 0);\n          regs[m1+o+1] = rest;\n        }\n        /* skip initializer of passed arguments */\n        if (o > 0 && argc > m1+m2)\n          pc += (argc - m1 - m2)*3;\n      }\n      else {\n        mrb_int rnum = 0;\n        if (argv0 != argv) {\n          value_move(&regs[1], argv, m1+o);\n        }\n        if (r) {\n          rnum = argc-m1-o-m2;\n          rest = mrb_ary_new_from_values(mrb, rnum, argv+m1+o);\n          regs[m1+o+1] = rest;\n        }\n        if (m2 > 0 && argc-m2 > m1) {\n          value_move(&regs[m1+o+r+1], &argv[m1+o+rnum], m2);\n        }\n        pc += o*3;\n      }\n\n      /* need to be update blk first to protect blk from GC */\n      regs[blk_pos] = blk;              /* move block */\n      if (kd) {\n        if (mrb_nil_p(kdict))\n          kdict = mrb_hash_new_capa(mrb, 0);\n        regs[kw_pos] = kdict;           /* set kwhash */\n      }\n\n      /* format arguments for generated code */\n      mrb->c->ci->n = len;\n\n      /* clear local (but non-argument) variables */\n      if (irep->nlocals-blk_pos-1 > 0) {\n        stack_clear(&regs[blk_pos+1], irep->nlocals-blk_pos-1);\n      }\n      JUMP;\n    }\n\n    CASE(OP_KARG, BB) {\n      mrb_value k = mrb_symbol_value(syms[b]);\n      mrb_int kidx = mrb_ci_kidx(mrb->c->ci);\n      mrb_value kdict;\n\n      if (kidx < 0 || !mrb_hash_p(kdict=regs[kidx]) || !mrb_hash_key_p(mrb, kdict, k)) {\n        mrb_value str = mrb_format(mrb, \"missing keyword: %v\", k);\n        mrb_exc_set(mrb, mrb_exc_new_str(mrb, E_ARGUMENT_ERROR, str));\n        goto L_RAISE;\n      }\n      regs[a] = mrb_hash_get(mrb, kdict, k);\n      mrb_hash_delete_key(mrb, kdict, k);\n      NEXT;\n    }\n\n    CASE(OP_KEY_P, BB) {\n      mrb_value k = mrb_symbol_value(syms[b]);\n      mrb_int kidx = mrb_ci_kidx(mrb->c->ci);\n      mrb_value kdict;\n      mrb_bool key_p = FALSE;\n\n      if (kidx >= 0 && mrb_hash_p(kdict=regs[kidx])) {\n        key_p = mrb_hash_key_p(mrb, kdict, k);\n      }\n      regs[a] = mrb_bool_value(key_p);\n      NEXT;\n    }\n\n    CASE(OP_KEYEND, Z) {\n      mrb_int kidx = mrb_ci_kidx(mrb->c->ci);\n      mrb_value kdict;\n\n      if (kidx >= 0 && mrb_hash_p(kdict=regs[kidx]) && !mrb_hash_empty_p(mrb, kdict)) {\n        mrb_value keys = mrb_hash_keys(mrb, kdict);\n        mrb_value key1 = RARRAY_PTR(keys)[0];\n        mrb_value str = mrb_format(mrb, \"unknown keyword: %v\", key1);\n        mrb_exc_set(mrb, mrb_exc_new_str(mrb, E_ARGUMENT_ERROR, str));\n        goto L_RAISE;\n      }\n      NEXT;\n    }\n\n    CASE(OP_BREAK, B) {\n      c = OP_R_BREAK;\n      goto L_RETURN;\n    }\n    CASE(OP_RETURN_BLK, B) {\n      c = OP_R_RETURN;\n      goto L_RETURN;\n    }\n    CASE(OP_RETURN, B)\n    c = OP_R_NORMAL;\n    L_RETURN:\n    {\n      mrb_callinfo *ci;\n\n      ci = mrb->c->ci;\n      if (ci->mid) {\n        mrb_value blk = regs[mrb_ci_bidx(ci)];\n\n        if (mrb_proc_p(blk)) {\n          struct RProc *p = mrb_proc_ptr(blk);\n\n          if (!MRB_PROC_STRICT_P(p) &&\n              ci > mrb->c->cibase && MRB_PROC_ENV(p) == mrb_vm_ci_env(&ci[-1])) {\n            p->flags |= MRB_PROC_ORPHAN;\n          }\n        }\n      }\n\n      if (mrb->exc) {\n      L_RAISE:\n        ci = mrb->c->ci;\n        if (ci == mrb->c->cibase) {\n          ch = catch_handler_find(mrb, ci, pc, MRB_CATCH_FILTER_ALL);\n          if (ch == NULL) goto L_FTOP;\n          goto L_CATCH;\n        }\n        while ((ch = catch_handler_find(mrb, ci, pc, MRB_CATCH_FILTER_ALL)) == NULL) {\n          ci = cipop(mrb);\n          if (ci[1].cci == CINFO_SKIP && prev_jmp) {\n            mrb->jmp = prev_jmp;\n            MRB_THROW(prev_jmp);\n          }\n          pc = ci[0].pc;\n          if (ci == mrb->c->cibase) {\n            ch = catch_handler_find(mrb, ci, pc, MRB_CATCH_FILTER_ALL);\n            if (ch == NULL) {\n            L_FTOP:             /* fiber top */\n              if (mrb->c == mrb->root_c) {\n                mrb->c->ci->stack = mrb->c->stbase;\n                goto L_STOP;\n              }\n              else {\n                struct mrb_context *c = mrb->c;\n\n                c->status = MRB_FIBER_TERMINATED;\n                mrb->c = c->prev;\n                c->prev = NULL;\n                goto L_RAISE;\n              }\n            }\n            break;\n          }\n        }\n      L_CATCH:\n        if (ch == NULL) goto L_STOP;\n        if (FALSE) {\n        L_CATCH_TAGGED_BREAK: /* from THROW_TAGGED_BREAK() or UNWIND_ENSURE() */\n          ci = mrb->c->ci;\n        }\n        proc = ci->proc;\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n        mrb_stack_extend(mrb, irep->nregs);\n        pc = irep->iseq + mrb_irep_catch_handler_unpack(ch->target);\n      }\n      else {\n        mrb_int acc;\n        mrb_value v;\n\n        ci = mrb->c->ci;\n        v = regs[a];\n        mrb_gc_protect(mrb, v);\n        switch (c) {\n        case OP_R_RETURN:\n          /* Fall through to OP_R_NORMAL otherwise */\n          if (ci->cci == CINFO_NONE && MRB_PROC_ENV_P(proc) && !MRB_PROC_STRICT_P(proc)) {\n            const struct RProc *dst;\n            mrb_callinfo *cibase;\n            cibase = mrb->c->cibase;\n            dst = top_proc(mrb, proc);\n\n            if (MRB_PROC_ENV_P(dst)) {\n              struct REnv *e = MRB_PROC_ENV(dst);\n\n              if (!MRB_ENV_ONSTACK_P(e) || (e->cxt && e->cxt != mrb->c)) {\n                localjump_error(mrb, LOCALJUMP_ERROR_RETURN);\n                goto L_RAISE;\n              }\n            }\n            /* check jump destination */\n            while (cibase <= ci && ci->proc != dst) {\n              if (ci->cci > CINFO_NONE) { /* jump cross C boundary */\n                localjump_error(mrb, LOCALJUMP_ERROR_RETURN);\n                goto L_RAISE;\n              }\n              ci--;\n            }\n            if (ci <= cibase) { /* no jump destination */\n              localjump_error(mrb, LOCALJUMP_ERROR_RETURN);\n              goto L_RAISE;\n            }\n            ci = mrb->c->ci;\n            while (cibase <= ci && ci->proc != dst) {\n              CHECKPOINT_RESTORE(RBREAK_TAG_RETURN_BLOCK) {\n                cibase = mrb->c->cibase;\n                dst = top_proc(mrb, proc);\n              }\n              CHECKPOINT_MAIN(RBREAK_TAG_RETURN_BLOCK) {\n                UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_RETURN_BLOCK, proc, v);\n              }\n              CHECKPOINT_END(RBREAK_TAG_RETURN_BLOCK);\n              ci = cipop(mrb);\n              pc = ci->pc;\n            }\n            proc = ci->proc;\n            mrb->exc = NULL; /* clear break object */\n            break;\n          }\n          /* fallthrough */\n        case OP_R_NORMAL:\n        NORMAL_RETURN:\n          if (ci == mrb->c->cibase) {\n            struct mrb_context *c;\n            c = mrb->c;\n\n            if (!c->prev) { /* toplevel return */\n              regs[irep->nlocals] = v;\n              goto CHECKPOINT_LABEL_MAKE(RBREAK_TAG_STOP);\n            }\n            if (!c->vmexec && c->prev->ci == c->prev->cibase) {\n              mrb_value exc = mrb_exc_new_lit(mrb, E_FIBER_ERROR, \"double resume\");\n              mrb_exc_set(mrb, exc);\n              goto L_RAISE;\n            }\n            CHECKPOINT_RESTORE(RBREAK_TAG_RETURN_TOPLEVEL) {\n              c = mrb->c;\n            }\n            CHECKPOINT_MAIN(RBREAK_TAG_RETURN_TOPLEVEL) {\n              UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_RETURN_TOPLEVEL, proc, v);\n            }\n            CHECKPOINT_END(RBREAK_TAG_RETURN_TOPLEVEL);\n            /* automatic yield at the end */\n            c->status = MRB_FIBER_TERMINATED;\n            mrb->c = c->prev;\n            mrb->c->status = MRB_FIBER_RUNNING;\n            c->prev = NULL;\n            if (c->vmexec) {\n              mrb_gc_arena_restore(mrb, ai);\n              c->vmexec = FALSE;\n              mrb->jmp = prev_jmp;\n              return v;\n            }\n            ci = mrb->c->ci;\n          }\n          CHECKPOINT_RESTORE(RBREAK_TAG_RETURN) {\n            /* do nothing */\n          }\n          CHECKPOINT_MAIN(RBREAK_TAG_RETURN) {\n            UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_RETURN, proc, v);\n          }\n          CHECKPOINT_END(RBREAK_TAG_RETURN);\n          mrb->exc = NULL; /* clear break object */\n          break;\n        case OP_R_BREAK:\n          if (MRB_PROC_STRICT_P(proc)) goto NORMAL_RETURN;\n          if (MRB_PROC_ORPHAN_P(proc)) {\n            mrb_value exc;\n\n          L_BREAK_ERROR:\n            exc = mrb_exc_new_lit(mrb, E_LOCALJUMP_ERROR,\n                                      \"break from proc-closure\");\n            mrb_exc_set(mrb, exc);\n            goto L_RAISE;\n          }\n          if (!MRB_PROC_ENV_P(proc) || !MRB_ENV_ONSTACK_P(MRB_PROC_ENV(proc))) {\n            goto L_BREAK_ERROR;\n          }\n          else {\n            struct REnv *e = MRB_PROC_ENV(proc);\n\n            if (e->cxt != mrb->c) {\n              goto L_BREAK_ERROR;\n            }\n          }\n          CHECKPOINT_RESTORE(RBREAK_TAG_BREAK) {\n            /* do nothing */\n          }\n          CHECKPOINT_MAIN(RBREAK_TAG_BREAK) {\n            UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_BREAK, proc, v);\n          }\n          CHECKPOINT_END(RBREAK_TAG_BREAK);\n          /* break from fiber block */\n          if (ci == mrb->c->cibase && ci->pc) {\n            struct mrb_context *c = mrb->c;\n\n            mrb->c = c->prev;\n            c->prev = NULL;\n            ci = mrb->c->ci;\n          }\n          if (ci->cci > CINFO_NONE) {\n            ci = cipop(mrb);\n            mrb_gc_arena_restore(mrb, ai);\n            mrb->c->vmexec = FALSE;\n            mrb->exc = (struct RObject*)break_new(mrb, RBREAK_TAG_BREAK, proc, v);\n            mrb->jmp = prev_jmp;\n            MRB_THROW(prev_jmp);\n          }\n          if (FALSE) {\n            struct RBreak *brk;\n\n          L_BREAK:\n            brk = (struct RBreak*)mrb->exc;\n            proc = mrb_break_proc_get(brk);\n            v = mrb_break_value_get(brk);\n            ci = mrb->c->ci;\n\n            switch (mrb_break_tag_get(brk)) {\n#define DISPATCH_CHECKPOINTS(n, i) case n: goto CHECKPOINT_LABEL_MAKE(n);\n              RBREAK_TAG_FOREACH(DISPATCH_CHECKPOINTS)\n#undef DISPATCH_CHECKPOINTS\n              default:\n                mrb_assert(!\"wrong break tag\");\n            }\n          }\n          while (mrb->c->cibase < ci && ci[-1].proc != proc->upper) {\n            if (ci[-1].cci == CINFO_SKIP) {\n              goto L_BREAK_ERROR;\n            }\n            CHECKPOINT_RESTORE(RBREAK_TAG_BREAK_UPPER) {\n              /* do nothing */\n            }\n            CHECKPOINT_MAIN(RBREAK_TAG_BREAK_UPPER) {\n              UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_BREAK_UPPER, proc, v);\n            }\n            CHECKPOINT_END(RBREAK_TAG_BREAK_UPPER);\n            ci = cipop(mrb);\n            pc = ci->pc;\n          }\n          CHECKPOINT_RESTORE(RBREAK_TAG_BREAK_INTARGET) {\n            /* do nothing */\n          }\n          CHECKPOINT_MAIN(RBREAK_TAG_BREAK_INTARGET) {\n            UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_BREAK_INTARGET, proc, v);\n          }\n          CHECKPOINT_END(RBREAK_TAG_BREAK_INTARGET);\n          if (ci == mrb->c->cibase) {\n            goto L_BREAK_ERROR;\n          }\n          mrb->exc = NULL; /* clear break object */\n          break;\n        default:\n          /* cannot happen */\n          break;\n        }\n        mrb_assert(ci == mrb->c->ci);\n        mrb_assert(mrb->exc == NULL);\n\n        if (mrb->c->vmexec && !mrb_vm_ci_target_class(ci)) {\n          mrb_gc_arena_restore(mrb, ai);\n          mrb->c->vmexec = FALSE;\n          mrb->jmp = prev_jmp;\n          return v;\n        }\n        acc = ci->cci;\n        ci = cipop(mrb);\n        if (acc == CINFO_SKIP || acc == CINFO_DIRECT) {\n          mrb_gc_arena_restore(mrb, ai);\n          mrb->jmp = prev_jmp;\n          return v;\n        }\n        pc = ci->pc;\n        DEBUG(fprintf(stderr, \"from :%s\\n\", mrb_sym_name(mrb, ci->mid)));\n        proc = ci->proc;\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n\n        ci[1].stack[0] = v;\n        mrb_gc_arena_restore(mrb, ai);\n      }\n      JUMP;\n    }\n\n    CASE(OP_BLKPUSH, BS) {\n      int m1 = (b>>11)&0x3f;\n      int r  = (b>>10)&0x1;\n      int m2 = (b>>5)&0x1f;\n      int kd = (b>>4)&0x1;\n      int lv = (b>>0)&0xf;\n      mrb_value *stack;\n\n      if (lv == 0) stack = regs + 1;\n      else {\n        struct REnv *e = uvenv(mrb, lv-1);\n        if (!e || (!MRB_ENV_ONSTACK_P(e) && e->mid == 0) ||\n            MRB_ENV_LEN(e) <= m1+r+m2+1) {\n          localjump_error(mrb, LOCALJUMP_ERROR_YIELD);\n          goto L_RAISE;\n        }\n        stack = e->stack + 1;\n      }\n      if (mrb_nil_p(stack[m1+r+m2+kd])) {\n        localjump_error(mrb, LOCALJUMP_ERROR_YIELD);\n        goto L_RAISE;\n      }\n      regs[a] = stack[m1+r+m2+kd];\n      NEXT;\n    }\n\n  L_INT_OVERFLOW:\n    {\n      mrb_value exc = mrb_exc_new_lit(mrb, E_RANGE_ERROR, \"integer overflow\");\n      mrb_exc_set(mrb, exc);\n    }\n    goto L_RAISE;\n\n#define TYPES2(a,b) ((((uint16_t)(a))<<8)|(((uint16_t)(b))&0xff))\n#define OP_MATH(op_name)                                                    \\\n  /* need to check if op is overridden */                                   \\\n  switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {                  \\\n    OP_MATH_CASE_INTEGER(op_name);                                          \\\n    OP_MATH_CASE_FLOAT(op_name, integer, float);                            \\\n    OP_MATH_CASE_FLOAT(op_name, float,  integer);                           \\\n    OP_MATH_CASE_FLOAT(op_name, float,  float);                             \\\n    OP_MATH_CASE_STRING_##op_name();                                        \\\n    default:                                                                \\\n      mid = MRB_OPSYM(op_name);                                             \\\n      goto L_SEND_SYM;                                                      \\\n  }                                                                         \\\n  NEXT;\n#define OP_MATH_CASE_INTEGER(op_name)                                       \\\n  case TYPES2(MRB_TT_INTEGER, MRB_TT_INTEGER):                              \\\n    {                                                                       \\\n      mrb_int x = mrb_integer(regs[a]), y = mrb_integer(regs[a+1]), z;      \\\n      if (mrb_int_##op_name##_overflow(x, y, &z))                           \\\n        OP_MATH_OVERFLOW_INT();                                             \\\n      else                                                                  \\\n        SET_INT_VALUE(mrb,regs[a], z);                                      \\\n    }                                                                       \\\n    break\n#ifdef MRB_NO_FLOAT\n#define OP_MATH_CASE_FLOAT(op_name, t1, t2) (void)0\n#else\n#define OP_MATH_CASE_FLOAT(op_name, t1, t2)                                     \\\n  case TYPES2(OP_MATH_TT_##t1, OP_MATH_TT_##t2):                                \\\n    {                                                                           \\\n      mrb_float z = mrb_##t1(regs[a]) OP_MATH_OP_##op_name mrb_##t2(regs[a+1]); \\\n      SET_FLOAT_VALUE(mrb, regs[a], z);                                         \\\n    }                                                                           \\\n    break\n#endif\n#define OP_MATH_OVERFLOW_INT() goto L_INT_OVERFLOW\n#define OP_MATH_CASE_STRING_add()                                           \\\n  case TYPES2(MRB_TT_STRING, MRB_TT_STRING):                                \\\n    regs[a] = mrb_str_plus(mrb, regs[a], regs[a+1]);                        \\\n    mrb_gc_arena_restore(mrb, ai);                                          \\\n    break\n#define OP_MATH_CASE_STRING_sub() (void)0\n#define OP_MATH_CASE_STRING_mul() (void)0\n#define OP_MATH_OP_add +\n#define OP_MATH_OP_sub -\n#define OP_MATH_OP_mul *\n#define OP_MATH_TT_integer MRB_TT_INTEGER\n#define OP_MATH_TT_float   MRB_TT_FLOAT\n\n    CASE(OP_ADD, B) {\n      OP_MATH(add);\n    }\n\n    CASE(OP_SUB, B) {\n      OP_MATH(sub);\n    }\n\n    CASE(OP_MUL, B) {\n      OP_MATH(mul);\n    }\n\n    CASE(OP_DIV, B) {\n#ifndef MRB_NO_FLOAT\n      mrb_float x, y, f;\n#endif\n\n      /* need to check if op is overridden */\n      switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {\n      case TYPES2(MRB_TT_INTEGER,MRB_TT_INTEGER):\n        {\n          mrb_int x = mrb_integer(regs[a]);\n          mrb_int y = mrb_integer(regs[a+1]);\n          mrb_int div = mrb_div_int(mrb, x, y);\n          SET_INT_VALUE(mrb, regs[a], div);\n        }\n        NEXT;\n#ifndef MRB_NO_FLOAT\n      case TYPES2(MRB_TT_INTEGER,MRB_TT_FLOAT):\n        x = (mrb_float)mrb_integer(regs[a]);\n        y = mrb_float(regs[a+1]);\n        break;\n      case TYPES2(MRB_TT_FLOAT,MRB_TT_INTEGER):\n        x = mrb_float(regs[a]);\n        y = (mrb_float)mrb_integer(regs[a+1]);\n        break;\n      case TYPES2(MRB_TT_FLOAT,MRB_TT_FLOAT):\n        x = mrb_float(regs[a]);\n        y = mrb_float(regs[a+1]);\n        break;\n#endif\n      default:\n        mid = MRB_OPSYM(div);\n        goto L_SEND_SYM;\n      }\n\n#ifndef MRB_NO_FLOAT\n      f = mrb_div_float(x, y);\n      SET_FLOAT_VALUE(mrb, regs[a], f);\n#endif\n      NEXT;\n    }\n\n#define OP_MATHI(op_name)                                                   \\\n  /* need to check if op is overridden */                                   \\\n  switch (mrb_type(regs[a])) {                                              \\\n    OP_MATHI_CASE_INTEGER(op_name);                                         \\\n    OP_MATHI_CASE_FLOAT(op_name);                                           \\\n    default:                                                                \\\n      SET_INT_VALUE(mrb,regs[a+1], b);                                      \\\n      mid = MRB_OPSYM(op_name);                                             \\\n      goto L_SEND_SYM;                                                      \\\n  }                                                                         \\\n  NEXT;\n#define OP_MATHI_CASE_INTEGER(op_name)                                      \\\n  case MRB_TT_INTEGER:                                                      \\\n    {                                                                       \\\n      mrb_int x = mrb_integer(regs[a]), y = (mrb_int)b, z;                  \\\n      if (mrb_int_##op_name##_overflow(x, y, &z))                           \\\n        OP_MATH_OVERFLOW_INT();                                             \\\n      else                                                                  \\\n        SET_INT_VALUE(mrb,regs[a], z);                                      \\\n    }                                                                       \\\n    break\n#ifdef MRB_NO_FLOAT\n#define OP_MATHI_CASE_FLOAT(op_name) (void)0\n#else\n#define OP_MATHI_CASE_FLOAT(op_name)                                        \\\n  case MRB_TT_FLOAT:                                                        \\\n    {                                                                       \\\n      mrb_float z = mrb_float(regs[a]) OP_MATH_OP_##op_name b;              \\\n      SET_FLOAT_VALUE(mrb, regs[a], z);                                     \\\n    }                                                                       \\\n    break\n#endif\n\n    CASE(OP_ADDI, BB) {\n      OP_MATHI(add);\n    }\n\n    CASE(OP_SUBI, BB) {\n      OP_MATHI(sub);\n    }\n\n#define OP_CMP_BODY(op,v1,v2) (v1(regs[a]) op v2(regs[a+1]))\n\n#ifdef MRB_NO_FLOAT\n#define OP_CMP(op,sym) do {\\\n  int result;\\\n  /* need to check if - is overridden */\\\n  switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {\\\n  case TYPES2(MRB_TT_INTEGER,MRB_TT_INTEGER):\\\n    result = OP_CMP_BODY(op,mrb_fixnum,mrb_fixnum);\\\n    break;\\\n  default:\\\n    mid = MRB_OPSYM(sym);\\\n    goto L_SEND_SYM;\\\n  }\\\n  if (result) {\\\n    SET_TRUE_VALUE(regs[a]);\\\n  }\\\n  else {\\\n    SET_FALSE_VALUE(regs[a]);\\\n  }\\\n} while(0)\n#else\n#define OP_CMP(op, sym) do {\\\n  int result;\\\n  /* need to check if - is overridden */\\\n  switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {\\\n  case TYPES2(MRB_TT_INTEGER,MRB_TT_INTEGER):\\\n    result = OP_CMP_BODY(op,mrb_fixnum,mrb_fixnum);\\\n    break;\\\n  case TYPES2(MRB_TT_INTEGER,MRB_TT_FLOAT):\\\n    result = OP_CMP_BODY(op,mrb_fixnum,mrb_float);\\\n    break;\\\n  case TYPES2(MRB_TT_FLOAT,MRB_TT_INTEGER):\\\n    result = OP_CMP_BODY(op,mrb_float,mrb_fixnum);\\\n    break;\\\n  case TYPES2(MRB_TT_FLOAT,MRB_TT_FLOAT):\\\n    result = OP_CMP_BODY(op,mrb_float,mrb_float);\\\n    break;\\\n  default:\\\n    mid = MRB_OPSYM(sym);\\\n    goto L_SEND_SYM;\\\n  }\\\n  if (result) {\\\n    SET_TRUE_VALUE(regs[a]);\\\n  }\\\n  else {\\\n    SET_FALSE_VALUE(regs[a]);\\\n  }\\\n} while(0)\n#endif\n\n    CASE(OP_EQ, B) {\n      if (mrb_obj_eq(mrb, regs[a], regs[a+1])) {\n        SET_TRUE_VALUE(regs[a]);\n      }\n      else {\n        OP_CMP(==,eq);\n      }\n      NEXT;\n    }\n\n    CASE(OP_LT, B) {\n      OP_CMP(<,lt);\n      NEXT;\n    }\n\n    CASE(OP_LE, B) {\n      OP_CMP(<=,le);\n      NEXT;\n    }\n\n    CASE(OP_GT, B) {\n      OP_CMP(>,gt);\n      NEXT;\n    }\n\n    CASE(OP_GE, B) {\n      OP_CMP(>=,ge);\n      NEXT;\n    }\n\n    CASE(OP_ARRAY, BB) {\n      regs[a] = mrb_ary_new_from_values(mrb, b, &regs[a]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n    CASE(OP_ARRAY2, BBB) {\n      regs[a] = mrb_ary_new_from_values(mrb, c, &regs[b]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_ARYCAT, B) {\n      mrb_value splat = mrb_ary_splat(mrb, regs[a+1]);\n      if (mrb_nil_p(regs[a])) {\n        regs[a] = splat;\n      }\n      else {\n        mrb_assert(mrb_array_p(regs[a]));\n        mrb_ary_concat(mrb, regs[a], splat);\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_ARYPUSH, BB) {\n      mrb_assert(mrb_array_p(regs[a]));\n      for (mrb_int i=0; i<b; i++) {\n        mrb_ary_push(mrb, regs[a], regs[a+i+1]);\n      }\n      NEXT;\n    }\n\n    CASE(OP_ARYDUP, B) {\n      mrb_value ary = regs[a];\n      if (mrb_array_p(ary)) {\n        ary = mrb_ary_new_from_values(mrb, RARRAY_LEN(ary), RARRAY_PTR(ary));\n      }\n      else {\n        ary = mrb_ary_new_from_values(mrb, 1, &ary);\n      }\n      regs[a] = ary;\n      NEXT;\n    }\n\n    CASE(OP_AREF, BBB) {\n      mrb_value v = regs[b];\n\n      if (!mrb_array_p(v)) {\n        if (c == 0) {\n          regs[a] = v;\n        }\n        else {\n          SET_NIL_VALUE(regs[a]);\n        }\n      }\n      else {\n        v = mrb_ary_ref(mrb, v, c);\n        regs[a] = v;\n      }\n      NEXT;\n    }\n\n    CASE(OP_ASET, BBB) {\n      mrb_assert(mrb_array_p(regs[a]));\n      mrb_ary_set(mrb, regs[b], c, regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_APOST, BBB) {\n      mrb_value v = regs[a];\n      int pre  = b;\n      int post = c;\n      struct RArray *ary;\n      int len, idx;\n\n      if (!mrb_array_p(v)) {\n        v = mrb_ary_new_from_values(mrb, 1, &regs[a]);\n      }\n      ary = mrb_ary_ptr(v);\n      len = (int)ARY_LEN(ary);\n      if (len > pre + post) {\n        v = mrb_ary_new_from_values(mrb, len - pre - post, ARY_PTR(ary)+pre);\n        regs[a++] = v;\n        while (post--) {\n          regs[a++] = ARY_PTR(ary)[len-post-1];\n        }\n      }\n      else {\n        v = mrb_ary_new_capa(mrb, 0);\n        regs[a++] = v;\n        for (idx=0; idx+pre<len; idx++) {\n          regs[a+idx] = ARY_PTR(ary)[pre+idx];\n        }\n        while (idx < post) {\n          SET_NIL_VALUE(regs[a+idx]);\n          idx++;\n        }\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_INTERN, B) {\n      mrb_assert(mrb_string_p(regs[a]));\n      mrb_sym sym = mrb_intern_str(mrb, regs[a]);\n      regs[a] = mrb_symbol_value(sym);\n      NEXT;\n    }\n\n    CASE(OP_SYMBOL, BB) {\n      size_t len;\n      mrb_sym sym;\n\n      mrb_assert((pool[b].tt&IREP_TT_NFLAG)==0);\n      len = pool[b].tt >> 2;\n      if (pool[b].tt & IREP_TT_SFLAG) {\n        sym = mrb_intern_static(mrb, pool[b].u.str, len);\n      }\n      else {\n        sym  = mrb_intern(mrb, pool[b].u.str, len);\n      }\n      regs[a] = mrb_symbol_value(sym);\n      NEXT;\n    }\n\n    CASE(OP_STRING, BB) {\n      size_t len;\n\n      mrb_assert((pool[b].tt&IREP_TT_NFLAG)==0);\n      len = pool[b].tt >> 2;\n      if (pool[b].tt & IREP_TT_SFLAG) {\n        regs[a] = mrb_str_new_static(mrb, pool[b].u.str, len);\n      }\n      else {\n        regs[a] = mrb_str_new(mrb, pool[b].u.str, len);\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_STRCAT, B) {\n      mrb_assert(mrb_string_p(regs[a]));\n      mrb_str_concat(mrb, regs[a], regs[a+1]);\n      NEXT;\n    }\n\n    CASE(OP_HASH, BB) {\n      mrb_value hash = mrb_hash_new_capa(mrb, b);\n      int i;\n      int lim = a+b*2;\n\n      for (i=a; i<lim; i+=2) {\n        mrb_hash_set(mrb, hash, regs[i], regs[i+1]);\n      }\n      regs[a] = hash;\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_HASHADD, BB) {\n      mrb_value hash;\n      int i;\n      int lim = a+b*2+1;\n\n      hash = regs[a];\n      mrb_assert(mrb_hash_p(hash));\n      for (i=a+1; i<lim; i+=2) {\n        mrb_hash_set(mrb, hash, regs[i], regs[i+1]);\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n    CASE(OP_HASHCAT, B) {\n      mrb_value hash = regs[a];\n\n      mrb_assert(mrb_hash_p(hash));\n      mrb_hash_merge(mrb, hash, regs[a+1]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_LAMBDA, BB)\n    c = OP_L_LAMBDA;\n    L_MAKE_LAMBDA:\n    {\n      struct RProc *p;\n      const mrb_irep *nirep = irep->reps[b];\n\n      if (c & OP_L_CAPTURE) {\n        p = mrb_closure_new(mrb, nirep);\n      }\n      else {\n        p = mrb_proc_new(mrb, nirep);\n        p->flags |= MRB_PROC_SCOPE;\n      }\n      if (c & OP_L_STRICT) p->flags |= MRB_PROC_STRICT;\n      regs[a] = mrb_obj_value(p);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n    CASE(OP_BLOCK, BB) {\n      c = OP_L_BLOCK;\n      goto L_MAKE_LAMBDA;\n    }\n    CASE(OP_METHOD, BB) {\n      c = OP_L_METHOD;\n      goto L_MAKE_LAMBDA;\n    }\n\n    CASE(OP_RANGE_INC, B) {\n      regs[a] = mrb_range_new(mrb, regs[a], regs[a+1], FALSE);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_RANGE_EXC, B) {\n      regs[a] = mrb_range_new(mrb, regs[a], regs[a+1], TRUE);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_OCLASS, B) {\n      regs[a] = mrb_obj_value(mrb->object_class);\n      NEXT;\n    }\n\n    CASE(OP_CLASS, BB) {\n      struct RClass *c = 0, *baseclass;\n      mrb_value base, super;\n      mrb_sym id = syms[b];\n\n      base = regs[a];\n      super = regs[a+1];\n      if (mrb_nil_p(base)) {\n        baseclass = MRB_PROC_TARGET_CLASS(mrb->c->ci->proc);\n        if (!baseclass) baseclass = mrb->object_class;\n        base = mrb_obj_value(baseclass);\n      }\n      c = mrb_vm_define_class(mrb, base, super, id);\n      regs[a] = mrb_obj_value(c);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_MODULE, BB) {\n      struct RClass *cls = 0, *baseclass;\n      mrb_value base;\n      mrb_sym id = syms[b];\n\n      base = regs[a];\n      if (mrb_nil_p(base)) {\n        baseclass = MRB_PROC_TARGET_CLASS(mrb->c->ci->proc);\n        if (!baseclass) baseclass = mrb->object_class;\n        base = mrb_obj_value(baseclass);\n      }\n      cls = mrb_vm_define_module(mrb, base, id);\n      regs[a] = mrb_obj_value(cls);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_EXEC, BB)\n    {\n      mrb_value recv = regs[a];\n      struct RProc *p;\n      const mrb_irep *nirep = irep->reps[b];\n\n      /* prepare closure */\n      p = mrb_proc_new(mrb, nirep);\n      p->c = NULL;\n      mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)proc);\n      MRB_PROC_SET_TARGET_CLASS(p, mrb_class_ptr(recv));\n      p->flags |= MRB_PROC_SCOPE;\n\n      /* prepare call stack */\n      cipush(mrb, a, 0, mrb_class_ptr(recv), p, 0, 0);\n\n      irep = p->body.irep;\n      pool = irep->pool;\n      syms = irep->syms;\n      mrb_stack_extend(mrb, irep->nregs);\n      stack_clear(regs+1, irep->nregs-1);\n      pc = irep->iseq;\n      JUMP;\n    }\n\n    CASE(OP_DEF, BB) {\n      struct RClass *target = mrb_class_ptr(regs[a]);\n      struct RProc *p = mrb_proc_ptr(regs[a+1]);\n      mrb_method_t m;\n      mrb_sym mid = syms[b];\n\n      MRB_METHOD_FROM_PROC(m, p);\n      mrb_define_method_raw(mrb, target, mid, m);\n      mrb_method_added(mrb, target, mid);\n      mrb_gc_arena_restore(mrb, ai);\n      regs[a] = mrb_symbol_value(mid);\n      NEXT;\n    }\n\n    CASE(OP_SCLASS, B) {\n      regs[a] = mrb_singleton_class(mrb, regs[a]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_TCLASS, B) {\n      struct RClass *target = check_target_class(mrb);\n      if (!target) goto L_RAISE;\n      regs[a] = mrb_obj_value(target);\n      NEXT;\n    }\n\n    CASE(OP_ALIAS, BB) {\n      struct RClass *target = check_target_class(mrb);\n\n      if (!target) goto L_RAISE;\n      mrb_alias_method(mrb, target, syms[a], syms[b]);\n      mrb_method_added(mrb, target, syms[a]);\n      NEXT;\n    }\n    CASE(OP_UNDEF, B) {\n      struct RClass *target = check_target_class(mrb);\n\n      if (!target) goto L_RAISE;\n      mrb_undef_method_id(mrb, target, syms[a]);\n      NEXT;\n    }\n\n    CASE(OP_DEBUG, Z) {\n      FETCH_BBB();\n#ifdef MRB_USE_DEBUG_HOOK\n      mrb->debug_op_hook(mrb, irep, pc, regs);\n#else\n#ifndef MRB_NO_STDIO\n      printf(\"OP_DEBUG %d %d %d\\n\", a, b, c);\n#else\n      abort();\n#endif\n#endif\n      NEXT;\n    }\n\n    CASE(OP_ERR, B) {\n      size_t len = pool[a].tt >> 2;\n      mrb_value exc;\n\n      mrb_assert((pool[a].tt&IREP_TT_NFLAG)==0);\n      exc = mrb_exc_new(mrb, E_LOCALJUMP_ERROR, pool[a].u.str, len);\n      mrb_exc_set(mrb, exc);\n      goto L_RAISE;\n    }\n\n    CASE(OP_EXT1, Z) {\n      insn = READ_B();\n      switch (insn) {\n#define OPCODE(insn,ops) case OP_ ## insn: FETCH_ ## ops ## _1(); mrb->c->ci->pc = pc; goto L_OP_ ## insn ## _BODY;\n#include \"mruby/ops.h\"\n#undef OPCODE\n      }\n      pc--;\n      NEXT;\n    }\n    CASE(OP_EXT2, Z) {\n      insn = READ_B();\n      switch (insn) {\n#define OPCODE(insn,ops) case OP_ ## insn: FETCH_ ## ops ## _2(); mrb->c->ci->pc = pc; goto L_OP_ ## insn ## _BODY;\n#include \"mruby/ops.h\"\n#undef OPCODE\n      }\n      pc--;\n      NEXT;\n    }\n    CASE(OP_EXT3, Z) {\n      uint8_t insn = READ_B();\n      switch (insn) {\n#define OPCODE(insn,ops) case OP_ ## insn: FETCH_ ## ops ## _3(); mrb->c->ci->pc = pc; goto L_OP_ ## insn ## _BODY;\n#include \"mruby/ops.h\"\n#undef OPCODE\n      }\n      pc--;\n      NEXT;\n    }\n\n    CASE(OP_STOP, Z) {\n      /*        stop VM */\n      CHECKPOINT_RESTORE(RBREAK_TAG_STOP) {\n        /* do nothing */\n      }\n      CHECKPOINT_MAIN(RBREAK_TAG_STOP) {\n        UNWIND_ENSURE(mrb, mrb->c->ci, pc, RBREAK_TAG_STOP, proc, mrb_nil_value());\n      }\n      CHECKPOINT_END(RBREAK_TAG_STOP);\n    L_STOP:\n      mrb->jmp = prev_jmp;\n      if (mrb->exc) {\n        mrb_assert(mrb->exc->tt == MRB_TT_EXCEPTION);\n        return mrb_obj_value(mrb->exc);\n      }\n      return regs[irep->nlocals];\n    }\n  }\n  END_DISPATCH;\n#undef regs\n  }\n  MRB_CATCH(&c_jmp) {\n    mrb_callinfo *ci = mrb->c->ci;\n    while (ci > mrb->c->cibase && ci->cci == CINFO_DIRECT) {\n      ci = cipop(mrb);\n    }\n    exc_catched = TRUE;\n    pc = ci->pc;\n    goto RETRY_TRY_BLOCK;\n  }\n  MRB_END_EXC(&c_jmp);\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145020,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "mrb_vm_exec(mrb_state *mrb, const struct RProc *proc, const mrb_code *pc)\n{\n  /* mrb_assert(MRB_PROC_CFUNC_P(proc)) */\n  const mrb_irep *irep = proc->body.irep;\n  const mrb_pool_value *pool = irep->pool;\n  const mrb_sym *syms = irep->syms;\n  mrb_code insn;\n  int ai = mrb_gc_arena_save(mrb);\n  struct mrb_jmpbuf *prev_jmp = mrb->jmp;\n  struct mrb_jmpbuf c_jmp;\n  uint32_t a;\n  uint16_t b;\n  uint16_t c;\n  mrb_sym mid;\n  const struct mrb_irep_catch_handler *ch;\n\n#ifdef DIRECT_THREADED\n  static const void * const optable[] = {\n#define OPCODE(x,_) &&L_OP_ ## x,\n#include \"mruby/ops.h\"\n#undef OPCODE\n  };\n#endif\n\n  mrb_bool exc_catched = FALSE;\nRETRY_TRY_BLOCK:\n\n  MRB_TRY(&c_jmp) {\n\n  if (exc_catched) {\n    exc_catched = FALSE;\n    mrb_gc_arena_restore(mrb, ai);\n    if (mrb->exc && mrb->exc->tt == MRB_TT_BREAK)\n      goto L_BREAK;\n    goto L_RAISE;\n  }\n  mrb->jmp = &c_jmp;\n  mrb_vm_ci_proc_set(mrb->c->ci, proc);\n\n#define regs (mrb->c->ci->stack)\n  INIT_DISPATCH {\n    CASE(OP_NOP, Z) {\n      /* do nothing */\n      NEXT;\n    }\n\n    CASE(OP_MOVE, BB) {\n      regs[a] = regs[b];\n      NEXT;\n    }\n\n    CASE(OP_LOADL, BB) {\n      switch (pool[b].tt) {   /* number */\n      case IREP_TT_INT32:\n        regs[a] = mrb_int_value(mrb, (mrb_int)pool[b].u.i32);\n        break;\n      case IREP_TT_INT64:\n#if defined(MRB_INT64)\n        regs[a] = mrb_int_value(mrb, (mrb_int)pool[b].u.i64);\n        break;\n#else\n#if defined(MRB_64BIT)\n        if (INT32_MIN <= pool[b].u.i64 && pool[b].u.i64 <= INT32_MAX) {\n          regs[a] = mrb_int_value(mrb, (mrb_int)pool[b].u.i64);\n          break;\n        }\n#endif\n        goto L_INT_OVERFLOW;\n#endif\n      case IREP_TT_BIGINT:\n        goto L_INT_OVERFLOW;\n#ifndef MRB_NO_FLOAT\n      case IREP_TT_FLOAT:\n        regs[a] = mrb_float_value(mrb, pool[b].u.f);\n        break;\n#endif\n      default:\n        /* should not happen (tt:string) */\n        regs[a] = mrb_nil_value();\n        break;\n      }\n      NEXT;\n    }\n\n    CASE(OP_LOADI, BB) {\n      SET_FIXNUM_VALUE(regs[a], b);\n      NEXT;\n    }\n\n    CASE(OP_LOADINEG, BB) {\n      SET_FIXNUM_VALUE(regs[a], -b);\n      NEXT;\n    }\n\n    CASE(OP_LOADI__1,B) goto L_LOADI;\n    CASE(OP_LOADI_0,B) goto L_LOADI;\n    CASE(OP_LOADI_1,B) goto L_LOADI;\n    CASE(OP_LOADI_2,B) goto L_LOADI;\n    CASE(OP_LOADI_3,B) goto L_LOADI;\n    CASE(OP_LOADI_4,B) goto L_LOADI;\n    CASE(OP_LOADI_5,B) goto L_LOADI;\n    CASE(OP_LOADI_6,B) goto L_LOADI;\n    CASE(OP_LOADI_7, B) {\n    L_LOADI:\n      SET_FIXNUM_VALUE(regs[a], (mrb_int)insn - (mrb_int)OP_LOADI_0);\n      NEXT;\n    }\n\n    CASE(OP_LOADI16, BS) {\n      SET_FIXNUM_VALUE(regs[a], (mrb_int)(int16_t)b);\n      NEXT;\n    }\n\n    CASE(OP_LOADI32, BSS) {\n      SET_INT_VALUE(mrb, regs[a], (int32_t)(((uint32_t)b<<16)+c));\n      NEXT;\n    }\n\n    CASE(OP_LOADSYM, BB) {\n      SET_SYM_VALUE(regs[a], syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_LOADNIL, B) {\n      SET_NIL_VALUE(regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_LOADSELF, B) {\n      regs[a] = regs[0];\n      NEXT;\n    }\n\n    CASE(OP_LOADT, B) {\n      SET_TRUE_VALUE(regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_LOADF, B) {\n      SET_FALSE_VALUE(regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETGV, BB) {\n      mrb_value val = mrb_gv_get(mrb, syms[b]);\n      regs[a] = val;\n      NEXT;\n    }\n\n    CASE(OP_SETGV, BB) {\n      mrb_gv_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETSV, BB) {\n      mrb_value val = mrb_vm_special_get(mrb, syms[b]);\n      regs[a] = val;\n      NEXT;\n    }\n\n    CASE(OP_SETSV, BB) {\n      mrb_vm_special_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETIV, BB) {\n      regs[a] = mrb_iv_get(mrb, regs[0], syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_SETIV, BB) {\n      mrb_iv_set(mrb, regs[0], syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETCV, BB) {\n      mrb_value val;\n      val = mrb_vm_cv_get(mrb, syms[b]);\n      regs[a] = val;\n      NEXT;\n    }\n\n    CASE(OP_SETCV, BB) {\n      mrb_vm_cv_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETIDX, B) {\n      mrb_value va = regs[a], vb = regs[a+1];\n      switch (mrb_type(va)) {\n      case MRB_TT_ARRAY:\n        if (!mrb_integer_p(vb)) goto getidx_fallback;\n        regs[a] = mrb_ary_entry(va, mrb_integer(vb));\n        break;\n      case MRB_TT_HASH:\n        regs[a] = mrb_hash_get(mrb, va, vb);\n        break;\n      case MRB_TT_STRING:\n        switch (mrb_type(vb)) {\n        case MRB_TT_INTEGER:\n        case MRB_TT_STRING:\n        case MRB_TT_RANGE:\n          regs[a] = mrb_str_aref(mrb, va, vb, mrb_undef_value());\n          break;\n        default:\n          goto getidx_fallback;\n        }\n        break;\n      default:\n      getidx_fallback:\n        mid = MRB_OPSYM(aref);\n        goto L_SEND_SYM;\n      }\n      NEXT;\n    }\n\n    CASE(OP_SETIDX, B) {\n      c = 2;\n      mid = MRB_OPSYM(aset);\n      SET_NIL_VALUE(regs[a+3]);\n      goto L_SENDB_SYM;\n    }\n\n    CASE(OP_GETCONST, BB) {\n      regs[a] = mrb_vm_const_get(mrb, syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_SETCONST, BB) {\n      mrb_vm_const_set(mrb, syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETMCNST, BB) {\n      regs[a] = mrb_const_get(mrb, regs[a], syms[b]);\n      NEXT;\n    }\n\n    CASE(OP_SETMCNST, BB) {\n      mrb_const_set(mrb, regs[a+1], syms[b], regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_GETUPVAR, BBB) {\n      mrb_value *regs_a = regs + a;\n      struct REnv *e = uvenv(mrb, c);\n\n      if (e && b < MRB_ENV_LEN(e)) {\n        *regs_a = e->stack[b];\n      }\n      else {\n        *regs_a = mrb_nil_value();\n      }\n      NEXT;\n    }\n\n    CASE(OP_SETUPVAR, BBB) {\n      struct REnv *e = uvenv(mrb, c);\n\n      if (e) {\n        mrb_value *regs_a = regs + a;\n\n        if (b < MRB_ENV_LEN(e)) {\n          e->stack[b] = *regs_a;\n          mrb_write_barrier(mrb, (struct RBasic*)e);\n        }\n      }\n      NEXT;\n    }\n\n    CASE(OP_JMP, S) {\n      pc += (int16_t)a;\n      JUMP;\n    }\n    CASE(OP_JMPIF, BS) {\n      if (mrb_test(regs[a])) {\n        pc += (int16_t)b;\n        JUMP;\n      }\n      NEXT;\n    }\n    CASE(OP_JMPNOT, BS) {\n      if (!mrb_test(regs[a])) {\n        pc += (int16_t)b;\n        JUMP;\n      }\n      NEXT;\n    }\n    CASE(OP_JMPNIL, BS) {\n      if (mrb_nil_p(regs[a])) {\n        pc += (int16_t)b;\n        JUMP;\n      }\n      NEXT;\n    }\n\n    CASE(OP_JMPUW, S) {\n      a = (uint32_t)((pc - irep->iseq) + (int16_t)a);\n      CHECKPOINT_RESTORE(RBREAK_TAG_JUMP) {\n        struct RBreak *brk = (struct RBreak*)mrb->exc;\n        mrb_value target = mrb_break_value_get(brk);\n        mrb_assert(mrb_integer_p(target));\n        a = (uint32_t)mrb_integer(target);\n        mrb_assert(a >= 0 && a < irep->ilen);\n      }\n      CHECKPOINT_MAIN(RBREAK_TAG_JUMP) {\n        ch = catch_handler_find(mrb, mrb->c->ci, pc, MRB_CATCH_FILTER_ENSURE);\n        if (ch) {\n          /* avoiding a jump from a catch handler into the same handler */\n          if (a < mrb_irep_catch_handler_unpack(ch->begin) || a >= mrb_irep_catch_handler_unpack(ch->end)) {\n            THROW_TAGGED_BREAK(mrb, RBREAK_TAG_JUMP, proc, mrb_fixnum_value(a));\n          }\n        }\n      }\n      CHECKPOINT_END(RBREAK_TAG_JUMP);\n\n      mrb->exc = NULL; /* clear break object */\n      pc = irep->iseq + a;\n      JUMP;\n    }\n\n    CASE(OP_EXCEPT, B) {\n      mrb_value exc;\n\n      if (mrb->exc == NULL) {\n        exc = mrb_nil_value();\n      }\n      else {\n        switch (mrb->exc->tt) {\n        case MRB_TT_BREAK:\n        case MRB_TT_EXCEPTION:\n          exc = mrb_obj_value(mrb->exc);\n          break;\n        default:\n          mrb_assert(!\"bad mrb_type\");\n          exc = mrb_nil_value();\n          break;\n        }\n        mrb->exc = NULL;\n      }\n      regs[a] = exc;\n      NEXT;\n    }\n    CASE(OP_RESCUE, BB) {\n      mrb_value exc = regs[a];  /* exc on stack */\n      mrb_value e = regs[b];\n      struct RClass *ec;\n\n      switch (mrb_type(e)) {\n      case MRB_TT_CLASS:\n      case MRB_TT_MODULE:\n        break;\n      default:\n        {\n          mrb_value exc;\n\n          exc = mrb_exc_new_lit(mrb, E_TYPE_ERROR,\n                                    \"class or module required for rescue clause\");\n          mrb_exc_set(mrb, exc);\n          goto L_RAISE;\n        }\n      }\n      ec = mrb_class_ptr(e);\n      regs[b] = mrb_bool_value(mrb_obj_is_kind_of(mrb, exc, ec));\n      NEXT;\n    }\n\n    CASE(OP_RAISEIF, B) {\n      mrb_value exc = regs[a];\n      if (mrb_break_p(exc)) {\n        mrb->exc = mrb_obj_ptr(exc);\n        goto L_BREAK;\n      }\n      mrb_exc_set(mrb, exc);\n      if (mrb->exc) {\n        goto L_RAISE;\n      }\n      NEXT;\n    }\n\n    CASE(OP_SSEND, BBB) {\n      regs[a] = regs[0];\n      insn = OP_SEND;\n    }\n    goto L_SENDB;\n\n    CASE(OP_SSENDB, BBB) {\n      regs[a] = regs[0];\n    }\n    goto L_SENDB;\n\n    CASE(OP_SEND, BBB)\n    goto L_SENDB;\n\n    L_SEND_SYM:\n    c = 1;\n    /* push nil after arguments */\n    SET_NIL_VALUE(regs[a+2]);\n    goto L_SENDB_SYM;\n\n    CASE(OP_SENDB, BBB)\n    L_SENDB:\n    mid = syms[b];\n    L_SENDB_SYM:\n    {\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_method_t m;\n      struct RClass *cls;\n      mrb_value recv, blk;\n\n      ARGUMENT_NORMALIZE(a, &c, insn);\n\n      recv = regs[a];\n      cls = mrb_class(mrb, recv);\n      m = mrb_method_search_vm(mrb, &cls, mid);\n      if (MRB_METHOD_UNDEF_P(m)) {\n        m = prepare_missing(mrb, recv, mid, &cls, a, &c, blk, 0);\n        mid = MRB_SYM(method_missing);\n      }\n\n      /* push callinfo */\n      ci = cipush(mrb, a, 0, cls, NULL, mid, c);\n\n      if (MRB_METHOD_CFUNC_P(m)) {\n        if (MRB_METHOD_PROC_P(m)) {\n          struct RProc *p = MRB_METHOD_PROC(m);\n\n          mrb_vm_ci_proc_set(ci, p);\n          recv = p->body.func(mrb, recv);\n        }\n        else {\n          if (MRB_METHOD_NOARG_P(m)) {\n            check_method_noarg(mrb, ci);\n          }\n          recv = MRB_METHOD_FUNC(m)(mrb, recv);\n        }\n        mrb_gc_arena_shrink(mrb, ai);\n        if (mrb->exc) goto L_RAISE;\n        ci = mrb->c->ci;\n        if (mrb_proc_p(blk)) {\n          struct RProc *p = mrb_proc_ptr(blk);\n          if (p && !MRB_PROC_STRICT_P(p) && MRB_PROC_ENV(p) == mrb_vm_ci_env(&ci[-1])) {\n            p->flags |= MRB_PROC_ORPHAN;\n          }\n        }\n        if (!ci->u.target_class) { /* return from context modifying method (resume/yield) */\n          if (ci->cci == CINFO_RESUMED) {\n            mrb->jmp = prev_jmp;\n            return recv;\n          }\n          else {\n            mrb_assert(!MRB_PROC_CFUNC_P(ci[-1].proc));\n            proc = ci[-1].proc;\n            irep = proc->body.irep;\n            pool = irep->pool;\n            syms = irep->syms;\n          }\n        }\n        ci->stack[0] = recv;\n        /* pop stackpos */\n        ci = cipop(mrb);\n        pc = ci->pc;\n      }\n      else {\n        /* setup environment for calling method */\n        mrb_vm_ci_proc_set(ci, (proc = MRB_METHOD_PROC(m)));\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n        mrb_stack_extend(mrb, (irep->nregs < 4) ? 4 : irep->nregs);\n        pc = irep->iseq;\n      }\n    }\n    JUMP;\n\n    CASE(OP_CALL, Z) {\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_value recv = ci->stack[0];\n      struct RProc *m = mrb_proc_ptr(recv);\n\n      /* replace callinfo */\n      ci->u.target_class = MRB_PROC_TARGET_CLASS(m);\n      mrb_vm_ci_proc_set(ci, m);\n      if (MRB_PROC_ENV_P(m)) {\n        ci->mid = MRB_PROC_ENV(m)->mid;\n      }\n\n      /* prepare stack */\n      if (MRB_PROC_CFUNC_P(m)) {\n        recv = MRB_PROC_CFUNC(m)(mrb, recv);\n        mrb_gc_arena_shrink(mrb, ai);\n        if (mrb->exc) goto L_RAISE;\n        /* pop stackpos */\n        ci = cipop(mrb);\n        pc = ci->pc;\n        ci[1].stack[0] = recv;\n        irep = mrb->c->ci->proc->body.irep;\n      }\n      else {\n        /* setup environment for calling method */\n        proc = m;\n        irep = m->body.irep;\n        if (!irep) {\n          mrb->c->ci->stack[0] = mrb_nil_value();\n          a = 0;\n          c = OP_R_NORMAL;\n          goto L_OP_RETURN_BODY;\n        }\n        mrb_int nargs = mrb_ci_bidx(ci)+1;\n        if (nargs < irep->nregs) {\n          mrb_stack_extend(mrb, irep->nregs);\n          stack_clear(regs+nargs, irep->nregs-nargs);\n        }\n        if (MRB_PROC_ENV_P(m)) {\n          regs[0] = MRB_PROC_ENV(m)->stack[0];\n        }\n        pc = irep->iseq;\n      }\n      pool = irep->pool;\n      syms = irep->syms;\n      JUMP;\n    }\n\n    CASE(OP_SUPER, BB) {\n      mrb_method_t m;\n      struct RClass *cls;\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_value recv, blk;\n      const struct RProc *p = ci->proc;\n      mrb_sym mid = ci->mid;\n      struct RClass* target_class = MRB_PROC_TARGET_CLASS(p);\n\n      if (MRB_PROC_ENV_P(p) && p->e.env->mid && p->e.env->mid != mid) { /* alias support */\n        mid = p->e.env->mid;    /* restore old mid */\n      }\n\n      if (mid == 0 || !target_class) {\n        mrb_value exc = mrb_exc_new_lit(mrb, E_NOMETHOD_ERROR, \"super called outside of method\");\n        mrb_exc_set(mrb, exc);\n        goto L_RAISE;\n      }\n      if (target_class->flags & MRB_FL_CLASS_IS_PREPENDED) {\n        target_class = mrb_vm_ci_target_class(ci);\n      }\n      else if (target_class->tt == MRB_TT_MODULE) {\n        target_class = mrb_vm_ci_target_class(ci);\n        if (target_class->tt != MRB_TT_ICLASS) {\n          goto super_typeerror;\n        }\n      }\n      recv = regs[0];\n      if (!mrb_obj_is_kind_of(mrb, recv, target_class)) {\n      super_typeerror: ;\n        mrb_value exc = mrb_exc_new_lit(mrb, E_TYPE_ERROR,\n                                            \"self has wrong type to call super in this context\");\n        mrb_exc_set(mrb, exc);\n        goto L_RAISE;\n      }\n\n      ARGUMENT_NORMALIZE(a, &b, OP_SUPER);\n\n      cls = target_class->super;\n      m = mrb_method_search_vm(mrb, &cls, mid);\n      if (MRB_METHOD_UNDEF_P(m)) {\n        m = prepare_missing(mrb, recv, mid, &cls, a, &b, blk, 1);\n        mid = MRB_SYM(method_missing);\n      }\n\n      /* push callinfo */\n      ci = cipush(mrb, a, 0, cls, NULL, mid, b);\n\n      /* prepare stack */\n      ci->stack[0] = recv;\n\n      if (MRB_METHOD_CFUNC_P(m)) {\n        mrb_value v;\n\n        if (MRB_METHOD_PROC_P(m)) {\n          mrb_vm_ci_proc_set(ci, MRB_METHOD_PROC(m));\n        }\n        v = MRB_METHOD_CFUNC(m)(mrb, recv);\n        mrb_gc_arena_restore(mrb, ai);\n        if (mrb->exc) goto L_RAISE;\n        ci = mrb->c->ci;\n        mrb_assert(!mrb_break_p(v));\n        if (!mrb_vm_ci_target_class(ci)) { /* return from context modifying method (resume/yield) */\n          if (ci->cci == CINFO_RESUMED) {\n            mrb->jmp = prev_jmp;\n            return v;\n          }\n          else {\n            mrb_assert(!MRB_PROC_CFUNC_P(ci[-1].proc));\n            proc = ci[-1].proc;\n            irep = proc->body.irep;\n            pool = irep->pool;\n            syms = irep->syms;\n          }\n        }\n        mrb->c->ci->stack[0] = v;\n        ci = cipop(mrb);\n        pc = ci->pc;\n      }\n      else {\n        /* setup environment for calling method */\n        mrb_vm_ci_proc_set(ci, (proc = MRB_METHOD_PROC(m)));\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n        mrb_stack_extend(mrb, (irep->nregs < 4) ? 4 : irep->nregs);\n        pc = irep->iseq;\n      }\n      JUMP;\n    }\n\n    CASE(OP_ARGARY, BS) {\n      mrb_int m1 = (b>>11)&0x3f;\n      mrb_int r  = (b>>10)&0x1;\n      mrb_int m2 = (b>>5)&0x1f;\n      mrb_int kd = (b>>4)&0x1;\n      mrb_int lv = (b>>0)&0xf;\n      mrb_value *stack;\n\n      if (mrb->c->ci->mid == 0 || mrb_vm_ci_target_class(mrb->c->ci) == NULL) {\n        mrb_value exc;\n\n      L_NOSUPER:\n        exc = mrb_exc_new_lit(mrb, E_NOMETHOD_ERROR, \"super called outside of method\");\n        mrb_exc_set(mrb, exc);\n        goto L_RAISE;\n      }\n      if (lv == 0) stack = regs + 1;\n      else {\n        struct REnv *e = uvenv(mrb, lv-1);\n        if (!e) goto L_NOSUPER;\n        if (MRB_ENV_LEN(e) <= m1+r+m2+1)\n          goto L_NOSUPER;\n        stack = e->stack + 1;\n      }\n      if (r == 0) {\n        regs[a] = mrb_ary_new_from_values(mrb, m1+m2, stack);\n      }\n      else {\n        mrb_value *pp = NULL;\n        struct RArray *rest;\n        mrb_int len = 0;\n\n        if (mrb_array_p(stack[m1])) {\n          struct RArray *ary = mrb_ary_ptr(stack[m1]);\n\n          pp = ARY_PTR(ary);\n          len = ARY_LEN(ary);\n        }\n        regs[a] = mrb_ary_new_capa(mrb, m1+len+m2);\n        rest = mrb_ary_ptr(regs[a]);\n        if (m1 > 0) {\n          stack_copy(ARY_PTR(rest), stack, m1);\n        }\n        if (len > 0) {\n          stack_copy(ARY_PTR(rest)+m1, pp, len);\n        }\n        if (m2 > 0) {\n          stack_copy(ARY_PTR(rest)+m1+len, stack+m1+1, m2);\n        }\n        ARY_SET_LEN(rest, m1+len+m2);\n      }\n      if (kd) {\n        regs[a+1] = stack[m1+r+m2];\n        regs[a+2] = stack[m1+r+m2+1];\n      }\n      else {\n        regs[a+1] = stack[m1+r+m2];\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_ENTER, W) {\n      mrb_int m1 = MRB_ASPEC_REQ(a);\n      mrb_int o  = MRB_ASPEC_OPT(a);\n      mrb_int r  = MRB_ASPEC_REST(a);\n      mrb_int m2 = MRB_ASPEC_POST(a);\n      mrb_int kd = (MRB_ASPEC_KEY(a) > 0 || MRB_ASPEC_KDICT(a))? 1 : 0;\n      /* unused\n      int b  = MRB_ASPEC_BLOCK(a);\n      */\n      mrb_int const len = m1 + o + r + m2;\n\n      mrb_callinfo *ci = mrb->c->ci;\n      mrb_int argc = ci->n;\n      mrb_value *argv = regs+1;\n      mrb_value * const argv0 = argv;\n      mrb_int const kw_pos = len + kd;    /* where kwhash should be */\n      mrb_int const blk_pos = kw_pos + 1; /* where block should be */\n      mrb_value blk = regs[mrb_ci_bidx(ci)];\n      mrb_value kdict = mrb_nil_value();\n\n      /* keyword arguments */\n      if (ci->nk > 0) {\n        mrb_int kidx = mrb_ci_kidx(ci);\n        kdict = regs[kidx];\n        if (!mrb_hash_p(kdict) || mrb_hash_size(mrb, kdict) == 0) {\n          kdict = mrb_nil_value();\n          ci->nk = 0;\n        }\n      }\n      if (!kd && !mrb_nil_p(kdict)) {\n        if (argc < 14) {\n          ci->n++;\n          argc++;    /* include kdict in normal arguments */\n        }\n        else if (argc == 14) {\n          /* pack arguments and kdict */\n          regs[1] = mrb_ary_new_from_values(mrb, argc+1, &regs[1]);\n          argc = ci->n = 15;\n        }\n        else {/* argc == 15 */\n          /* push kdict to packed arguments */\n          mrb_ary_push(mrb, regs[1], regs[2]);\n        }\n        ci->nk = 0;\n      }\n      if (kd && MRB_ASPEC_KEY(a) > 0 && mrb_hash_p(kdict)) {\n        kdict = mrb_hash_dup(mrb, kdict);\n      }\n\n      /* arguments is passed with Array */\n      if (argc == 15) {\n        struct RArray *ary = mrb_ary_ptr(regs[1]);\n        argv = ARY_PTR(ary);\n        argc = (int)ARY_LEN(ary);\n        mrb_gc_protect(mrb, regs[1]);\n      }\n\n      /* strict argument check */\n      if (ci->proc && MRB_PROC_STRICT_P(ci->proc)) {\n        if (argc < m1 + m2 || (r == 0 && argc > len)) {\n          argnum_error(mrb, m1+m2);\n          goto L_RAISE;\n        }\n      }\n      /* extract first argument array to arguments */\n      else if (len > 1 && argc == 1 && mrb_array_p(argv[0])) {\n        mrb_gc_protect(mrb, argv[0]);\n        argc = (int)RARRAY_LEN(argv[0]);\n        argv = RARRAY_PTR(argv[0]);\n      }\n\n      /* rest arguments */\n      mrb_value rest = mrb_nil_value();\n      if (argc < len) {\n        mrb_int mlen = m2;\n        if (argc < m1+m2) {\n          mlen = m1 < argc ? argc - m1 : 0;\n        }\n\n        /* copy mandatory and optional arguments */\n        if (argv0 != argv && argv) {\n          value_move(&regs[1], argv, argc-mlen); /* m1 + o */\n        }\n        if (argc < m1) {\n          stack_clear(&regs[argc+1], m1-argc);\n        }\n        /* copy post mandatory arguments */\n        if (mlen) {\n          value_move(&regs[len-m2+1], &argv[argc-mlen], mlen);\n        }\n        if (mlen < m2) {\n          stack_clear(&regs[len-m2+mlen+1], m2-mlen);\n        }\n        /* initialize rest arguments with empty Array */\n        if (r) {\n          rest = mrb_ary_new_capa(mrb, 0);\n          regs[m1+o+1] = rest;\n        }\n        /* skip initializer of passed arguments */\n        if (o > 0 && argc > m1+m2)\n          pc += (argc - m1 - m2)*3;\n      }\n      else {\n        mrb_int rnum = 0;\n        if (argv0 != argv) {\n          value_move(&regs[1], argv, m1+o);\n        }\n        if (r) {\n          rnum = argc-m1-o-m2;\n          rest = mrb_ary_new_from_values(mrb, rnum, argv+m1+o);\n          regs[m1+o+1] = rest;\n        }\n        if (m2 > 0 && argc-m2 > m1) {\n          value_move(&regs[m1+o+r+1], &argv[m1+o+rnum], m2);\n        }\n        pc += o*3;\n      }\n\n      /* need to be update blk first to protect blk from GC */\n      regs[blk_pos] = blk;              /* move block */\n      if (kd) {\n        if (mrb_nil_p(kdict))\n          kdict = mrb_hash_new_capa(mrb, 0);\n        regs[kw_pos] = kdict;           /* set kwhash */\n      }\n\n      /* format arguments for generated code */\n      mrb->c->ci->n = len;\n\n      /* clear local (but non-argument) variables */\n      if (irep->nlocals-blk_pos-1 > 0) {\n        stack_clear(&regs[blk_pos+1], irep->nlocals-blk_pos-1);\n      }\n      JUMP;\n    }\n\n    CASE(OP_KARG, BB) {\n      mrb_value k = mrb_symbol_value(syms[b]);\n      mrb_int kidx = mrb_ci_kidx(mrb->c->ci);\n      mrb_value kdict;\n\n      if (kidx < 0 || !mrb_hash_p(kdict=regs[kidx]) || !mrb_hash_key_p(mrb, kdict, k)) {\n        mrb_value str = mrb_format(mrb, \"missing keyword: %v\", k);\n        mrb_exc_set(mrb, mrb_exc_new_str(mrb, E_ARGUMENT_ERROR, str));\n        goto L_RAISE;\n      }\n      regs[a] = mrb_hash_get(mrb, kdict, k);\n      mrb_hash_delete_key(mrb, kdict, k);\n      NEXT;\n    }\n\n    CASE(OP_KEY_P, BB) {\n      mrb_value k = mrb_symbol_value(syms[b]);\n      mrb_int kidx = mrb_ci_kidx(mrb->c->ci);\n      mrb_value kdict;\n      mrb_bool key_p = FALSE;\n\n      if (kidx >= 0 && mrb_hash_p(kdict=regs[kidx])) {\n        key_p = mrb_hash_key_p(mrb, kdict, k);\n      }\n      regs[a] = mrb_bool_value(key_p);\n      NEXT;\n    }\n\n    CASE(OP_KEYEND, Z) {\n      mrb_int kidx = mrb_ci_kidx(mrb->c->ci);\n      mrb_value kdict;\n\n      if (kidx >= 0 && mrb_hash_p(kdict=regs[kidx]) && !mrb_hash_empty_p(mrb, kdict)) {\n        mrb_value keys = mrb_hash_keys(mrb, kdict);\n        mrb_value key1 = RARRAY_PTR(keys)[0];\n        mrb_value str = mrb_format(mrb, \"unknown keyword: %v\", key1);\n        mrb_exc_set(mrb, mrb_exc_new_str(mrb, E_ARGUMENT_ERROR, str));\n        goto L_RAISE;\n      }\n      NEXT;\n    }\n\n    CASE(OP_BREAK, B) {\n      c = OP_R_BREAK;\n      goto L_RETURN;\n    }\n    CASE(OP_RETURN_BLK, B) {\n      c = OP_R_RETURN;\n      goto L_RETURN;\n    }\n    CASE(OP_RETURN, B)\n    c = OP_R_NORMAL;\n    L_RETURN:\n    {\n      mrb_callinfo *ci;\n\n      ci = mrb->c->ci;\n      if (ci->mid) {\n        mrb_value blk = regs[mrb_ci_bidx(ci)];\n\n        if (mrb_proc_p(blk)) {\n          struct RProc *p = mrb_proc_ptr(blk);\n\n          if (!MRB_PROC_STRICT_P(p) &&\n              ci > mrb->c->cibase && MRB_PROC_ENV(p) == mrb_vm_ci_env(&ci[-1])) {\n            p->flags |= MRB_PROC_ORPHAN;\n          }\n        }\n      }\n\n      if (mrb->exc) {\n      L_RAISE:\n        ci = mrb->c->ci;\n        if (ci == mrb->c->cibase) {\n          ch = catch_handler_find(mrb, ci, pc, MRB_CATCH_FILTER_ALL);\n          if (ch == NULL) goto L_FTOP;\n          goto L_CATCH;\n        }\n        while ((ch = catch_handler_find(mrb, ci, pc, MRB_CATCH_FILTER_ALL)) == NULL) {\n          ci = cipop(mrb);\n          if (ci[1].cci == CINFO_SKIP && prev_jmp) {\n            mrb->jmp = prev_jmp;\n            MRB_THROW(prev_jmp);\n          }\n          pc = ci[0].pc;\n          if (ci == mrb->c->cibase) {\n            ch = catch_handler_find(mrb, ci, pc, MRB_CATCH_FILTER_ALL);\n            if (ch == NULL) {\n            L_FTOP:             /* fiber top */\n              if (mrb->c == mrb->root_c) {\n                mrb->c->ci->stack = mrb->c->stbase;\n                goto L_STOP;\n              }\n              else {\n                struct mrb_context *c = mrb->c;\n\n                c->status = MRB_FIBER_TERMINATED;\n                mrb->c = c->prev;\n                c->prev = NULL;\n                goto L_RAISE;\n              }\n            }\n            break;\n          }\n        }\n      L_CATCH:\n        if (ch == NULL) goto L_STOP;\n        if (FALSE) {\n        L_CATCH_TAGGED_BREAK: /* from THROW_TAGGED_BREAK() or UNWIND_ENSURE() */\n          ci = mrb->c->ci;\n        }\n        proc = ci->proc;\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n        mrb_stack_extend(mrb, irep->nregs);\n        pc = irep->iseq + mrb_irep_catch_handler_unpack(ch->target);\n      }\n      else {\n        mrb_int acc;\n        mrb_value v;\n\n        ci = mrb->c->ci;\n        v = regs[a];\n        mrb_gc_protect(mrb, v);\n        switch (c) {\n        case OP_R_RETURN:\n          /* Fall through to OP_R_NORMAL otherwise */\n          if (ci->cci == CINFO_NONE && MRB_PROC_ENV_P(proc) && !MRB_PROC_STRICT_P(proc)) {\n            const struct RProc *dst;\n            mrb_callinfo *cibase;\n            cibase = mrb->c->cibase;\n            dst = top_proc(mrb, proc);\n\n            if (MRB_PROC_ENV_P(dst)) {\n              struct REnv *e = MRB_PROC_ENV(dst);\n\n              if (!MRB_ENV_ONSTACK_P(e) || (e->cxt && e->cxt != mrb->c)) {\n                localjump_error(mrb, LOCALJUMP_ERROR_RETURN);\n                goto L_RAISE;\n              }\n            }\n            /* check jump destination */\n            while (cibase <= ci && ci->proc != dst) {\n              if (ci->cci > CINFO_NONE) { /* jump cross C boundary */\n                localjump_error(mrb, LOCALJUMP_ERROR_RETURN);\n                goto L_RAISE;\n              }\n              ci--;\n            }\n            if (ci <= cibase) { /* no jump destination */\n              localjump_error(mrb, LOCALJUMP_ERROR_RETURN);\n              goto L_RAISE;\n            }\n            ci = mrb->c->ci;\n            while (cibase <= ci && ci->proc != dst) {\n              CHECKPOINT_RESTORE(RBREAK_TAG_RETURN_BLOCK) {\n                cibase = mrb->c->cibase;\n                dst = top_proc(mrb, proc);\n              }\n              CHECKPOINT_MAIN(RBREAK_TAG_RETURN_BLOCK) {\n                UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_RETURN_BLOCK, proc, v);\n              }\n              CHECKPOINT_END(RBREAK_TAG_RETURN_BLOCK);\n              ci = cipop(mrb);\n              pc = ci->pc;\n            }\n            proc = ci->proc;\n            mrb->exc = NULL; /* clear break object */\n            break;\n          }\n          /* fallthrough */\n        case OP_R_NORMAL:\n        NORMAL_RETURN:\n          if (ci == mrb->c->cibase) {\n            struct mrb_context *c;\n            c = mrb->c;\n\n            if (!c->prev) { /* toplevel return */\n              regs[irep->nlocals] = v;\n              goto CHECKPOINT_LABEL_MAKE(RBREAK_TAG_STOP);\n            }\n            if (!c->vmexec && c->prev->ci == c->prev->cibase) {\n              mrb_value exc = mrb_exc_new_lit(mrb, E_FIBER_ERROR, \"double resume\");\n              mrb_exc_set(mrb, exc);\n              goto L_RAISE;\n            }\n            CHECKPOINT_RESTORE(RBREAK_TAG_RETURN_TOPLEVEL) {\n              c = mrb->c;\n            }\n            CHECKPOINT_MAIN(RBREAK_TAG_RETURN_TOPLEVEL) {\n              UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_RETURN_TOPLEVEL, proc, v);\n            }\n            CHECKPOINT_END(RBREAK_TAG_RETURN_TOPLEVEL);\n            /* automatic yield at the end */\n            c->status = MRB_FIBER_TERMINATED;\n            mrb->c = c->prev;\n            mrb->c->status = MRB_FIBER_RUNNING;\n            c->prev = NULL;\n            if (c->vmexec) {\n              mrb_gc_arena_restore(mrb, ai);\n              c->vmexec = FALSE;\n              mrb->jmp = prev_jmp;\n              return v;\n            }\n            ci = mrb->c->ci;\n          }\n          CHECKPOINT_RESTORE(RBREAK_TAG_RETURN) {\n            /* do nothing */\n          }\n          CHECKPOINT_MAIN(RBREAK_TAG_RETURN) {\n            UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_RETURN, proc, v);\n          }\n          CHECKPOINT_END(RBREAK_TAG_RETURN);\n          mrb->exc = NULL; /* clear break object */\n          break;\n        case OP_R_BREAK:\n          if (MRB_PROC_STRICT_P(proc)) goto NORMAL_RETURN;\n          if (MRB_PROC_ORPHAN_P(proc)) {\n            mrb_value exc;\n\n          L_BREAK_ERROR:\n            exc = mrb_exc_new_lit(mrb, E_LOCALJUMP_ERROR,\n                                      \"break from proc-closure\");\n            mrb_exc_set(mrb, exc);\n            goto L_RAISE;\n          }\n          if (!MRB_PROC_ENV_P(proc) || !MRB_ENV_ONSTACK_P(MRB_PROC_ENV(proc))) {\n            goto L_BREAK_ERROR;\n          }\n          else {\n            struct REnv *e = MRB_PROC_ENV(proc);\n\n            if (e->cxt != mrb->c) {\n              goto L_BREAK_ERROR;\n            }\n          }\n          CHECKPOINT_RESTORE(RBREAK_TAG_BREAK) {\n            /* do nothing */\n          }\n          CHECKPOINT_MAIN(RBREAK_TAG_BREAK) {\n            UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_BREAK, proc, v);\n          }\n          CHECKPOINT_END(RBREAK_TAG_BREAK);\n          /* break from fiber block */\n          if (ci == mrb->c->cibase && ci->pc) {\n            struct mrb_context *c = mrb->c;\n\n            mrb->c = c->prev;\n            c->prev = NULL;\n            ci = mrb->c->ci;\n          }\n          if (ci->cci > CINFO_NONE) {\n            ci = cipop(mrb);\n            mrb_gc_arena_restore(mrb, ai);\n            mrb->c->vmexec = FALSE;\n            mrb->exc = (struct RObject*)break_new(mrb, RBREAK_TAG_BREAK, proc, v);\n            mrb->jmp = prev_jmp;\n            MRB_THROW(prev_jmp);\n          }\n          if (FALSE) {\n            struct RBreak *brk;\n\n          L_BREAK:\n            brk = (struct RBreak*)mrb->exc;\n            proc = mrb_break_proc_get(brk);\n            v = mrb_break_value_get(brk);\n            ci = mrb->c->ci;\n\n            switch (mrb_break_tag_get(brk)) {\n#define DISPATCH_CHECKPOINTS(n, i) case n: goto CHECKPOINT_LABEL_MAKE(n);\n              RBREAK_TAG_FOREACH(DISPATCH_CHECKPOINTS)\n#undef DISPATCH_CHECKPOINTS\n              default:\n                mrb_assert(!\"wrong break tag\");\n            }\n          }\n          while (mrb->c->cibase < ci && ci[-1].proc != proc->upper) {\n            if (ci[-1].cci == CINFO_SKIP) {\n              goto L_BREAK_ERROR;\n            }\n            CHECKPOINT_RESTORE(RBREAK_TAG_BREAK_UPPER) {\n              /* do nothing */\n            }\n            CHECKPOINT_MAIN(RBREAK_TAG_BREAK_UPPER) {\n              UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_BREAK_UPPER, proc, v);\n            }\n            CHECKPOINT_END(RBREAK_TAG_BREAK_UPPER);\n            ci = cipop(mrb);\n            pc = ci->pc;\n          }\n          CHECKPOINT_RESTORE(RBREAK_TAG_BREAK_INTARGET) {\n            /* do nothing */\n          }\n          CHECKPOINT_MAIN(RBREAK_TAG_BREAK_INTARGET) {\n            UNWIND_ENSURE(mrb, ci, pc, RBREAK_TAG_BREAK_INTARGET, proc, v);\n          }\n          CHECKPOINT_END(RBREAK_TAG_BREAK_INTARGET);\n          if (ci == mrb->c->cibase) {\n            goto L_BREAK_ERROR;\n          }\n          mrb->exc = NULL; /* clear break object */\n          break;\n        default:\n          /* cannot happen */\n          break;\n        }\n        mrb_assert(ci == mrb->c->ci);\n        mrb_assert(mrb->exc == NULL);\n\n        if (mrb->c->vmexec && !mrb_vm_ci_target_class(ci)) {\n          mrb_gc_arena_restore(mrb, ai);\n          mrb->c->vmexec = FALSE;\n          mrb->jmp = prev_jmp;\n          return v;\n        }\n        acc = ci->cci;\n        ci = cipop(mrb);\n        if (acc == CINFO_SKIP || acc == CINFO_DIRECT) {\n          mrb_gc_arena_restore(mrb, ai);\n          mrb->jmp = prev_jmp;\n          return v;\n        }\n        pc = ci->pc;\n        DEBUG(fprintf(stderr, \"from :%s\\n\", mrb_sym_name(mrb, ci->mid)));\n        proc = ci->proc;\n        irep = proc->body.irep;\n        pool = irep->pool;\n        syms = irep->syms;\n\n        ci[1].stack[0] = v;\n        mrb_gc_arena_restore(mrb, ai);\n      }\n      JUMP;\n    }\n\n    CASE(OP_BLKPUSH, BS) {\n      int m1 = (b>>11)&0x3f;\n      int r  = (b>>10)&0x1;\n      int m2 = (b>>5)&0x1f;\n      int kd = (b>>4)&0x1;\n      int lv = (b>>0)&0xf;\n      mrb_value *stack;\n\n      if (lv == 0) stack = regs + 1;\n      else {\n        struct REnv *e = uvenv(mrb, lv-1);\n        if (!e || (!MRB_ENV_ONSTACK_P(e) && e->mid == 0) ||\n            MRB_ENV_LEN(e) <= m1+r+m2+1) {\n          localjump_error(mrb, LOCALJUMP_ERROR_YIELD);\n          goto L_RAISE;\n        }\n        stack = e->stack + 1;\n      }\n      if (mrb_nil_p(stack[m1+r+m2+kd])) {\n        localjump_error(mrb, LOCALJUMP_ERROR_YIELD);\n        goto L_RAISE;\n      }\n      regs[a] = stack[m1+r+m2+kd];\n      NEXT;\n    }\n\n  L_INT_OVERFLOW:\n    {\n      mrb_value exc = mrb_exc_new_lit(mrb, E_RANGE_ERROR, \"integer overflow\");\n      mrb_exc_set(mrb, exc);\n    }\n    goto L_RAISE;\n\n#define TYPES2(a,b) ((((uint16_t)(a))<<8)|(((uint16_t)(b))&0xff))\n#define OP_MATH(op_name)                                                    \\\n  /* need to check if op is overridden */                                   \\\n  switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {                  \\\n    OP_MATH_CASE_INTEGER(op_name);                                          \\\n    OP_MATH_CASE_FLOAT(op_name, integer, float);                            \\\n    OP_MATH_CASE_FLOAT(op_name, float,  integer);                           \\\n    OP_MATH_CASE_FLOAT(op_name, float,  float);                             \\\n    OP_MATH_CASE_STRING_##op_name();                                        \\\n    default:                                                                \\\n      mid = MRB_OPSYM(op_name);                                             \\\n      goto L_SEND_SYM;                                                      \\\n  }                                                                         \\\n  NEXT;\n#define OP_MATH_CASE_INTEGER(op_name)                                       \\\n  case TYPES2(MRB_TT_INTEGER, MRB_TT_INTEGER):                              \\\n    {                                                                       \\\n      mrb_int x = mrb_integer(regs[a]), y = mrb_integer(regs[a+1]), z;      \\\n      if (mrb_int_##op_name##_overflow(x, y, &z))                           \\\n        OP_MATH_OVERFLOW_INT();                                             \\\n      else                                                                  \\\n        SET_INT_VALUE(mrb,regs[a], z);                                      \\\n    }                                                                       \\\n    break\n#ifdef MRB_NO_FLOAT\n#define OP_MATH_CASE_FLOAT(op_name, t1, t2) (void)0\n#else\n#define OP_MATH_CASE_FLOAT(op_name, t1, t2)                                     \\\n  case TYPES2(OP_MATH_TT_##t1, OP_MATH_TT_##t2):                                \\\n    {                                                                           \\\n      mrb_float z = mrb_##t1(regs[a]) OP_MATH_OP_##op_name mrb_##t2(regs[a+1]); \\\n      SET_FLOAT_VALUE(mrb, regs[a], z);                                         \\\n    }                                                                           \\\n    break\n#endif\n#define OP_MATH_OVERFLOW_INT() goto L_INT_OVERFLOW\n#define OP_MATH_CASE_STRING_add()                                           \\\n  case TYPES2(MRB_TT_STRING, MRB_TT_STRING):                                \\\n    regs[a] = mrb_str_plus(mrb, regs[a], regs[a+1]);                        \\\n    mrb_gc_arena_restore(mrb, ai);                                          \\\n    break\n#define OP_MATH_CASE_STRING_sub() (void)0\n#define OP_MATH_CASE_STRING_mul() (void)0\n#define OP_MATH_OP_add +\n#define OP_MATH_OP_sub -\n#define OP_MATH_OP_mul *\n#define OP_MATH_TT_integer MRB_TT_INTEGER\n#define OP_MATH_TT_float   MRB_TT_FLOAT\n\n    CASE(OP_ADD, B) {\n      OP_MATH(add);\n    }\n\n    CASE(OP_SUB, B) {\n      OP_MATH(sub);\n    }\n\n    CASE(OP_MUL, B) {\n      OP_MATH(mul);\n    }\n\n    CASE(OP_DIV, B) {\n#ifndef MRB_NO_FLOAT\n      mrb_float x, y, f;\n#endif\n\n      /* need to check if op is overridden */\n      switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {\n      case TYPES2(MRB_TT_INTEGER,MRB_TT_INTEGER):\n        {\n          mrb_int x = mrb_integer(regs[a]);\n          mrb_int y = mrb_integer(regs[a+1]);\n          mrb_int div = mrb_div_int(mrb, x, y);\n          SET_INT_VALUE(mrb, regs[a], div);\n        }\n        NEXT;\n#ifndef MRB_NO_FLOAT\n      case TYPES2(MRB_TT_INTEGER,MRB_TT_FLOAT):\n        x = (mrb_float)mrb_integer(regs[a]);\n        y = mrb_float(regs[a+1]);\n        break;\n      case TYPES2(MRB_TT_FLOAT,MRB_TT_INTEGER):\n        x = mrb_float(regs[a]);\n        y = (mrb_float)mrb_integer(regs[a+1]);\n        break;\n      case TYPES2(MRB_TT_FLOAT,MRB_TT_FLOAT):\n        x = mrb_float(regs[a]);\n        y = mrb_float(regs[a+1]);\n        break;\n#endif\n      default:\n        mid = MRB_OPSYM(div);\n        goto L_SEND_SYM;\n      }\n\n#ifndef MRB_NO_FLOAT\n      f = mrb_div_float(x, y);\n      SET_FLOAT_VALUE(mrb, regs[a], f);\n#endif\n      NEXT;\n    }\n\n#define OP_MATHI(op_name)                                                   \\\n  /* need to check if op is overridden */                                   \\\n  switch (mrb_type(regs[a])) {                                              \\\n    OP_MATHI_CASE_INTEGER(op_name);                                         \\\n    OP_MATHI_CASE_FLOAT(op_name);                                           \\\n    default:                                                                \\\n      SET_INT_VALUE(mrb,regs[a+1], b);                                      \\\n      mid = MRB_OPSYM(op_name);                                             \\\n      goto L_SEND_SYM;                                                      \\\n  }                                                                         \\\n  NEXT;\n#define OP_MATHI_CASE_INTEGER(op_name)                                      \\\n  case MRB_TT_INTEGER:                                                      \\\n    {                                                                       \\\n      mrb_int x = mrb_integer(regs[a]), y = (mrb_int)b, z;                  \\\n      if (mrb_int_##op_name##_overflow(x, y, &z))                           \\\n        OP_MATH_OVERFLOW_INT();                                             \\\n      else                                                                  \\\n        SET_INT_VALUE(mrb,regs[a], z);                                      \\\n    }                                                                       \\\n    break\n#ifdef MRB_NO_FLOAT\n#define OP_MATHI_CASE_FLOAT(op_name) (void)0\n#else\n#define OP_MATHI_CASE_FLOAT(op_name)                                        \\\n  case MRB_TT_FLOAT:                                                        \\\n    {                                                                       \\\n      mrb_float z = mrb_float(regs[a]) OP_MATH_OP_##op_name b;              \\\n      SET_FLOAT_VALUE(mrb, regs[a], z);                                     \\\n    }                                                                       \\\n    break\n#endif\n\n    CASE(OP_ADDI, BB) {\n      OP_MATHI(add);\n    }\n\n    CASE(OP_SUBI, BB) {\n      OP_MATHI(sub);\n    }\n\n#define OP_CMP_BODY(op,v1,v2) (v1(regs[a]) op v2(regs[a+1]))\n\n#ifdef MRB_NO_FLOAT\n#define OP_CMP(op,sym) do {\\\n  int result;\\\n  /* need to check if - is overridden */\\\n  switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {\\\n  case TYPES2(MRB_TT_INTEGER,MRB_TT_INTEGER):\\\n    result = OP_CMP_BODY(op,mrb_fixnum,mrb_fixnum);\\\n    break;\\\n  default:\\\n    mid = MRB_OPSYM(sym);\\\n    goto L_SEND_SYM;\\\n  }\\\n  if (result) {\\\n    SET_TRUE_VALUE(regs[a]);\\\n  }\\\n  else {\\\n    SET_FALSE_VALUE(regs[a]);\\\n  }\\\n} while(0)\n#else\n#define OP_CMP(op, sym) do {\\\n  int result;\\\n  /* need to check if - is overridden */\\\n  switch (TYPES2(mrb_type(regs[a]),mrb_type(regs[a+1]))) {\\\n  case TYPES2(MRB_TT_INTEGER,MRB_TT_INTEGER):\\\n    result = OP_CMP_BODY(op,mrb_fixnum,mrb_fixnum);\\\n    break;\\\n  case TYPES2(MRB_TT_INTEGER,MRB_TT_FLOAT):\\\n    result = OP_CMP_BODY(op,mrb_fixnum,mrb_float);\\\n    break;\\\n  case TYPES2(MRB_TT_FLOAT,MRB_TT_INTEGER):\\\n    result = OP_CMP_BODY(op,mrb_float,mrb_fixnum);\\\n    break;\\\n  case TYPES2(MRB_TT_FLOAT,MRB_TT_FLOAT):\\\n    result = OP_CMP_BODY(op,mrb_float,mrb_float);\\\n    break;\\\n  default:\\\n    mid = MRB_OPSYM(sym);\\\n    goto L_SEND_SYM;\\\n  }\\\n  if (result) {\\\n    SET_TRUE_VALUE(regs[a]);\\\n  }\\\n  else {\\\n    SET_FALSE_VALUE(regs[a]);\\\n  }\\\n} while(0)\n#endif\n\n    CASE(OP_EQ, B) {\n      if (mrb_obj_eq(mrb, regs[a], regs[a+1])) {\n        SET_TRUE_VALUE(regs[a]);\n      }\n      else {\n        OP_CMP(==,eq);\n      }\n      NEXT;\n    }\n\n    CASE(OP_LT, B) {\n      OP_CMP(<,lt);\n      NEXT;\n    }\n\n    CASE(OP_LE, B) {\n      OP_CMP(<=,le);\n      NEXT;\n    }\n\n    CASE(OP_GT, B) {\n      OP_CMP(>,gt);\n      NEXT;\n    }\n\n    CASE(OP_GE, B) {\n      OP_CMP(>=,ge);\n      NEXT;\n    }\n\n    CASE(OP_ARRAY, BB) {\n      regs[a] = mrb_ary_new_from_values(mrb, b, &regs[a]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n    CASE(OP_ARRAY2, BBB) {\n      regs[a] = mrb_ary_new_from_values(mrb, c, &regs[b]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_ARYCAT, B) {\n      mrb_value splat = mrb_ary_splat(mrb, regs[a+1]);\n      if (mrb_nil_p(regs[a])) {\n        regs[a] = splat;\n      }\n      else {\n        mrb_assert(mrb_array_p(regs[a]));\n        mrb_ary_concat(mrb, regs[a], splat);\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_ARYPUSH, BB) {\n      mrb_assert(mrb_array_p(regs[a]));\n      for (mrb_int i=0; i<b; i++) {\n        mrb_ary_push(mrb, regs[a], regs[a+i+1]);\n      }\n      NEXT;\n    }\n\n    CASE(OP_ARYDUP, B) {\n      mrb_value ary = regs[a];\n      if (mrb_array_p(ary)) {\n        ary = mrb_ary_new_from_values(mrb, RARRAY_LEN(ary), RARRAY_PTR(ary));\n      }\n      else {\n        ary = mrb_ary_new_from_values(mrb, 1, &ary);\n      }\n      regs[a] = ary;\n      NEXT;\n    }\n\n    CASE(OP_AREF, BBB) {\n      mrb_value v = regs[b];\n\n      if (!mrb_array_p(v)) {\n        if (c == 0) {\n          regs[a] = v;\n        }\n        else {\n          SET_NIL_VALUE(regs[a]);\n        }\n      }\n      else {\n        v = mrb_ary_ref(mrb, v, c);\n        regs[a] = v;\n      }\n      NEXT;\n    }\n\n    CASE(OP_ASET, BBB) {\n      mrb_assert(mrb_array_p(regs[a]));\n      mrb_ary_set(mrb, regs[b], c, regs[a]);\n      NEXT;\n    }\n\n    CASE(OP_APOST, BBB) {\n      mrb_value v = regs[a];\n      int pre  = b;\n      int post = c;\n      struct RArray *ary;\n      int len, idx;\n\n      if (!mrb_array_p(v)) {\n        v = mrb_ary_new_from_values(mrb, 1, &regs[a]);\n      }\n      ary = mrb_ary_ptr(v);\n      len = (int)ARY_LEN(ary);\n      if (len > pre + post) {\n        v = mrb_ary_new_from_values(mrb, len - pre - post, ARY_PTR(ary)+pre);\n        regs[a++] = v;\n        while (post--) {\n          regs[a++] = ARY_PTR(ary)[len-post-1];\n        }\n      }\n      else {\n        v = mrb_ary_new_capa(mrb, 0);\n        regs[a++] = v;\n        for (idx=0; idx+pre<len; idx++) {\n          regs[a+idx] = ARY_PTR(ary)[pre+idx];\n        }\n        while (idx < post) {\n          SET_NIL_VALUE(regs[a+idx]);\n          idx++;\n        }\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_INTERN, B) {\n      mrb_assert(mrb_string_p(regs[a]));\n      mrb_sym sym = mrb_intern_str(mrb, regs[a]);\n      regs[a] = mrb_symbol_value(sym);\n      NEXT;\n    }\n\n    CASE(OP_SYMBOL, BB) {\n      size_t len;\n      mrb_sym sym;\n\n      mrb_assert((pool[b].tt&IREP_TT_NFLAG)==0);\n      len = pool[b].tt >> 2;\n      if (pool[b].tt & IREP_TT_SFLAG) {\n        sym = mrb_intern_static(mrb, pool[b].u.str, len);\n      }\n      else {\n        sym  = mrb_intern(mrb, pool[b].u.str, len);\n      }\n      regs[a] = mrb_symbol_value(sym);\n      NEXT;\n    }\n\n    CASE(OP_STRING, BB) {\n      size_t len;\n\n      mrb_assert((pool[b].tt&IREP_TT_NFLAG)==0);\n      len = pool[b].tt >> 2;\n      if (pool[b].tt & IREP_TT_SFLAG) {\n        regs[a] = mrb_str_new_static(mrb, pool[b].u.str, len);\n      }\n      else {\n        regs[a] = mrb_str_new(mrb, pool[b].u.str, len);\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_STRCAT, B) {\n      mrb_assert(mrb_string_p(regs[a]));\n      mrb_str_concat(mrb, regs[a], regs[a+1]);\n      NEXT;\n    }\n\n    CASE(OP_HASH, BB) {\n      mrb_value hash = mrb_hash_new_capa(mrb, b);\n      int i;\n      int lim = a+b*2;\n\n      for (i=a; i<lim; i+=2) {\n        mrb_hash_set(mrb, hash, regs[i], regs[i+1]);\n      }\n      regs[a] = hash;\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_HASHADD, BB) {\n      mrb_value hash;\n      int i;\n      int lim = a+b*2+1;\n\n      hash = regs[a];\n      mrb_ensure_hash_type(mrb, hash);\n      for (i=a+1; i<lim; i+=2) {\n        mrb_hash_set(mrb, hash, regs[i], regs[i+1]);\n      }\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n    CASE(OP_HASHCAT, B) {\n      mrb_value hash = regs[a];\n\n      mrb_assert(mrb_hash_p(hash));\n      mrb_hash_merge(mrb, hash, regs[a+1]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_LAMBDA, BB)\n    c = OP_L_LAMBDA;\n    L_MAKE_LAMBDA:\n    {\n      struct RProc *p;\n      const mrb_irep *nirep = irep->reps[b];\n\n      if (c & OP_L_CAPTURE) {\n        p = mrb_closure_new(mrb, nirep);\n      }\n      else {\n        p = mrb_proc_new(mrb, nirep);\n        p->flags |= MRB_PROC_SCOPE;\n      }\n      if (c & OP_L_STRICT) p->flags |= MRB_PROC_STRICT;\n      regs[a] = mrb_obj_value(p);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n    CASE(OP_BLOCK, BB) {\n      c = OP_L_BLOCK;\n      goto L_MAKE_LAMBDA;\n    }\n    CASE(OP_METHOD, BB) {\n      c = OP_L_METHOD;\n      goto L_MAKE_LAMBDA;\n    }\n\n    CASE(OP_RANGE_INC, B) {\n      regs[a] = mrb_range_new(mrb, regs[a], regs[a+1], FALSE);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_RANGE_EXC, B) {\n      regs[a] = mrb_range_new(mrb, regs[a], regs[a+1], TRUE);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_OCLASS, B) {\n      regs[a] = mrb_obj_value(mrb->object_class);\n      NEXT;\n    }\n\n    CASE(OP_CLASS, BB) {\n      struct RClass *c = 0, *baseclass;\n      mrb_value base, super;\n      mrb_sym id = syms[b];\n\n      base = regs[a];\n      super = regs[a+1];\n      if (mrb_nil_p(base)) {\n        baseclass = MRB_PROC_TARGET_CLASS(mrb->c->ci->proc);\n        if (!baseclass) baseclass = mrb->object_class;\n        base = mrb_obj_value(baseclass);\n      }\n      c = mrb_vm_define_class(mrb, base, super, id);\n      regs[a] = mrb_obj_value(c);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_MODULE, BB) {\n      struct RClass *cls = 0, *baseclass;\n      mrb_value base;\n      mrb_sym id = syms[b];\n\n      base = regs[a];\n      if (mrb_nil_p(base)) {\n        baseclass = MRB_PROC_TARGET_CLASS(mrb->c->ci->proc);\n        if (!baseclass) baseclass = mrb->object_class;\n        base = mrb_obj_value(baseclass);\n      }\n      cls = mrb_vm_define_module(mrb, base, id);\n      regs[a] = mrb_obj_value(cls);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_EXEC, BB)\n    {\n      mrb_value recv = regs[a];\n      struct RProc *p;\n      const mrb_irep *nirep = irep->reps[b];\n\n      /* prepare closure */\n      p = mrb_proc_new(mrb, nirep);\n      p->c = NULL;\n      mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)proc);\n      MRB_PROC_SET_TARGET_CLASS(p, mrb_class_ptr(recv));\n      p->flags |= MRB_PROC_SCOPE;\n\n      /* prepare call stack */\n      cipush(mrb, a, 0, mrb_class_ptr(recv), p, 0, 0);\n\n      irep = p->body.irep;\n      pool = irep->pool;\n      syms = irep->syms;\n      mrb_stack_extend(mrb, irep->nregs);\n      stack_clear(regs+1, irep->nregs-1);\n      pc = irep->iseq;\n      JUMP;\n    }\n\n    CASE(OP_DEF, BB) {\n      struct RClass *target = mrb_class_ptr(regs[a]);\n      struct RProc *p = mrb_proc_ptr(regs[a+1]);\n      mrb_method_t m;\n      mrb_sym mid = syms[b];\n\n      MRB_METHOD_FROM_PROC(m, p);\n      mrb_define_method_raw(mrb, target, mid, m);\n      mrb_method_added(mrb, target, mid);\n      mrb_gc_arena_restore(mrb, ai);\n      regs[a] = mrb_symbol_value(mid);\n      NEXT;\n    }\n\n    CASE(OP_SCLASS, B) {\n      regs[a] = mrb_singleton_class(mrb, regs[a]);\n      mrb_gc_arena_restore(mrb, ai);\n      NEXT;\n    }\n\n    CASE(OP_TCLASS, B) {\n      struct RClass *target = check_target_class(mrb);\n      if (!target) goto L_RAISE;\n      regs[a] = mrb_obj_value(target);\n      NEXT;\n    }\n\n    CASE(OP_ALIAS, BB) {\n      struct RClass *target = check_target_class(mrb);\n\n      if (!target) goto L_RAISE;\n      mrb_alias_method(mrb, target, syms[a], syms[b]);\n      mrb_method_added(mrb, target, syms[a]);\n      NEXT;\n    }\n    CASE(OP_UNDEF, B) {\n      struct RClass *target = check_target_class(mrb);\n\n      if (!target) goto L_RAISE;\n      mrb_undef_method_id(mrb, target, syms[a]);\n      NEXT;\n    }\n\n    CASE(OP_DEBUG, Z) {\n      FETCH_BBB();\n#ifdef MRB_USE_DEBUG_HOOK\n      mrb->debug_op_hook(mrb, irep, pc, regs);\n#else\n#ifndef MRB_NO_STDIO\n      printf(\"OP_DEBUG %d %d %d\\n\", a, b, c);\n#else\n      abort();\n#endif\n#endif\n      NEXT;\n    }\n\n    CASE(OP_ERR, B) {\n      size_t len = pool[a].tt >> 2;\n      mrb_value exc;\n\n      mrb_assert((pool[a].tt&IREP_TT_NFLAG)==0);\n      exc = mrb_exc_new(mrb, E_LOCALJUMP_ERROR, pool[a].u.str, len);\n      mrb_exc_set(mrb, exc);\n      goto L_RAISE;\n    }\n\n    CASE(OP_EXT1, Z) {\n      insn = READ_B();\n      switch (insn) {\n#define OPCODE(insn,ops) case OP_ ## insn: FETCH_ ## ops ## _1(); mrb->c->ci->pc = pc; goto L_OP_ ## insn ## _BODY;\n#include \"mruby/ops.h\"\n#undef OPCODE\n      }\n      pc--;\n      NEXT;\n    }\n    CASE(OP_EXT2, Z) {\n      insn = READ_B();\n      switch (insn) {\n#define OPCODE(insn,ops) case OP_ ## insn: FETCH_ ## ops ## _2(); mrb->c->ci->pc = pc; goto L_OP_ ## insn ## _BODY;\n#include \"mruby/ops.h\"\n#undef OPCODE\n      }\n      pc--;\n      NEXT;\n    }\n    CASE(OP_EXT3, Z) {\n      uint8_t insn = READ_B();\n      switch (insn) {\n#define OPCODE(insn,ops) case OP_ ## insn: FETCH_ ## ops ## _3(); mrb->c->ci->pc = pc; goto L_OP_ ## insn ## _BODY;\n#include \"mruby/ops.h\"\n#undef OPCODE\n      }\n      pc--;\n      NEXT;\n    }\n\n    CASE(OP_STOP, Z) {\n      /*        stop VM */\n      CHECKPOINT_RESTORE(RBREAK_TAG_STOP) {\n        /* do nothing */\n      }\n      CHECKPOINT_MAIN(RBREAK_TAG_STOP) {\n        UNWIND_ENSURE(mrb, mrb->c->ci, pc, RBREAK_TAG_STOP, proc, mrb_nil_value());\n      }\n      CHECKPOINT_END(RBREAK_TAG_STOP);\n    L_STOP:\n      mrb->jmp = prev_jmp;\n      if (mrb->exc) {\n        mrb_assert(mrb->exc->tt == MRB_TT_EXCEPTION);\n        return mrb_obj_value(mrb->exc);\n      }\n      return regs[irep->nlocals];\n    }\n  }\n  END_DISPATCH;\n#undef regs\n  }\n  MRB_CATCH(&c_jmp) {\n    mrb_callinfo *ci = mrb->c->ci;\n    while (ci > mrb->c->cibase && ci->cci == CINFO_DIRECT) {\n      ci = cipop(mrb);\n    }\n    exc_catched = TRUE;\n    pc = ci->pc;\n    goto RETRY_TRY_BLOCK;\n  }\n  MRB_END_EXC(&c_jmp);\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145021,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "void svr_mailowner(\n\n  job   *pjob,       /* I */\n  int   mailpoint,  /* note, single character  */\n  int    force,      /* if set to MAIL_FORCE, force mail delivery */\n  char *text)      /* (optional) additional message text */\n\n  {\n  char *cmdbuf;\n  int    i;\n  char *mailfrom;\n  char  mailto[1024];\n  char *bodyfmt, *subjectfmt;\n  char bodyfmtbuf[1024];\n  FILE *outmail;\n\n  struct array_strings *pas;\n\n  if ((server.sv_attr[SRV_ATR_MailDomain].at_flags & ATR_VFLAG_SET) &&\n      (server.sv_attr[SRV_ATR_MailDomain].at_val.at_str != NULL) &&\n      (!strcasecmp(\"never\", server.sv_attr[SRV_ATR_MailDomain].at_val.at_str)))\n    {\n    /* never send user mail under any conditions */\n    if (LOGLEVEL >= 3) \n      {\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        \"Not sending email: Mail domain set to 'never'\\n\");\n      }\n\n    return;\n    }\n\n  if (LOGLEVEL >= 3)\n    {\n    char tmpBuf[LOG_BUF_SIZE];\n\n    snprintf(tmpBuf, LOG_BUF_SIZE, \"preparing to send '%c' mail for job %s to %s (%.64s)\\n\",\n             (char)mailpoint,\n             pjob->ji_qs.ji_jobid,\n             pjob->ji_wattr[JOB_ATR_job_owner].at_val.at_str,\n             (text != NULL) ? text : \"---\");\n\n    log_event(\n      PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      tmpBuf);\n    }\n\n  /*\n   * if force is true, force the mail out regardless of mailpoint\n   * unless server no_mail_force attribute is set to true\n   */\n\n  if ((force != MAIL_FORCE) ||\n    (server.sv_attr[(int)SRV_ATR_NoMailForce].at_val.at_long == TRUE))\n    {\n\n    if (pjob->ji_wattr[JOB_ATR_mailpnts].at_flags & ATR_VFLAG_SET)\n      {\n      if (*(pjob->ji_wattr[JOB_ATR_mailpnts].at_val.at_str) ==  MAIL_NONE)\n        {\n        /* do not send mail. No mail requested on job */\n        log_event(PBSEVENT_JOB,\n                  PBS_EVENTCLASS_JOB,\n                  pjob->ji_qs.ji_jobid,\n                  \"Not sending email: job requested no e-mail\");\n        return;\n        }\n      /* see if user specified mail of this type */\n      if (strchr(\n            pjob->ji_wattr[JOB_ATR_mailpnts].at_val.at_str,\n            mailpoint) == NULL)\n        {\n        /* do not send mail */\n        log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n          PBS_EVENTCLASS_JOB,\n          pjob->ji_qs.ji_jobid,\n          \"Not sending email: User does not want mail of this type.\\n\");\n\n        return;\n        }\n      }\n    else if (mailpoint != MAIL_ABORT) /* not set, default to abort */\n      {\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        \"Not sending email: Default mailpoint does not include this type.\\n\");\n\n      return;\n      }\n    }\n\n  /*\n   * ok, now we will fork a process to do the mailing to not\n   * hold up the server's other work.\n   */\n\n  if (fork())\n    {\n    return;  /* its all up to the child now */\n    }\n\n  /*\n   * From here on, we are a child process of the server.\n   * Fix up file descriptors and signal handlers.\n   */\n\n  rpp_terminate();\n\n  net_close(-1);\n\n  /* Who is mail from, if SRV_ATR_mailfrom not set use default */\n\n  if ((mailfrom = server.sv_attr[SRV_ATR_mailfrom].at_val.at_str) == NULL)\n    {\n    if (LOGLEVEL >= 5)\n      {\n      char tmpBuf[LOG_BUF_SIZE];\n\n      snprintf(tmpBuf,sizeof(tmpBuf),\n        \"Updated mailto from user list: '%s'\\n\",\n        mailto);\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        tmpBuf);\n      }\n    mailfrom = PBS_DEFAULT_MAIL;\n    }\n\n  /* Who does the mail go to?  If mail-list, them; else owner */\n\n  *mailto = '\\0';\n\n  if (pjob->ji_wattr[JOB_ATR_mailuser].at_flags & ATR_VFLAG_SET)\n    {\n    /* has mail user list, send to them rather than owner */\n\n    pas = pjob->ji_wattr[JOB_ATR_mailuser].at_val.at_arst;\n\n    if (pas != NULL)\n      {\n      for (i = 0;i < pas->as_usedptr;i++)\n        {\n        if ((strlen(mailto) + strlen(pas->as_string[i]) + 2) < sizeof(mailto))\n          {\n          strcat(mailto, pas->as_string[i]);\n          strcat(mailto, \" \");\n          }\n        }\n      }\n    }\n  else\n    {\n    /* no mail user list, just send to owner */\n\n    if ((server.sv_attr[SRV_ATR_MailDomain].at_flags & ATR_VFLAG_SET) &&\n        (server.sv_attr[SRV_ATR_MailDomain].at_val.at_str != NULL))\n      {\n      strcpy(mailto, pjob->ji_wattr[JOB_ATR_euser].at_val.at_str);\n      strcat(mailto, \"@\");\n      strcat(mailto, server.sv_attr[SRV_ATR_MailDomain].at_val.at_str);\n\n      if (LOGLEVEL >= 5) \n        {\n        char tmpBuf[LOG_BUF_SIZE];\n\n        snprintf(tmpBuf,sizeof(tmpBuf),\n          \"Updated mailto from job owner and mail domain: '%s'\\n\",\n          mailto);\n        log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n          PBS_EVENTCLASS_JOB,\n          pjob->ji_qs.ji_jobid,\n          tmpBuf);\n        }\n      }\n    else\n      {\n#ifdef TMAILDOMAIN\n      strcpy(mailto, pjob->ji_wattr[JOB_ATR_euser].at_val.at_str);\n      strcat(mailto, \"@\");\n      strcat(mailto, TMAILDOMAIN);\n#else /* TMAILDOMAIN */\n      strcpy(mailto, pjob->ji_wattr[JOB_ATR_job_owner].at_val.at_str);\n#endif /* TMAILDOMAIN */\n\n      if (LOGLEVEL >= 5)\n        {\n        char tmpBuf[LOG_BUF_SIZE];\n\n        snprintf(tmpBuf,sizeof(tmpBuf),\n          \"Updated mailto from job owner: '%s'\\n\",\n          mailto);\n        log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n          PBS_EVENTCLASS_JOB,\n          pjob->ji_qs.ji_jobid,\n          tmpBuf);\n        }\n      }\n    }\n\n  /* mail subject line formating statement */\n\n  if ((server.sv_attr[SRV_ATR_MailSubjectFmt].at_flags & ATR_VFLAG_SET) &&\n      (server.sv_attr[SRV_ATR_MailSubjectFmt].at_val.at_str != NULL))\n    {\n    subjectfmt = server.sv_attr[SRV_ATR_MailSubjectFmt].at_val.at_str;\n    }\n  else\n    {\n    subjectfmt = \"PBS JOB %i\";\n    }\n\n  /* mail body formating statement */\n\n  if ((server.sv_attr[SRV_ATR_MailBodyFmt].at_flags & ATR_VFLAG_SET) &&\n      (server.sv_attr[SRV_ATR_MailBodyFmt].at_val.at_str != NULL))\n    {\n    bodyfmt = server.sv_attr[SRV_ATR_MailBodyFmt].at_val.at_str;\n    }\n  else\n    {\n    bodyfmt =  strcpy(bodyfmtbuf, \"PBS Job Id: %i\\n\"\n                                  \"Job Name:   %j\\n\");\n    if (pjob->ji_wattr[JOB_ATR_exec_host].at_flags & ATR_VFLAG_SET)\n      {\n      strcat(bodyfmt, \"Exec host:  %h\\n\");\n      }\n\n    strcat(bodyfmt, \"%m\\n\");\n\n    if (text != NULL)\n      {\n      strcat(bodyfmt, \"%d\\n\");\n      }\n    }\n  /* setup sendmail command line with -f from_whom */\n\n  i = strlen(SENDMAIL_CMD) + strlen(mailfrom) + strlen(mailto) + 6;\n\n  if ((cmdbuf = malloc(i)) == NULL)\n    {\n    char tmpBuf[LOG_BUF_SIZE];\n\n    snprintf(tmpBuf,sizeof(tmpBuf),\n      \"Unable to popen() command '%s' for writing: '%s' (error %d)\\n\",\n      cmdbuf,\n      strerror(errno),\n      errno);\n    log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      tmpBuf);\n\n    exit(1);\n    }\n\n  sprintf(cmdbuf, \"%s -f %s %s\",\n\n          SENDMAIL_CMD,\n          mailfrom,\n          mailto);\n\n  outmail = (FILE *)popen(cmdbuf, \"w\");\n\n  if (outmail == NULL)\n    {\n    char tmpBuf[LOG_BUF_SIZE];\n\n    snprintf(tmpBuf,sizeof(tmpBuf),\n      \"Unable to popen() command '%s' for writing: '%s' (error %d)\\n\",\n      cmdbuf,\n      strerror(errno),\n      errno);\n    log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      tmpBuf);\n\n    exit(1);\n    }\n\n  /* Pipe in mail headers: To: and Subject: */\n\n  fprintf(outmail, \"To: %s\\n\",\n          mailto);\n\n  fprintf(outmail, \"Subject: \");\n  svr_format_job(outmail, pjob, subjectfmt, mailpoint, text);\n  fprintf(outmail, \"\\n\");\n\n  /* Set \"Precedence: bulk\" to avoid vacation messages, etc */\n\n  fprintf(outmail, \"Precedence: bulk\\n\\n\");\n\n  /* Now pipe in the email body */\n  svr_format_job(outmail, pjob, bodyfmt, mailpoint, text);\n\n  errno = 0;\n  if ((i = pclose(outmail)) != 0)\n    {\n    char tmpBuf[LOG_BUF_SIZE];\n\n    snprintf(tmpBuf,sizeof(tmpBuf),\n      \"Email '%c' to %s failed: Child process '%s' %s %d (errno %d:%s)\\n\",\n      mailpoint,\n      mailto,\n      cmdbuf,\n      ((WIFEXITED(i)) ? (\"returned\") : ((WIFSIGNALED(i)) ? (\"killed by signal\") : (\"croaked\"))),\n      ((WIFEXITED(i)) ? (WEXITSTATUS(i)) : ((WIFSIGNALED(i)) ? (WTERMSIG(i)) : (i))),\n      errno,\n      strerror(errno));\n    log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      tmpBuf);\n    }\n  else if (LOGLEVEL >= 4)\n    {\n    log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      \"Email sent successfully\\n\");\n    }\n\n  exit(0);\n\n  /*NOTREACHED*/\n\n  return;\n  }  /* END svr_mailowner() */",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145196,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "void svr_mailowner(\n\n  job   *pjob,       /* I */\n  int    mailpoint,  /* note, single character  */\n  int    force,      /* if set to MAIL_FORCE, force mail delivery */\n  char *text)      /* (optional) additional message text */\n\n  {\n  int         status = 0;\n  int         numargs = 0;\n  int         pipes[2];\n  int         counter;\n  pid_t       pid;\n  char       *mailptr;\n  char       *mailfrom = NULL;\n  char        tmpBuf[LOG_BUF_SIZE];\n  // We call sendmail with cmd_name + 2 arguments + # of mailto addresses + 1 for null\n  char       *sendmail_args[100];\n  char        mailto[1024];\n  FILE       *stream;\n\n  struct array_strings *pas;\n\n  if ((server.sv_attr[SRV_ATR_MailDomain].at_flags & ATR_VFLAG_SET) &&\n      (server.sv_attr[SRV_ATR_MailDomain].at_val.at_str != NULL) &&\n      (!strcasecmp(\"never\", server.sv_attr[SRV_ATR_MailDomain].at_val.at_str)))\n    {\n    /* never send user mail under any conditions */\n    if (LOGLEVEL >= 3) \n      {\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        \"Not sending email: Mail domain set to 'never'\\n\");\n      }\n\n    return;\n    }\n\n  if (LOGLEVEL >= 3)\n    {\n    char tmpBuf[LOG_BUF_SIZE];\n\n    snprintf(tmpBuf, LOG_BUF_SIZE, \"preparing to send '%c' mail for job %s to %s (%.64s)\\n\",\n             (char)mailpoint,\n             pjob->ji_qs.ji_jobid,\n             pjob->ji_wattr[JOB_ATR_job_owner].at_val.at_str,\n             (text != NULL) ? text : \"---\");\n\n    log_event(\n      PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      tmpBuf);\n    }\n\n  /*\n   * if force is true, force the mail out regardless of mailpoint\n   * unless server no_mail_force attribute is set to true\n   */\n\n  if ((force != MAIL_FORCE) ||\n    (server.sv_attr[(int)SRV_ATR_NoMailForce].at_val.at_long == TRUE))\n    {\n\n    if (pjob->ji_wattr[JOB_ATR_mailpnts].at_flags & ATR_VFLAG_SET)\n      {\n      if (*(pjob->ji_wattr[JOB_ATR_mailpnts].at_val.at_str) ==  MAIL_NONE)\n        {\n        /* do not send mail. No mail requested on job */\n        log_event(PBSEVENT_JOB,\n                  PBS_EVENTCLASS_JOB,\n                  pjob->ji_qs.ji_jobid,\n                  \"Not sending email: job requested no e-mail\");\n        return;\n        }\n      /* see if user specified mail of this type */\n      if (strchr(\n            pjob->ji_wattr[JOB_ATR_mailpnts].at_val.at_str,\n            mailpoint) == NULL)\n        {\n        /* do not send mail */\n        log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n          PBS_EVENTCLASS_JOB,\n          pjob->ji_qs.ji_jobid,\n          \"Not sending email: User does not want mail of this type.\\n\");\n\n        return;\n        }\n      }\n    else if (mailpoint != MAIL_ABORT) /* not set, default to abort */\n      {\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        \"Not sending email: Default mailpoint does not include this type.\\n\");\n\n      return;\n      }\n    }\n\n  /*\n   * ok, now we will fork a process to do the mailing to not\n   * hold up the server's other work.\n   */\n\n  if (fork())\n    {\n    return;  /* its all up to the child now */\n    }\n\n  /* Close the rest of the open file descriptors */\n  int numfds = sysconf(_SC_OPEN_MAX);\n  while (--numfds > 0)\n    close(numfds);\n  \n  /* Who is mail from, if SRV_ATR_mailfrom not set use default */\n  if ((mailfrom = server.sv_attr[SRV_ATR_mailfrom].at_val.at_str) == NULL)\n    {\n    if (LOGLEVEL >= 5)\n      {\n      char tmpBuf[LOG_BUF_SIZE];\n\n      snprintf(tmpBuf,sizeof(tmpBuf),\n        \"Updated mailto from user list: '%s'\\n\",\n        mailto);\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        tmpBuf);\n      }\n    mailfrom = PBS_DEFAULT_MAIL;\n    }\n  \n  /* Who does the mail go to?  If mail-list, them; else owner */\n  *mailto = '\\0';\n\n  if (pjob->ji_wattr[JOB_ATR_mailuser].at_flags & ATR_VFLAG_SET)\n    {\n    /* has mail user list, send to them rather than owner */\n    pas = pjob->ji_wattr[JOB_ATR_mailuser].at_val.at_arst;\n\n    if (pas != NULL)\n      {\n      int i;\n      for (i = 0;i < pas->as_usedptr;i++)\n        {\n        if ((strlen(mailto) + strlen(pas->as_string[i]) + 2) < sizeof(mailto))\n          {\n          strcat(mailto, pas->as_string[i]);\n          strcat(mailto, \" \");\n          }\n        }\n      }\n    }\n  else\n    {\n    /* no mail user list, just send to owner */\n    if ((server.sv_attr[SRV_ATR_MailDomain].at_flags & ATR_VFLAG_SET) &&\n        (server.sv_attr[SRV_ATR_MailDomain].at_val.at_str != NULL))\n      {\n      strcpy(mailto, pjob->ji_wattr[JOB_ATR_euser].at_val.at_str);\n      strcat(mailto, \"@\");\n      strcat(mailto, server.sv_attr[SRV_ATR_MailDomain].at_val.at_str);\n\n      if (LOGLEVEL >= 5) \n        {\n        char tmpBuf[LOG_BUF_SIZE];\n\n        snprintf(tmpBuf,sizeof(tmpBuf),\n          \"Updated mailto from job owner and mail domain: '%s'\\n\",\n          mailto);\n        log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n          PBS_EVENTCLASS_JOB,\n          pjob->ji_qs.ji_jobid,\n          tmpBuf);\n        }\n      }\n    else\n      {\n#ifdef TMAILDOMAIN\n      strcpy(mailto, pjob->ji_wattr[JOB_ATR_euser].at_val.at_str);\n      strcat(mailto, \"@\");\n      strcat(mailto, TMAILDOMAIN);\n#else /* TMAILDOMAIN */\n      strcpy(mailto, pjob->ji_wattr[JOB_ATR_job_owner].at_val.at_str);\n#endif /* TMAILDOMAIN */\n\n      if (LOGLEVEL >= 5)\n        {\n        char tmpBuf[LOG_BUF_SIZE];\n\n        snprintf(tmpBuf,sizeof(tmpBuf),\n          \"Updated mailto from job owner: '%s'\\n\",\n          mailto);\n        log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n          PBS_EVENTCLASS_JOB,\n          pjob->ji_qs.ji_jobid,\n          tmpBuf);\n        }\n      }\n    }\n\n  sendmail_args[numargs++] = (char *)SENDMAIL_CMD;\n  sendmail_args[numargs++] = (char *)\"-f\";\n  sendmail_args[numargs++] = (char *)mailfrom;\n\n  /* Add the e-mail addresses to the command line */\n  mailptr = strdup(mailto);\n  sendmail_args[numargs++] = mailptr;\n  for (counter=0; counter < (int)strlen(mailptr); counter++)\n    {\n    if (mailptr[counter] == ',')\n      {\n      mailptr[counter] = '\\0';\n      sendmail_args[numargs++] = mailptr + counter + 1;\n      if (numargs >= 99)\n        break;\n      }\n    }\n\n  sendmail_args[numargs] = NULL;\n  \n  /* Create a pipe to talk to the sendmail process we are about to fork */\n  if (pipe(pipes) == -1)\n    {\n    snprintf(tmpBuf, sizeof(tmpBuf), \"Unable to pipes for sending e-mail\\n\");\n    log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      tmpBuf);\n\n    free(mailptr);\n    exit(-1);\n    }\n\n  if ((pid=fork()) == -1)\n    {\n    snprintf(tmpBuf, sizeof(tmpBuf), \"Unable to fork for sending e-mail\\n\");\n    log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n      PBS_EVENTCLASS_JOB,\n      pjob->ji_qs.ji_jobid,\n      tmpBuf);\n\n    free(mailptr);\n    close(pipes[0]);\n    close(pipes[1]);\n    exit(-1);\n    }\n  else if (pid == 0)\n    {\n    /* CHILD */\n\n    /* Make stdin the read end of the pipe */\n    dup2(pipes[0], 0);\n\n    /* Close the rest of the open file descriptors */\n    int numfds = sysconf(_SC_OPEN_MAX);\n    while (--numfds > 0)\n      close(numfds);\n\n    execv(SENDMAIL_CMD, sendmail_args);\n    /* This never returns, but if the execv fails the child should exit */\n    exit(1);\n    }\n  else\n    {\n    /* This is the parent */\n\n    /* Close the read end of the pipe */\n    close(pipes[0]);\n\n    /* Write the body to the pipe */\n    stream = fdopen(pipes[1], \"w\");\n    write_email(stream, pjob, mailto, mailpoint, text);\n\n    fflush(stream);\n\n    /* Close and wait for the command to finish */\n    if (fclose(stream) != 0)\n      {\n      snprintf(tmpBuf,sizeof(tmpBuf),\n        \"Piping mail body to sendmail closed: errno %d:%s\\n\",\n        errno, strerror(errno));\n\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        tmpBuf);\n      }\n\n    // we aren't going to block in order to find out whether or not sendmail worked \n    if ((waitpid(pid, &status, WNOHANG) != 0) &&\n        (status != 0))\n      {\n      snprintf(tmpBuf,sizeof(tmpBuf),\n        \"Sendmail command returned %d. Mail may not have been sent\\n\",\n        status);\n\n      log_event(PBSEVENT_ERROR | PBSEVENT_ADMIN | PBSEVENT_JOB,\n        PBS_EVENTCLASS_JOB,\n        pjob->ji_qs.ji_jobid,\n        tmpBuf);\n      }\n\n    // don't leave zombies\n    while (waitpid(-1, &status, WNOHANG) != 0)\n      {\n      // zombie reaped, NO-OP\n      }\n      \n    free(mailptr);\n    exit(0);\n    }\n    \n  /* NOT REACHED */\n\n  exit(0);\n  }  /* END svr_mailowner() */",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145197,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "plugin_init (Ekiga::KickStart& kickstart)\n{\n#ifdef DEBUG\n  // should make it easier to test ekiga without installing\n  gchar* path = g_build_path (G_DIR_SEPARATOR_S,\n\t\t\t      g_get_tmp_dir (), \"ekiga_debug_plugins\", NULL);\n  plugin_parse_directory (kickstart, path);\n  g_free (path);\n#else\n  plugin_parse_directory (kickstart,\n\t\t\t  EKIGA_PLUGIN_DIR);\n#endif\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145306,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "plugin_init (Ekiga::KickStart& kickstart)\n{\n#if DEBUG\n  // should make it easier to test ekiga without installing\n  gchar* path = g_build_path (G_DIR_SEPARATOR_S,\n\t\t\t      g_get_tmp_dir (), \"ekiga_debug_plugins\", NULL);\n  plugin_parse_directory (kickstart, path);\n  g_free (path);\n#else\n  plugin_parse_directory (kickstart,\n\t\t\t  EKIGA_PLUGIN_DIR);\n#endif\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145307,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "MultiPartInputFile::Data::chunkOffsetReconstruction(OPENEXR_IMF_INTERNAL_NAMESPACE::IStream& is, const vector<InputPartData*>& parts)\n{\n    //\n    // Reconstruct broken chunk offset tables. Stop once we received any exception.\n    //\n\n    Int64 position = is.tellg();\n\n    \n    //\n    // check we understand all the parts available: if not, we cannot continue\n    // exceptions thrown here should trickle back up to the constructor\n    //\n    \n    for (size_t i = 0; i < parts.size(); i++)\n    {\n        Header& header=parts[i]->header;\n        \n        //\n        // do we have a valid type entry?\n        // we only need them for true multipart files or single part non-image (deep) files\n        //\n        if(!header.hasType() && (isMultiPart(version) || isNonImage(version)))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with missing type\");\n        }\n        if(!isSupportedType(header.type()))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with unknown type \"+header.type());\n        }\n    }\n    \n    \n    // how many chunks should we read? We should stop when we reach the end\n    size_t total_chunks = 0;\n        \n    // for tiled-based parts, array of (pointers to) tileOffsets objects\n    // to create mapping between tile coordinates and chunk table indices\n    \n    \n    vector<TileOffsets*> tileOffsets(parts.size());\n    \n    // for scanline-based parts, number of scanlines in each chunk\n    vector<int> rowsizes(parts.size());\n        \n    for(size_t i = 0 ; i < parts.size() ; i++)\n    {\n        total_chunks += parts[i]->chunkOffsets.size();\n        if (isTiled(parts[i]->header.type()))\n        {\n            tileOffsets[i] = createTileOffsets(parts[i]->header);\n        }else{\n            tileOffsets[i] = NULL;\n            // (TODO) fix this so that it doesn't need to be revised for future compression types.\n            switch(parts[i]->header.compression())\n            {\n                case DWAB_COMPRESSION :\n                    rowsizes[i] = 256;\n                    break;\n                case PIZ_COMPRESSION :\n                case B44_COMPRESSION :\n                case B44A_COMPRESSION :\n                case DWAA_COMPRESSION :\n                    rowsizes[i]=32;\n                    break;\n                case ZIP_COMPRESSION :\n                case PXR24_COMPRESSION :\n                    rowsizes[i]=16;\n                    break;\n                case ZIPS_COMPRESSION :\n                case RLE_COMPRESSION :\n                case NO_COMPRESSION :\n                    rowsizes[i]=1;\n                    break;\n                default :\n                    throw(IEX_NAMESPACE::ArgExc(\"Unknown compression method in chunk offset reconstruction\"));\n            }\n        }\n     }\n        \n     try\n     {\n            \n        //\n        // \n        //\n        \n        Int64 chunk_start = position;\n        for (size_t i = 0; i < total_chunks ; i++)\n        {\n            //\n            // do we have a part number?\n            //\n            \n            int partNumber = 0;\n            if(isMultiPart(version))\n            {\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, partNumber);\n            }\n            \n            \n            \n            if(partNumber<0 || partNumber> static_cast<int>(parts.size()))\n            {\n                throw IEX_NAMESPACE::IoExc(\"part number out of range\");\n            }\n            \n            Header& header = parts[partNumber]->header;\n\n            // size of chunk NOT including multipart field\n            \n            Int64 size_of_chunk=0;\n\n            if (isTiled(header.type()))\n            {\n                //\n                // \n                //\n                int tilex,tiley,levelx,levely;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tilex);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tiley);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levelx);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levely);\n                \n                //std::cout << \"chunk_start for \" << tilex <<',' << tiley << ',' << levelx << ' ' << levely << ':' << chunk_start << std::endl;\n                    \n                \n                if(!tileOffsets[partNumber])\n                {\n                    // this shouldn't actually happen - we should have allocated a valid\n                    // tileOffsets for any part which isTiled\n                    throw IEX_NAMESPACE::IoExc(\"part not tiled\");\n                    \n                }\n                \n                if(!tileOffsets[partNumber]->isValidTile(tilex,tiley,levelx,levely))\n                {\n                    throw IEX_NAMESPACE::IoExc(\"invalid tile coordinates\");\n                }\n                \n                (*tileOffsets[partNumber])(tilex,tiley,levelx,levely)=chunk_start;\n                \n                // compute chunk sizes - different procedure for deep tiles and regular\n                // ones\n                if(header.type()==DEEPTILE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    //add 40 byte header to packed sizes (tile coordinates, packed sizes, unpacked size)\n                    size_of_chunk=packed_offset+packed_sample+40;\n                }\n                else\n                {\n                    \n                    // regular image has 20 bytes of header, 4 byte chunksize;\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);\n                    size_of_chunk=chunksize+20;\n                }\n            }\n            else\n            {\n                int y_coordinate;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, y_coordinate);\n                \n                \n                if(y_coordinate < header.dataWindow().min.y || y_coordinate > header.dataWindow().max.y)\n                {\n                   throw IEX_NAMESPACE::IoExc(\"y out of range\");\n                }\n                y_coordinate -= header.dataWindow().min.y;\n                y_coordinate /= rowsizes[partNumber];   \n                \n                if(y_coordinate < 0 || y_coordinate >= int(parts[partNumber]->chunkOffsets.size()))\n                {\n                   throw IEX_NAMESPACE::IoExc(\"chunk index out of range\");\n                }\n                \n                parts[partNumber]->chunkOffsets[y_coordinate]=chunk_start;\n                \n                if(header.type()==DEEPSCANLINE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    \n                    size_of_chunk=packed_offset+packed_sample+28;\n                }\n                else\n                {\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);   \n                    size_of_chunk=chunksize+8;\n                }\n                \n            }\n            \n            if(isMultiPart(version))\n            {\n                chunk_start+=4;\n            }\n            \n            chunk_start+=size_of_chunk;\n            \n            is.seekg(chunk_start);\n            \n        }\n        \n    }\n    catch (...)\n    {\n        //\n        // Suppress all exceptions.  This functions is\n        // called only to reconstruct the line offset\n        // table for incomplete files, and exceptions\n        // are likely.\n        //\n    }\n\n    // copy tiled part data back to chunk offsets\n    \n    for(size_t partNumber=0;partNumber<parts.size();partNumber++)\n    {\n        if(tileOffsets[partNumber])\n        {\n            size_t pos=0;\n            vector<vector<vector <Int64> > > offsets = tileOffsets[partNumber]->getOffsets();\n            for (size_t l = 0; l < offsets.size(); l++)\n                for (size_t y = 0; y < offsets[l].size(); y++)\n                    for (size_t x = 0; x < offsets[l][y].size(); x++)\n                    {\n                        parts[ partNumber ]->chunkOffsets[pos] = offsets[l][y][x];\n                        pos++;\n                    }\n           delete tileOffsets[partNumber];\n        }\n    }\n\n    is.clear();\n    is.seekg (position);\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145380,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "MultiPartInputFile::Data::chunkOffsetReconstruction(OPENEXR_IMF_INTERNAL_NAMESPACE::IStream& is, const vector<InputPartData*>& parts)\n{\n    //\n    // Reconstruct broken chunk offset tables. Stop once we received any exception.\n    //\n\n    Int64 position = is.tellg();\n\n    \n    //\n    // check we understand all the parts available: if not, we cannot continue\n    // exceptions thrown here should trickle back up to the constructor\n    //\n    \n    for (size_t i = 0; i < parts.size(); i++)\n    {\n        Header& header=parts[i]->header;\n        \n        //\n        // do we have a valid type entry?\n        // we only need them for true multipart files or single part non-image (deep) files\n        //\n        if(!header.hasType() && (isMultiPart(version) || isNonImage(version)))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with missing type\");\n        }\n        if(!isSupportedType(header.type()))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with unknown type \"+header.type());\n        }\n    }\n    \n    \n    // how many chunks should we read? We should stop when we reach the end\n    size_t total_chunks = 0;\n        \n    // for tiled-based parts, array of (pointers to) tileOffsets objects\n    // to create mapping between tile coordinates and chunk table indices\n    \n    \n    vector<TileOffsets*> tileOffsets(parts.size());\n    \n    // for scanline-based parts, number of scanlines in each chunk\n    vector<int> rowsizes(parts.size());\n        \n    for(size_t i = 0 ; i < parts.size() ; i++)\n    {\n        total_chunks += parts[i]->chunkOffsets.size();\n        if (isTiled(parts[i]->header.type()))\n        {\n            tileOffsets[i] = createTileOffsets(parts[i]->header);\n        }else{\n            tileOffsets[i] = NULL;\n            // (TODO) fix this so that it doesn't need to be revised for future compression types.\n            switch(parts[i]->header.compression())\n            {\n                case DWAB_COMPRESSION :\n                    rowsizes[i] = 256;\n                    break;\n                case PIZ_COMPRESSION :\n                case B44_COMPRESSION :\n                case B44A_COMPRESSION :\n                case DWAA_COMPRESSION :\n                    rowsizes[i]=32;\n                    break;\n                case ZIP_COMPRESSION :\n                case PXR24_COMPRESSION :\n                    rowsizes[i]=16;\n                    break;\n                case ZIPS_COMPRESSION :\n                case RLE_COMPRESSION :\n                case NO_COMPRESSION :\n                    rowsizes[i]=1;\n                    break;\n                default :\n                    throw(IEX_NAMESPACE::ArgExc(\"Unknown compression method in chunk offset reconstruction\"));\n            }\n        }\n     }\n        \n     try\n     {\n            \n        //\n        // \n        //\n        \n        Int64 chunk_start = position;\n        for (size_t i = 0; i < total_chunks ; i++)\n        {\n            //\n            // do we have a part number?\n            //\n            \n            int partNumber = 0;\n            if(isMultiPart(version))\n            {\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, partNumber);\n            }\n            \n            \n            \n            if(partNumber<0 || partNumber>= static_cast<int>(parts.size()))\n            {\n                throw IEX_NAMESPACE::IoExc(\"part number out of range\");\n            }\n            \n            Header& header = parts[partNumber]->header;\n\n            // size of chunk NOT including multipart field\n            \n            Int64 size_of_chunk=0;\n\n            if (isTiled(header.type()))\n            {\n                //\n                // \n                //\n                int tilex,tiley,levelx,levely;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tilex);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tiley);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levelx);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levely);\n                \n                //std::cout << \"chunk_start for \" << tilex <<',' << tiley << ',' << levelx << ' ' << levely << ':' << chunk_start << std::endl;\n                    \n                \n                if(!tileOffsets[partNumber])\n                {\n                    // this shouldn't actually happen - we should have allocated a valid\n                    // tileOffsets for any part which isTiled\n                    throw IEX_NAMESPACE::IoExc(\"part not tiled\");\n                    \n                }\n                \n                if(!tileOffsets[partNumber]->isValidTile(tilex,tiley,levelx,levely))\n                {\n                    throw IEX_NAMESPACE::IoExc(\"invalid tile coordinates\");\n                }\n                \n                (*tileOffsets[partNumber])(tilex,tiley,levelx,levely)=chunk_start;\n                \n                // compute chunk sizes - different procedure for deep tiles and regular\n                // ones\n                if(header.type()==DEEPTILE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    //add 40 byte header to packed sizes (tile coordinates, packed sizes, unpacked size)\n                    size_of_chunk=packed_offset+packed_sample+40;\n                }\n                else\n                {\n                    \n                    // regular image has 20 bytes of header, 4 byte chunksize;\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);\n                    size_of_chunk=chunksize+20;\n                }\n            }\n            else\n            {\n                int y_coordinate;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, y_coordinate);\n                \n                \n                if(y_coordinate < header.dataWindow().min.y || y_coordinate > header.dataWindow().max.y)\n                {\n                   throw IEX_NAMESPACE::IoExc(\"y out of range\");\n                }\n                y_coordinate -= header.dataWindow().min.y;\n                y_coordinate /= rowsizes[partNumber];   \n                \n                if(y_coordinate < 0 || y_coordinate >= int(parts[partNumber]->chunkOffsets.size()))\n                {\n                   throw IEX_NAMESPACE::IoExc(\"chunk index out of range\");\n                }\n                \n                parts[partNumber]->chunkOffsets[y_coordinate]=chunk_start;\n                \n                if(header.type()==DEEPSCANLINE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    \n                    size_of_chunk=packed_offset+packed_sample+28;\n                }\n                else\n                {\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);   \n                    size_of_chunk=chunksize+8;\n                }\n                \n            }\n            \n            if(isMultiPart(version))\n            {\n                chunk_start+=4;\n            }\n            \n            chunk_start+=size_of_chunk;\n            \n            is.seekg(chunk_start);\n            \n        }\n        \n    }\n    catch (...)\n    {\n        //\n        // Suppress all exceptions.  This functions is\n        // called only to reconstruct the line offset\n        // table for incomplete files, and exceptions\n        // are likely.\n        //\n    }\n\n    // copy tiled part data back to chunk offsets\n    \n    for(size_t partNumber=0;partNumber<parts.size();partNumber++)\n    {\n        if(tileOffsets[partNumber])\n        {\n            size_t pos=0;\n            vector<vector<vector <Int64> > > offsets = tileOffsets[partNumber]->getOffsets();\n            for (size_t l = 0; l < offsets.size(); l++)\n                for (size_t y = 0; y < offsets[l].size(); y++)\n                    for (size_t x = 0; x < offsets[l][y].size(); x++)\n                    {\n                        parts[ partNumber ]->chunkOffsets[pos] = offsets[l][y][x];\n                        pos++;\n                    }\n           delete tileOffsets[partNumber];\n        }\n    }\n\n    is.clear();\n    is.seekg (position);\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145381,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static void cli_session_setup_gensec_remote_done(struct tevent_req *subreq)\n{\n\tstruct tevent_req *req =\n\t\ttevent_req_callback_data(subreq,\n\t\tstruct tevent_req);\n\tstruct cli_session_setup_gensec_state *state =\n\t\ttevent_req_data(req,\n\t\tstruct cli_session_setup_gensec_state);\n\tNTSTATUS status;\n\n\tTALLOC_FREE(state->inbuf);\n\tTALLOC_FREE(state->recv_iov);\n\n\tstatus = cli_sesssetup_blob_recv(subreq, state, &state->blob_in,\n\t\t\t\t\t &state->inbuf, &state->recv_iov);\n\tTALLOC_FREE(subreq);\n\tdata_blob_free(&state->blob_out);\n\tif (!NT_STATUS_IS_OK(status) &&\n\t    !NT_STATUS_EQUAL(status, NT_STATUS_MORE_PROCESSING_REQUIRED))\n\t{\n\t\ttevent_req_nterror(req, status);\n\t\treturn;\n\t}\n\n\tif (NT_STATUS_IS_OK(status)) {\n\t\tstruct smbXcli_session *session = NULL;\n\t\tbool is_guest = false;\n\n\t\tif (smbXcli_conn_protocol(state->cli->conn) >= PROTOCOL_SMB2_02) {\n\t\t\tsession = state->cli->smb2.session;\n\t\t} else {\n\t\t\tsession = state->cli->smb1.session;\n\t\t}\n\n\t\tis_guest = smbXcli_session_is_guest(session);\n\t\tif (is_guest) {\n\t\t\t/*\n\t\t\t * We can't finish the gensec handshake, we don't\n\t\t\t * have a negotiated session key.\n\t\t\t *\n\t\t\t * So just pretend we are completely done.\n\t\t\t */\n\t\t\tstate->blob_in = data_blob_null;\n\t\t\tstate->local_ready = true;\n\t\t}\n\n\t\tstate->remote_ready = true;\n\t}\n\n\tif (state->local_ready && state->remote_ready) {\n\t\tcli_session_setup_gensec_ready(req);\n\t\treturn;\n\t}\n\n\tcli_session_setup_gensec_local_next(req);\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145982,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static void cli_session_setup_gensec_remote_done(struct tevent_req *subreq)\n{\n\tstruct tevent_req *req =\n\t\ttevent_req_callback_data(subreq,\n\t\tstruct tevent_req);\n\tstruct cli_session_setup_gensec_state *state =\n\t\ttevent_req_data(req,\n\t\tstruct cli_session_setup_gensec_state);\n\tNTSTATUS status;\n\n\tTALLOC_FREE(state->inbuf);\n\tTALLOC_FREE(state->recv_iov);\n\n\tstatus = cli_sesssetup_blob_recv(subreq, state, &state->blob_in,\n\t\t\t\t\t &state->inbuf, &state->recv_iov);\n\tTALLOC_FREE(subreq);\n\tdata_blob_free(&state->blob_out);\n\tif (!NT_STATUS_IS_OK(status) &&\n\t    !NT_STATUS_EQUAL(status, NT_STATUS_MORE_PROCESSING_REQUIRED))\n\t{\n\t\ttevent_req_nterror(req, status);\n\t\treturn;\n\t}\n\n\tif (NT_STATUS_IS_OK(status)) {\n\t\tstruct smbXcli_session *session = NULL;\n\t\tbool is_guest = false;\n\n\t\tif (smbXcli_conn_protocol(state->cli->conn) >= PROTOCOL_SMB2_02) {\n\t\t\tsession = state->cli->smb2.session;\n\t\t} else {\n\t\t\tsession = state->cli->smb1.session;\n\t\t}\n\n\t\tis_guest = smbXcli_session_is_guest(session);\n\t\tif (is_guest) {\n\t\t\t/*\n\t\t\t * We can't finish the gensec handshake, we don't\n\t\t\t * have a negotiated session key.\n\t\t\t *\n\t\t\t * So just pretend we are completely done.\n\t\t\t *\n\t\t\t * Note that smbXcli_session_is_guest()\n\t\t\t * always returns false if we require signing.\n\t\t\t */\n\t\t\tstate->blob_in = data_blob_null;\n\t\t\tstate->local_ready = true;\n\t\t}\n\n\t\tstate->remote_ready = true;\n\t}\n\n\tif (state->local_ready && state->remote_ready) {\n\t\tcli_session_setup_gensec_ready(req);\n\t\treturn;\n\t}\n\n\tcli_session_setup_gensec_local_next(req);\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 145983,
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "int virtio_load(VirtIODevice *vdev, QEMUFile *f)\n{\n    int num, i, ret;\n    uint32_t features;\n    uint32_t supported_features;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->load_config) {\n        ret = k->load_config(qbus->parent, f);\n        if (ret)\n            return ret;\n    }\n\n    qemu_get_8s(f, &vdev->status);\n    qemu_get_8s(f, &vdev->isr);\n    qemu_get_be16s(f, &vdev->queue_sel);\n    qemu_get_be32s(f, &features);\n\n    if (virtio_set_features(vdev, features) < 0) {\n        supported_features = k->get_features(qbus->parent);\n        error_report(\"Features 0x%x unsupported. Allowed features: 0x%x\",\n                     features, supported_features);\n        return -1;\n    }\n    vdev->config_len = qemu_get_be32(f);\n    qemu_get_buffer(f, vdev->config, vdev->config_len);\n\n    num = qemu_get_be32(f);\n\n    for (i = 0; i < num; i++) {\n        vdev->vq[i].vring.num = qemu_get_be32(f);\n        if (k->has_variable_vring_alignment) {\n            vdev->vq[i].vring.align = qemu_get_be32(f);\n        }\n        vdev->vq[i].pa = qemu_get_be64(f);\n        qemu_get_be16s(f, &vdev->vq[i].last_avail_idx);\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n\n        if (vdev->vq[i].pa) {\n            uint16_t nheads;\n            virtqueue_init(&vdev->vq[i]);\n            nheads = vring_avail_idx(&vdev->vq[i]) - vdev->vq[i].last_avail_idx;\n            /* Check it isn't doing very strange things with descriptor numbers. */\n            if (nheads > vdev->vq[i].vring.num) {\n                error_report(\"VQ %d size 0x%x Guest index 0x%x \"\n                             \"inconsistent with Host index 0x%x: delta 0x%x\",\n                             i, vdev->vq[i].vring.num,\n                             vring_avail_idx(&vdev->vq[i]),\n                             vdev->vq[i].last_avail_idx, nheads);\n                return -1;\n            }\n        } else if (vdev->vq[i].last_avail_idx) {\n            error_report(\"VQ %d address 0x0 \"\n                         \"inconsistent with Host index 0x%x\",\n                         i, vdev->vq[i].last_avail_idx);\n                return -1;\n\t}\n        if (k->load_queue) {\n            ret = k->load_queue(qbus->parent, i, f);\n            if (ret)\n                return ret;\n        }\n    }\n\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n    return 0;\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 146852,
    "human": "Correct. With CWE-94",
    "reason": "It is vulnerable to code injection via memory corruption caused by an unchecked num value.",
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "int virtio_load(VirtIODevice *vdev, QEMUFile *f)\n{\n    int i, ret;\n    uint32_t num;\n    uint32_t features;\n    uint32_t supported_features;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->load_config) {\n        ret = k->load_config(qbus->parent, f);\n        if (ret)\n            return ret;\n    }\n\n    qemu_get_8s(f, &vdev->status);\n    qemu_get_8s(f, &vdev->isr);\n    qemu_get_be16s(f, &vdev->queue_sel);\n    qemu_get_be32s(f, &features);\n\n    if (virtio_set_features(vdev, features) < 0) {\n        supported_features = k->get_features(qbus->parent);\n        error_report(\"Features 0x%x unsupported. Allowed features: 0x%x\",\n                     features, supported_features);\n        return -1;\n    }\n    vdev->config_len = qemu_get_be32(f);\n    qemu_get_buffer(f, vdev->config, vdev->config_len);\n\n    num = qemu_get_be32(f);\n\n    if (num > VIRTIO_PCI_QUEUE_MAX) {\n        error_report(\"Invalid number of PCI queues: 0x%x\", num);\n        return -1;\n    }\n\n    for (i = 0; i < num; i++) {\n        vdev->vq[i].vring.num = qemu_get_be32(f);\n        if (k->has_variable_vring_alignment) {\n            vdev->vq[i].vring.align = qemu_get_be32(f);\n        }\n        vdev->vq[i].pa = qemu_get_be64(f);\n        qemu_get_be16s(f, &vdev->vq[i].last_avail_idx);\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n\n        if (vdev->vq[i].pa) {\n            uint16_t nheads;\n            virtqueue_init(&vdev->vq[i]);\n            nheads = vring_avail_idx(&vdev->vq[i]) - vdev->vq[i].last_avail_idx;\n            /* Check it isn't doing very strange things with descriptor numbers. */\n            if (nheads > vdev->vq[i].vring.num) {\n                error_report(\"VQ %d size 0x%x Guest index 0x%x \"\n                             \"inconsistent with Host index 0x%x: delta 0x%x\",\n                             i, vdev->vq[i].vring.num,\n                             vring_avail_idx(&vdev->vq[i]),\n                             vdev->vq[i].last_avail_idx, nheads);\n                return -1;\n            }\n        } else if (vdev->vq[i].last_avail_idx) {\n            error_report(\"VQ %d address 0x0 \"\n                         \"inconsistent with Host index 0x%x\",\n                         i, vdev->vq[i].last_avail_idx);\n                return -1;\n\t}\n        if (k->load_queue) {\n            ret = k->load_queue(qbus->parent, i, f);\n            if (ret)\n                return ret;\n        }\n    }\n\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n    return 0;\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 146853,
    "human": "Correct. Without CWE-94",
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static int get_iovec_page_array(const struct iovec __user *iov,\n\t\t\t\tunsigned int nr_vecs, struct page **pages,\n\t\t\t\tstruct partial_page *partial, int aligned)\n{\n\tint buffers = 0, error = 0;\n\n\tdown_read(&current->mm->mmap_sem);\n\n\twhile (nr_vecs) {\n\t\tunsigned long off, npages;\n\t\tstruct iovec entry;\n\t\tvoid __user *base;\n\t\tsize_t len;\n\t\tint i;\n\n\t\terror = -EFAULT;\n\t\tif (copy_from_user_mmap_sem(&entry, iov, sizeof(entry)))\n\t\t\tbreak;\n\n\t\tbase = entry.iov_base;\n\t\tlen = entry.iov_len;\n\n\t\t/*\n\t\t * Sanity check this iovec. 0 read succeeds.\n\t\t */\n\t\terror = 0;\n\t\tif (unlikely(!len))\n\t\t\tbreak;\n\t\terror = -EFAULT;\n\t\tif (unlikely(!base))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Get this base offset and number of pages, then map\n\t\t * in the user pages.\n\t\t */\n\t\toff = (unsigned long) base & ~PAGE_MASK;\n\n\t\t/*\n\t\t * If asked for alignment, the offset must be zero and the\n\t\t * length a multiple of the PAGE_SIZE.\n\t\t */\n\t\terror = -EINVAL;\n\t\tif (aligned && (off || len & ~PAGE_MASK))\n\t\t\tbreak;\n\n\t\tnpages = (off + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tif (npages > PIPE_BUFFERS - buffers)\n\t\t\tnpages = PIPE_BUFFERS - buffers;\n\n\t\terror = get_user_pages(current, current->mm,\n\t\t\t\t       (unsigned long) base, npages, 0, 0,\n\t\t\t\t       &pages[buffers], NULL);\n\n\t\tif (unlikely(error <= 0))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Fill this contiguous range into the partial page map.\n\t\t */\n\t\tfor (i = 0; i < error; i++) {\n\t\t\tconst int plen = min_t(size_t, len, PAGE_SIZE - off);\n\n\t\t\tpartial[buffers].offset = off;\n\t\t\tpartial[buffers].len = plen;\n\n\t\t\toff = 0;\n\t\t\tlen -= plen;\n\t\t\tbuffers++;\n\t\t}\n\n\t\t/*\n\t\t * We didn't complete this iov, stop here since it probably\n\t\t * means we have to move some of this into a pipe to\n\t\t * be able to continue.\n\t\t */\n\t\tif (len)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Don't continue if we mapped fewer pages than we asked for,\n\t\t * or if we mapped the max number of pages that we have\n\t\t * room for.\n\t\t */\n\t\tif (error < npages || buffers == PIPE_BUFFERS)\n\t\t\tbreak;\n\n\t\tnr_vecs--;\n\t\tiov++;\n\t}\n\n\tup_read(&current->mm->mmap_sem);\n\n\tif (buffers)\n\t\treturn buffers;\n\n\treturn error;\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 147510,
    "human": "Correct. With CWE-94",
    "reason": "It is vulnerable to code injection (via memory corruption or privilege escalation) due to missing access checks.",
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "static int get_iovec_page_array(const struct iovec __user *iov,\n\t\t\t\tunsigned int nr_vecs, struct page **pages,\n\t\t\t\tstruct partial_page *partial, int aligned)\n{\n\tint buffers = 0, error = 0;\n\n\tdown_read(&current->mm->mmap_sem);\n\n\twhile (nr_vecs) {\n\t\tunsigned long off, npages;\n\t\tstruct iovec entry;\n\t\tvoid __user *base;\n\t\tsize_t len;\n\t\tint i;\n\n\t\terror = -EFAULT;\n\t\tif (copy_from_user_mmap_sem(&entry, iov, sizeof(entry)))\n\t\t\tbreak;\n\n\t\tbase = entry.iov_base;\n\t\tlen = entry.iov_len;\n\n\t\t/*\n\t\t * Sanity check this iovec. 0 read succeeds.\n\t\t */\n\t\terror = 0;\n\t\tif (unlikely(!len))\n\t\t\tbreak;\n\t\terror = -EFAULT;\n\t\tif (!access_ok(VERIFY_READ, base, len))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Get this base offset and number of pages, then map\n\t\t * in the user pages.\n\t\t */\n\t\toff = (unsigned long) base & ~PAGE_MASK;\n\n\t\t/*\n\t\t * If asked for alignment, the offset must be zero and the\n\t\t * length a multiple of the PAGE_SIZE.\n\t\t */\n\t\terror = -EINVAL;\n\t\tif (aligned && (off || len & ~PAGE_MASK))\n\t\t\tbreak;\n\n\t\tnpages = (off + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tif (npages > PIPE_BUFFERS - buffers)\n\t\t\tnpages = PIPE_BUFFERS - buffers;\n\n\t\terror = get_user_pages(current, current->mm,\n\t\t\t\t       (unsigned long) base, npages, 0, 0,\n\t\t\t\t       &pages[buffers], NULL);\n\n\t\tif (unlikely(error <= 0))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Fill this contiguous range into the partial page map.\n\t\t */\n\t\tfor (i = 0; i < error; i++) {\n\t\t\tconst int plen = min_t(size_t, len, PAGE_SIZE - off);\n\n\t\t\tpartial[buffers].offset = off;\n\t\t\tpartial[buffers].len = plen;\n\n\t\t\toff = 0;\n\t\t\tlen -= plen;\n\t\t\tbuffers++;\n\t\t}\n\n\t\t/*\n\t\t * We didn't complete this iov, stop here since it probably\n\t\t * means we have to move some of this into a pipe to\n\t\t * be able to continue.\n\t\t */\n\t\tif (len)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Don't continue if we mapped fewer pages than we asked for,\n\t\t * or if we mapped the max number of pages that we have\n\t\t * room for.\n\t\t */\n\t\tif (error < npages || buffers == PIPE_BUFFERS)\n\t\t\tbreak;\n\n\t\tnr_vecs--;\n\t\tiov++;\n\t}\n\n\tup_read(&current->mm->mmap_sem);\n\n\tif (buffers)\n\t\treturn buffers;\n\n\treturn error;\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 147511,
    "human": "Correct. Without CWE-94",
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "export_desktop_file (const char         *app,\n                     const char         *branch,\n                     const char         *arch,\n                     GKeyFile           *metadata,\n                     const char * const *previous_ids,\n                     int                 parent_fd,\n                     const char         *name,\n                     struct stat        *stat_buf,\n                     char              **target,\n                     GCancellable       *cancellable,\n                     GError            **error)\n{\n  gboolean ret = FALSE;\n  glnx_autofd int desktop_fd = -1;\n  g_autofree char *tmpfile_name = g_strdup_printf (\"export-desktop-XXXXXX\");\n  g_autoptr(GOutputStream) out_stream = NULL;\n  g_autofree gchar *data = NULL;\n  gsize data_len;\n  g_autofree gchar *new_data = NULL;\n  gsize new_data_len;\n  g_autoptr(GKeyFile) keyfile = NULL;\n  g_autofree gchar *old_exec = NULL;\n  gint old_argc;\n  g_auto(GStrv) old_argv = NULL;\n  g_auto(GStrv) groups = NULL;\n  GString *new_exec = NULL;\n  g_autofree char *escaped_app = maybe_quote (app);\n  g_autofree char *escaped_branch = maybe_quote (branch);\n  g_autofree char *escaped_arch = maybe_quote (arch);\n  int i;\n\n  if (!flatpak_openat_noatime (parent_fd, name, &desktop_fd, cancellable, error))\n    goto out;\n\n  if (!read_fd (desktop_fd, stat_buf, &data, &data_len, error))\n    goto out;\n\n  keyfile = g_key_file_new ();\n  if (!g_key_file_load_from_data (keyfile, data, data_len, G_KEY_FILE_KEEP_TRANSLATIONS, error))\n    goto out;\n\n  if (g_str_has_suffix (name, \".service\"))\n    {\n      g_autofree gchar *dbus_name = NULL;\n      g_autofree gchar *expected_dbus_name = g_strndup (name, strlen (name) - strlen (\".service\"));\n\n      dbus_name = g_key_file_get_string (keyfile, \"D-BUS Service\", \"Name\", NULL);\n\n      if (dbus_name == NULL || strcmp (dbus_name, expected_dbus_name) != 0)\n        {\n          return flatpak_fail_error (error, FLATPAK_ERROR_EXPORT_FAILED,\n                                     _(\"D-Bus service file '%s' has wrong name\"), name);\n        }\n    }\n\n  if (g_str_has_suffix (name, \".desktop\"))\n    {\n      gsize length;\n      g_auto(GStrv) tags = g_key_file_get_string_list (metadata,\n                                                       \"Application\",\n                                                       \"tags\", &length,\n                                                       NULL);\n\n      if (tags != NULL)\n        {\n          g_key_file_set_string_list (keyfile,\n                                      G_KEY_FILE_DESKTOP_GROUP,\n                                      \"X-Flatpak-Tags\",\n                                      (const char * const *) tags, length);\n        }\n\n      /* Add a marker so consumers can easily find out that this launches a sandbox */\n      g_key_file_set_string (keyfile, G_KEY_FILE_DESKTOP_GROUP, \"X-Flatpak\", app);\n\n      /* If the app has been renamed, add its old .desktop filename to\n       * X-Flatpak-RenamedFrom in the new .desktop file, taking care not to\n       * introduce duplicates.\n       */\n      if (previous_ids != NULL)\n        {\n          const char *X_FLATPAK_RENAMED_FROM = \"X-Flatpak-RenamedFrom\";\n          g_auto(GStrv) renamed_from = g_key_file_get_string_list (keyfile,\n                                                                   G_KEY_FILE_DESKTOP_GROUP,\n                                                                   X_FLATPAK_RENAMED_FROM,\n                                                                   NULL, NULL);\n          g_autoptr(GPtrArray) merged = g_ptr_array_new_with_free_func (g_free);\n          g_autoptr(GHashTable) seen = g_hash_table_new (g_str_hash, g_str_equal);\n          const char *new_suffix;\n\n          for (i = 0; renamed_from != NULL && renamed_from[i] != NULL; i++)\n            {\n              if (!g_hash_table_contains (seen, renamed_from[i]))\n                {\n                  gchar *copy = g_strdup (renamed_from[i]);\n                  g_hash_table_insert (seen, copy, copy);\n                  g_ptr_array_add (merged, g_steal_pointer (&copy));\n                }\n            }\n\n          /* If an app was renamed from com.example.Foo to net.example.Bar, and\n           * the new version exports net.example.Bar-suffix.desktop, we assume the\n           * old version exported com.example.Foo-suffix.desktop.\n           *\n           * This assertion is true because\n           * flatpak_name_matches_one_wildcard_prefix() is called on all\n           * exported files before we get here.\n           */\n          g_assert (g_str_has_prefix (name, app));\n          /* \".desktop\" for the \"main\" desktop file; something like\n           * \"-suffix.desktop\" for extra ones.\n           */\n          new_suffix = name + strlen (app);\n\n          for (i = 0; previous_ids[i] != NULL; i++)\n            {\n              g_autofree gchar *previous_desktop = g_strconcat (previous_ids[i], new_suffix, NULL);\n              if (!g_hash_table_contains (seen, previous_desktop))\n                {\n                  g_hash_table_insert (seen, previous_desktop, previous_desktop);\n                  g_ptr_array_add (merged, g_steal_pointer (&previous_desktop));\n                }\n            }\n\n          if (merged->len > 0)\n            {\n              g_ptr_array_add (merged, NULL);\n              g_key_file_set_string_list (keyfile,\n                                          G_KEY_FILE_DESKTOP_GROUP,\n                                          X_FLATPAK_RENAMED_FROM,\n                                          (const char * const *) merged->pdata,\n                                          merged->len - 1);\n            }\n        }\n    }\n\n  groups = g_key_file_get_groups (keyfile, NULL);\n\n  for (i = 0; groups[i] != NULL; i++)\n    {\n      g_auto(GStrv) flatpak_run_opts = g_key_file_get_string_list (keyfile, groups[i], \"X-Flatpak-RunOptions\", NULL, NULL);\n      g_autofree char *flatpak_run_args = format_flatpak_run_args_from_run_opts (flatpak_run_opts);\n\n      g_key_file_remove_key (keyfile, groups[i], \"X-Flatpak-RunOptions\", NULL);\n      g_key_file_remove_key (keyfile, groups[i], \"TryExec\", NULL);\n\n      /* Remove this to make sure nothing tries to execute it outside the sandbox*/\n      g_key_file_remove_key (keyfile, groups[i], \"X-GNOME-Bugzilla-ExtraInfoScript\", NULL);\n\n      new_exec = g_string_new (\"\");\n      g_string_append_printf (new_exec,\n                              FLATPAK_BINDIR \"/flatpak run --branch=%s --arch=%s\",\n                              escaped_branch,\n                              escaped_arch);\n\n      if (flatpak_run_args != NULL)\n        g_string_append_printf (new_exec, \"%s\", flatpak_run_args);\n\n      old_exec = g_key_file_get_string (keyfile, groups[i], \"Exec\", NULL);\n      if (old_exec && g_shell_parse_argv (old_exec, &old_argc, &old_argv, NULL) && old_argc >= 1)\n        {\n          int j;\n          g_autofree char *command = maybe_quote (old_argv[0]);\n\n          g_string_append_printf (new_exec, \" --command=%s\", command);\n\n          for (j = 1; j < old_argc; j++)\n            {\n              if (strcasecmp (old_argv[j], \"%f\") == 0 ||\n                  strcasecmp (old_argv[j], \"%u\") == 0)\n                {\n                  g_string_append (new_exec, \" --file-forwarding\");\n                  break;\n                }\n            }\n\n          g_string_append (new_exec, \" \");\n          g_string_append (new_exec, escaped_app);\n\n          for (j = 1; j < old_argc; j++)\n            {\n              g_autofree char *arg = maybe_quote (old_argv[j]);\n\n              if (strcasecmp (arg, \"%f\") == 0)\n                g_string_append_printf (new_exec, \" @@ %s @@\", arg);\n              else if (strcasecmp (arg, \"%u\") == 0)\n                g_string_append_printf (new_exec, \" @@u %s @@\", arg);\n              else if (g_str_has_prefix (arg, \"@@\"))\n                g_print (_(\"Skipping invalid Exec argument %s\\n\"), arg);\n              else\n                g_string_append_printf (new_exec, \" %s\", arg);\n            }\n        }\n      else\n        {\n          g_string_append (new_exec, \" \");\n          g_string_append (new_exec, escaped_app);\n        }\n\n      g_key_file_set_string (keyfile, groups[i], G_KEY_FILE_DESKTOP_KEY_EXEC, new_exec->str);\n    }\n\n  new_data = g_key_file_to_data (keyfile, &new_data_len, error);\n  if (new_data == NULL)\n    goto out;\n\n  if (!flatpak_open_in_tmpdir_at (parent_fd, 0755, tmpfile_name, &out_stream, cancellable, error))\n    goto out;\n\n  if (!g_output_stream_write_all (out_stream, new_data, new_data_len, NULL, cancellable, error))\n    goto out;\n\n  if (!g_output_stream_close (out_stream, cancellable, error))\n    goto out;\n\n  if (target)\n    *target = g_steal_pointer (&tmpfile_name);\n\n  ret = TRUE;\nout:\n\n  if (new_exec != NULL)\n    g_string_free (new_exec, TRUE);\n\n  return ret;\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 148036,
    "human": "Correct. With CWE-94",
    "reason": "It is vulnerable to code injection due to insufficient validation of old_argv arguments.",
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  },
  {
    "CWE_ID": [
      "CWE-94"
    ],
    "code": "export_desktop_file (const char         *app,\n                     const char         *branch,\n                     const char         *arch,\n                     GKeyFile           *metadata,\n                     const char * const *previous_ids,\n                     int                 parent_fd,\n                     const char         *name,\n                     struct stat        *stat_buf,\n                     char              **target,\n                     GCancellable       *cancellable,\n                     GError            **error)\n{\n  gboolean ret = FALSE;\n  glnx_autofd int desktop_fd = -1;\n  g_autofree char *tmpfile_name = g_strdup_printf (\"export-desktop-XXXXXX\");\n  g_autoptr(GOutputStream) out_stream = NULL;\n  g_autofree gchar *data = NULL;\n  gsize data_len;\n  g_autofree gchar *new_data = NULL;\n  gsize new_data_len;\n  g_autoptr(GKeyFile) keyfile = NULL;\n  g_autofree gchar *old_exec = NULL;\n  gint old_argc;\n  g_auto(GStrv) old_argv = NULL;\n  g_auto(GStrv) groups = NULL;\n  GString *new_exec = NULL;\n  g_autofree char *escaped_app = maybe_quote (app);\n  g_autofree char *escaped_branch = maybe_quote (branch);\n  g_autofree char *escaped_arch = maybe_quote (arch);\n  int i;\n\n  if (!flatpak_openat_noatime (parent_fd, name, &desktop_fd, cancellable, error))\n    goto out;\n\n  if (!read_fd (desktop_fd, stat_buf, &data, &data_len, error))\n    goto out;\n\n  keyfile = g_key_file_new ();\n  if (!g_key_file_load_from_data (keyfile, data, data_len, G_KEY_FILE_KEEP_TRANSLATIONS, error))\n    goto out;\n\n  if (g_str_has_suffix (name, \".service\"))\n    {\n      g_autofree gchar *dbus_name = NULL;\n      g_autofree gchar *expected_dbus_name = g_strndup (name, strlen (name) - strlen (\".service\"));\n\n      dbus_name = g_key_file_get_string (keyfile, \"D-BUS Service\", \"Name\", NULL);\n\n      if (dbus_name == NULL || strcmp (dbus_name, expected_dbus_name) != 0)\n        {\n          return flatpak_fail_error (error, FLATPAK_ERROR_EXPORT_FAILED,\n                                     _(\"D-Bus service file '%s' has wrong name\"), name);\n        }\n    }\n\n  if (g_str_has_suffix (name, \".desktop\"))\n    {\n      gsize length;\n      g_auto(GStrv) tags = g_key_file_get_string_list (metadata,\n                                                       \"Application\",\n                                                       \"tags\", &length,\n                                                       NULL);\n\n      if (tags != NULL)\n        {\n          g_key_file_set_string_list (keyfile,\n                                      G_KEY_FILE_DESKTOP_GROUP,\n                                      \"X-Flatpak-Tags\",\n                                      (const char * const *) tags, length);\n        }\n\n      /* Add a marker so consumers can easily find out that this launches a sandbox */\n      g_key_file_set_string (keyfile, G_KEY_FILE_DESKTOP_GROUP, \"X-Flatpak\", app);\n\n      /* If the app has been renamed, add its old .desktop filename to\n       * X-Flatpak-RenamedFrom in the new .desktop file, taking care not to\n       * introduce duplicates.\n       */\n      if (previous_ids != NULL)\n        {\n          const char *X_FLATPAK_RENAMED_FROM = \"X-Flatpak-RenamedFrom\";\n          g_auto(GStrv) renamed_from = g_key_file_get_string_list (keyfile,\n                                                                   G_KEY_FILE_DESKTOP_GROUP,\n                                                                   X_FLATPAK_RENAMED_FROM,\n                                                                   NULL, NULL);\n          g_autoptr(GPtrArray) merged = g_ptr_array_new_with_free_func (g_free);\n          g_autoptr(GHashTable) seen = g_hash_table_new (g_str_hash, g_str_equal);\n          const char *new_suffix;\n\n          for (i = 0; renamed_from != NULL && renamed_from[i] != NULL; i++)\n            {\n              if (!g_hash_table_contains (seen, renamed_from[i]))\n                {\n                  gchar *copy = g_strdup (renamed_from[i]);\n                  g_hash_table_insert (seen, copy, copy);\n                  g_ptr_array_add (merged, g_steal_pointer (&copy));\n                }\n            }\n\n          /* If an app was renamed from com.example.Foo to net.example.Bar, and\n           * the new version exports net.example.Bar-suffix.desktop, we assume the\n           * old version exported com.example.Foo-suffix.desktop.\n           *\n           * This assertion is true because\n           * flatpak_name_matches_one_wildcard_prefix() is called on all\n           * exported files before we get here.\n           */\n          g_assert (g_str_has_prefix (name, app));\n          /* \".desktop\" for the \"main\" desktop file; something like\n           * \"-suffix.desktop\" for extra ones.\n           */\n          new_suffix = name + strlen (app);\n\n          for (i = 0; previous_ids[i] != NULL; i++)\n            {\n              g_autofree gchar *previous_desktop = g_strconcat (previous_ids[i], new_suffix, NULL);\n              if (!g_hash_table_contains (seen, previous_desktop))\n                {\n                  g_hash_table_insert (seen, previous_desktop, previous_desktop);\n                  g_ptr_array_add (merged, g_steal_pointer (&previous_desktop));\n                }\n            }\n\n          if (merged->len > 0)\n            {\n              g_ptr_array_add (merged, NULL);\n              g_key_file_set_string_list (keyfile,\n                                          G_KEY_FILE_DESKTOP_GROUP,\n                                          X_FLATPAK_RENAMED_FROM,\n                                          (const char * const *) merged->pdata,\n                                          merged->len - 1);\n            }\n        }\n    }\n\n  groups = g_key_file_get_groups (keyfile, NULL);\n\n  for (i = 0; groups[i] != NULL; i++)\n    {\n      g_auto(GStrv) flatpak_run_opts = g_key_file_get_string_list (keyfile, groups[i], \"X-Flatpak-RunOptions\", NULL, NULL);\n      g_autofree char *flatpak_run_args = format_flatpak_run_args_from_run_opts (flatpak_run_opts);\n\n      g_key_file_remove_key (keyfile, groups[i], \"X-Flatpak-RunOptions\", NULL);\n      g_key_file_remove_key (keyfile, groups[i], \"TryExec\", NULL);\n\n      /* Remove this to make sure nothing tries to execute it outside the sandbox*/\n      g_key_file_remove_key (keyfile, groups[i], \"X-GNOME-Bugzilla-ExtraInfoScript\", NULL);\n\n      new_exec = g_string_new (\"\");\n      g_string_append_printf (new_exec,\n                              FLATPAK_BINDIR \"/flatpak run --branch=%s --arch=%s\",\n                              escaped_branch,\n                              escaped_arch);\n\n      if (flatpak_run_args != NULL)\n        g_string_append_printf (new_exec, \"%s\", flatpak_run_args);\n\n      old_exec = g_key_file_get_string (keyfile, groups[i], \"Exec\", NULL);\n      if (old_exec && g_shell_parse_argv (old_exec, &old_argc, &old_argv, NULL) && old_argc >= 1)\n        {\n          int j;\n          g_autofree char *command = maybe_quote (old_argv[0]);\n\n          g_string_append_printf (new_exec, \" --command=%s\", command);\n\n          for (j = 1; j < old_argc; j++)\n            {\n              if (strcasecmp (old_argv[j], \"%f\") == 0 ||\n                  strcasecmp (old_argv[j], \"%u\") == 0)\n                {\n                  g_string_append (new_exec, \" --file-forwarding\");\n                  break;\n                }\n            }\n\n          g_string_append (new_exec, \" \");\n          g_string_append (new_exec, escaped_app);\n\n          for (j = 1; j < old_argc; j++)\n            {\n              g_autofree char *arg = maybe_quote (old_argv[j]);\n\n              if (strcasecmp (arg, \"%f\") == 0)\n                g_string_append_printf (new_exec, \" @@ %s @@\", arg);\n              else if (strcasecmp (arg, \"%u\") == 0)\n                g_string_append_printf (new_exec, \" @@u %s @@\", arg);\n              else if (g_str_has_prefix (arg, \"@@\"))\n                {\n                  flatpak_fail_error (error, FLATPAK_ERROR_EXPORT_FAILED,\n                                     _(\"Invalid Exec argument %s\"), arg);\n                  goto out;\n                }\n              else\n                g_string_append_printf (new_exec, \" %s\", arg);\n            }\n        }\n      else\n        {\n          g_string_append (new_exec, \" \");\n          g_string_append (new_exec, escaped_app);\n        }\n\n      g_key_file_set_string (keyfile, groups[i], G_KEY_FILE_DESKTOP_KEY_EXEC, new_exec->str);\n    }\n\n  new_data = g_key_file_to_data (keyfile, &new_data_len, error);\n  if (new_data == NULL)\n    goto out;\n\n  if (!flatpak_open_in_tmpdir_at (parent_fd, 0755, tmpfile_name, &out_stream, cancellable, error))\n    goto out;\n\n  if (!g_output_stream_write_all (out_stream, new_data, new_data_len, NULL, cancellable, error))\n    goto out;\n\n  if (!g_output_stream_close (out_stream, cancellable, error))\n    goto out;\n\n  if (target)\n    *target = g_steal_pointer (&tmpfile_name);\n\n  ret = TRUE;\nout:\n\n  if (new_exec != NULL)\n    g_string_free (new_exec, TRUE);\n\n  return ret;\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 148037,
    "human": "Correct. Without CWE-94",
    "RELATED_CWE": [
      "CWE-90",
      "CWE-611",
      "CWE-89"
    ]
  }
]