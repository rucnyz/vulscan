[
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static bool access_pmovs(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t const struct sys_reg_desc *r)\n{\n\tu64 mask = kvm_pmu_valid_counter_mask(vcpu);\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tif (r->CRm & 0x2)\n\t\t\t/* accessing PMOVSSET_EL0 */\n\t\t\tkvm_pmu_overflow_set(vcpu, p->regval & mask);\n\t\telse\n\t\t\t/* accessing PMOVSCLR_EL0 */\n\t\t\tvcpu_sys_reg(vcpu, PMOVSSET_EL0) &= ~(p->regval & mask);\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMOVSSET_EL0) & mask;\n\t}\n\n\treturn true;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19067,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static bool access_pmselr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t  const struct sys_reg_desc *r)\n{\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write)\n\t\tvcpu_sys_reg(vcpu, PMSELR_EL0) = p->regval;\n\telse\n\t\t/* return PMSELR.SEL field */\n\t\tp->regval = vcpu_sys_reg(vcpu, PMSELR_EL0)\n\t\t\t    & ARMV8_PMU_COUNTER_MASK;\n\n\treturn true;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19068,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static bool access_pmuserenr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t     const struct sys_reg_desc *r)\n{\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (p->is_write) {\n\t\tif (!vcpu_mode_priv(vcpu))\n\t\t\treturn false;\n\n\t\tvcpu_sys_reg(vcpu, PMUSERENR_EL0) = p->regval\n\t\t\t\t\t\t    & ARMV8_PMU_USERENR_MASK;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMUSERENR_EL0)\n\t\t\t    & ARMV8_PMU_USERENR_MASK;\n\t}\n\n\treturn true;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19071,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static int demux_c15_get(u64 id, void __user *uaddr)\n{\n\tu32 val;\n\tu32 __user *uval = uaddr;\n\n\t/* Fail if we have unknown bits set. */\n\tif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\n\t\t   | ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\n\t\treturn -ENOENT;\n\n\tswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\n\tcase KVM_REG_ARM_DEMUX_ID_CCSIDR:\n\t\tif (KVM_REG_SIZE(id) != 4)\n\t\t\treturn -ENOENT;\n\t\tval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\n\t\t\t>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\n\t\tif (!is_valid_cache(val))\n\t\t\treturn -ENOENT;\n\n\t\treturn put_user(get_ccsidr(val), uval);\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19076,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static int demux_c15_set(u64 id, void __user *uaddr)\n{\n\tu32 val, newval;\n\tu32 __user *uval = uaddr;\n\n\t/* Fail if we have unknown bits set. */\n\tif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\n\t\t   | ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\n\t\treturn -ENOENT;\n\n\tswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\n\tcase KVM_REG_ARM_DEMUX_ID_CCSIDR:\n\t\tif (KVM_REG_SIZE(id) != 4)\n\t\t\treturn -ENOENT;\n\t\tval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\n\t\t\t>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\n\t\tif (!is_valid_cache(val))\n\t\t\treturn -ENOENT;\n\n\t\tif (get_user(newval, uval))\n\t\t\treturn -EFAULT;\n\n\t\t/* This is also invariant: you can't change it. */\n\t\tif (newval != get_ccsidr(val))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19077,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static bool index_to_params(u64 id, struct sys_reg_params *params)\n{\n\tswitch (id & KVM_REG_SIZE_MASK) {\n\tcase KVM_REG_SIZE_U64:\n\t\t/* Any unused index bits means it's not valid. */\n\t\tif (id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK\n\t\t\t      | KVM_REG_ARM_COPROC_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP0_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP1_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_CRN_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_CRM_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP2_MASK))\n\t\t\treturn false;\n\t\tparams->Op0 = ((id & KVM_REG_ARM64_SYSREG_OP0_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP0_SHIFT);\n\t\tparams->Op1 = ((id & KVM_REG_ARM64_SYSREG_OP1_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP1_SHIFT);\n\t\tparams->CRn = ((id & KVM_REG_ARM64_SYSREG_CRN_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_CRN_SHIFT);\n\t\tparams->CRm = ((id & KVM_REG_ARM64_SYSREG_CRM_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_CRM_SHIFT);\n\t\tparams->Op2 = ((id & KVM_REG_ARM64_SYSREG_OP2_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP2_SHIFT);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19087,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "int kvm_arm_sys_reg_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tconst struct sys_reg_desc *r;\n\tvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\n\t\treturn demux_c15_get(reg->id, uaddr);\n\n\tif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\n\t\treturn -ENOENT;\n\n\tr = index_to_sys_reg_desc(vcpu, reg->id);\n\tif (!r)\n\t\treturn get_invariant_sys_reg(reg->id, uaddr);\n\n\tif (r->get_user)\n\t\treturn (r->get_user)(vcpu, r, reg, uaddr);\n\n\treturn reg_to_user(uaddr, &vcpu_sys_reg(vcpu, r->reg), reg->id);\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19090,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "int kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tconst struct sys_reg_desc *r;\n\tvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\n\t\treturn demux_c15_set(reg->id, uaddr);\n\n\tif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\n\t\treturn -ENOENT;\n\n\tr = index_to_sys_reg_desc(vcpu, reg->id);\n\tif (!r)\n\t\treturn set_invariant_sys_reg(reg->id, uaddr);\n\n\tif (r->set_user)\n\t\treturn (r->set_user)(vcpu, r, reg, uaddr);\n\n\treturn reg_from_user(&vcpu_sys_reg(vcpu, r->reg), uaddr, reg->id);\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19091,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static int kvm_handle_cp_32(struct kvm_vcpu *vcpu,\n\t\t\t    const struct sys_reg_desc *global,\n\t\t\t    size_t nr_global,\n\t\t\t    const struct sys_reg_desc *target_specific,\n\t\t\t    size_t nr_specific)\n{\n\tstruct sys_reg_params params;\n\tu32 hsr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt  = (hsr >> 5) & 0xf;\n\n\tparams.is_aarch32 = true;\n\tparams.is_32bit = true;\n\tparams.CRm = (hsr >> 1) & 0xf;\n\tparams.regval = vcpu_get_reg(vcpu, Rt);\n\tparams.is_write = ((hsr & 1) == 0);\n\tparams.CRn = (hsr >> 10) & 0xf;\n\tparams.Op0 = 0;\n\tparams.Op1 = (hsr >> 14) & 0x7;\n\tparams.Op2 = (hsr >> 17) & 0x7;\n\n\tif (!emulate_cp(vcpu, &params, target_specific, nr_specific) ||\n\t    !emulate_cp(vcpu, &params, global, nr_global)) {\n\t\tif (!params.is_write)\n\t\t\tvcpu_set_reg(vcpu, Rt, params.regval);\n\t\treturn 1;\n\t}\n\n\tunhandled_cp_access(vcpu, &params);\n\treturn 1;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19097,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "static int kvm_handle_cp_64(struct kvm_vcpu *vcpu,\n\t\t\t    const struct sys_reg_desc *global,\n\t\t\t    size_t nr_global,\n\t\t\t    const struct sys_reg_desc *target_specific,\n\t\t\t    size_t nr_specific)\n{\n\tstruct sys_reg_params params;\n\tu32 hsr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt = (hsr >> 5) & 0xf;\n\tint Rt2 = (hsr >> 10) & 0xf;\n\n\tparams.is_aarch32 = true;\n\tparams.is_32bit = false;\n\tparams.CRm = (hsr >> 1) & 0xf;\n\tparams.is_write = ((hsr & 1) == 0);\n\n\tparams.Op0 = 0;\n\tparams.Op1 = (hsr >> 16) & 0xf;\n\tparams.Op2 = 0;\n\tparams.CRn = 0;\n\n\t/*\n\t * Make a 64-bit value out of Rt and Rt2. As we use the same trap\n\t * backends between AArch32 and AArch64, we get away with it.\n\t */\n\tif (params.is_write) {\n\t\tparams.regval = vcpu_get_reg(vcpu, Rt) & 0xffffffff;\n\t\tparams.regval |= vcpu_get_reg(vcpu, Rt2) << 32;\n\t}\n\n\tif (!emulate_cp(vcpu, &params, target_specific, nr_specific))\n\t\tgoto out;\n\tif (!emulate_cp(vcpu, &params, global, nr_global))\n\t\tgoto out;\n\n\tunhandled_cp_access(vcpu, &params);\n\nout:\n\t/* Split up the value between registers for the read side */\n\tif (!params.is_write) {\n\t\tvcpu_set_reg(vcpu, Rt, lower_32_bits(params.regval));\n\t\tvcpu_set_reg(vcpu, Rt2, upper_32_bits(params.regval));\n\t}\n\n\treturn 1;\n}\n",
    "target": 0,
    "language": "c",
    "dataset": "primevul_nopair",
    "idx": 19098,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "Status ConstantFolding::IsSimplifiableReshape(\n    const NodeDef& node, const GraphProperties& properties) const {\n  if (!IsReshape(node)) {\n    return errors::Internal(\"Node \", node.name(), \" is not a Reshape node\");\n  }\n  if (2 > node.input_size()) {\n    return errors::Internal(\"Node \", node.name(),\n                            \" must have at most 2 inputs but has \",\n                            node.input_size());\n  }\n  const NodeDef* new_shape = node_map_->GetNode(node.input(1));\n  if (!IsReallyConstant(*new_shape)) {\n    return errors::Internal(\"Node \", node.name(), \" has shape \",\n                            new_shape->DebugString(),\n                            \" which is not a constant\");\n  }\n  TensorVector outputs;\n  auto outputs_cleanup = gtl::MakeCleanup([&outputs] {\n    for (const auto& output : outputs) {\n      delete output.tensor;\n    }\n  });\n\n  Status s = EvaluateNode(*new_shape, TensorVector(), &outputs);\n  if (!s.ok()) {\n    return errors::Internal(\"Could not evaluate node \", node.name());\n  }\n  if (outputs.size() != 1) {\n    return errors::Internal(\"Node \", node.name(),\n                            \" must have exactly 1 output but has \",\n                            outputs.size());\n  }\n\n  const std::vector<OpInfo::TensorProperties>& props =\n      properties.GetInputProperties(node.name());\n  if (props.empty()) {\n    return errors::Internal(\"Node \", node.name(), \" has no properties\");\n  }\n  const OpInfo::TensorProperties& prop = props[0];\n  if (prop.dtype() == DT_INVALID) {\n    return errors::Internal(\"Node \", node.name(), \" has property \",\n                            prop.DebugString(), \" with invalid dtype\");\n  }\n  const PartialTensorShape shape(prop.shape());\n  if (!shape.IsFullyDefined()) {\n    return errors::Internal(\"Node \", node.name(), \" has property \",\n                            prop.DebugString(), \" with shape \",\n                            shape.DebugString(), \" which is not fully defined\");\n  }\n\n  PartialTensorShape new_dims;\n  if (outputs[0]->dtype() == DT_INT32) {\n    std::vector<int32> shp;\n    for (int i = 0; i < outputs[0]->NumElements(); ++i) {\n      int32_t dim = outputs[0]->flat<int32>()(i);\n      shp.push_back(dim);\n    }\n    TF_CHECK_OK(TensorShapeUtils::MakeShape(shp, &new_dims));\n  } else {\n    std::vector<int64_t> shp;\n    for (int i = 0; i < outputs[0]->NumElements(); ++i) {\n      int64_t dim = outputs[0]->flat<int64_t>()(i);\n      shp.push_back(dim);\n    }\n    TF_CHECK_OK(TensorShapeUtils::MakeShape(shp, &new_dims));\n  }\n\n  if (!shape.IsCompatibleWith(new_dims)) {\n    return errors::Internal(\"Expected shape \", shape.DebugString(),\n                            \"to be compatible with \", new_dims.DebugString());\n  }\n\n  return Status::OK();\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 8,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "bool DependencyOptimizer::SafeToRemoveIdentity(const NodeDef& node) const {\n  if (!IsIdentity(node) && !IsIdentityN(node)) {\n    return true;\n  }\n\n  if (nodes_to_preserve_.find(node.name()) != nodes_to_preserve_.end()) {\n    return false;\n  }\n  if (!fetch_nodes_known_) {\n    // The output values of this node may be needed.\n    return false;\n  }\n\n  if (node.input_size() < 1) {\n    // Node lacks input, is invalid\n    return false;\n  }\n\n  const NodeDef* input = node_map_->GetNode(NodeName(node.input(0)));\n  CHECK(input != nullptr) << \"node = \" << node.name()\n                          << \" input = \" << node.input(0);\n  // Don't remove Identity nodes corresponding to Variable reads or following\n  // Recv.\n  if (IsVariable(*input) || IsRecv(*input)) {\n    return false;\n  }\n  for (const auto& consumer : node_map_->GetOutputs(node.name())) {\n    if (node.input_size() > 1 && (IsRetval(*consumer) || IsMerge(*consumer))) {\n      return false;\n    }\n    if (IsSwitch(*input)) {\n      for (const string& consumer_input : consumer->input()) {\n        if (consumer_input == AsControlDependency(node.name())) {\n          return false;\n        }\n      }\n    }\n  }\n  return true;\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 34,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "bool Tensor::FromProto(Allocator* a, const TensorProto& proto) {\n  CHECK_NOTNULL(a);\n  TensorBuffer* p = nullptr;\n  if (!TensorShape::IsValid(proto.tensor_shape())) return false;\n  if (proto.dtype() == DT_INVALID) return false;\n  TensorShape shape(proto.tensor_shape());\n  const int64_t N = shape.num_elements();\n  if (N > 0 && proto.dtype()) {\n    bool dtype_error = false;\n    if (!proto.tensor_content().empty()) {\n      const auto& content = proto.tensor_content();\n      CASES_WITH_DEFAULT(proto.dtype(), p = Helper<T>::Decode(a, content, N),\n                         dtype_error = true, dtype_error = true);\n    } else {\n      CASES_WITH_DEFAULT(proto.dtype(), p = FromProtoField<T>(a, proto, N),\n                         dtype_error = true, dtype_error = true);\n    }\n    if (dtype_error || p == nullptr) return false;\n  }\n  shape_ = shape;\n  set_dtype(proto.dtype());\n  UnrefIfNonNull(buf_);\n  buf_ = p;\n  // TODO(misard) add tracking of which kernels and steps are calling\n  // FromProto.\n  if (MemoryLoggingEnabled() && buf_ != nullptr && buf_->data() != nullptr) {\n    LogMemory::RecordTensorAllocation(\"Unknown (from Proto)\",\n                                      LogMemory::UNKNOWN_STEP_ID, *this);\n  }\n  return true;\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 46,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "bool RepeatedAttrDefEqual(\n    const protobuf::RepeatedPtrField<OpDef::AttrDef>& a1,\n    const protobuf::RepeatedPtrField<OpDef::AttrDef>& a2) {\n  std::unordered_map<string, const OpDef::AttrDef*> a1_set;\n  for (const OpDef::AttrDef& def : a1) {\n    DCHECK(a1_set.find(def.name()) == a1_set.end())\n        << \"AttrDef names must be unique, but '\" << def.name()\n        << \"' appears more than once\";\n    a1_set[def.name()] = &def;\n  }\n  for (const OpDef::AttrDef& def : a2) {\n    auto iter = a1_set.find(def.name());\n    if (iter == a1_set.end()) return false;\n    if (!AttrDefEqual(*iter->second, def)) return false;\n    a1_set.erase(iter);\n  }\n  if (!a1_set.empty()) return false;\n  return true;\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 98,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "WandPrivate void CLINoImageOperator(MagickCLI *cli_wand,\n  const char *option,const char *arg1n,const char *arg2n)\n{\n  const char    /* percent escaped versions of the args */\n    *arg1,\n    *arg2;\n\n#define _image_info     (cli_wand->wand.image_info)\n#define _images         (cli_wand->wand.images)\n#define _exception      (cli_wand->wand.exception)\n#define _process_flags  (cli_wand->process_flags)\n#define _option_type    ((CommandOptionFlags) cli_wand->command->flags)\n#define IfNormalOp      (*option=='-')\n#define IfPlusOp        (*option!='-')\n\n  assert(cli_wand != (MagickCLI *) NULL);\n  assert(cli_wand->signature == MagickWandSignature);\n  assert(cli_wand->wand.signature == MagickWandSignature);\n\n  if (cli_wand->wand.debug != MagickFalse)\n    (void) CLILogEvent(cli_wand,CommandEvent,GetMagickModule(),\n      \"- NoImage Operator: %s \\\"%s\\\" \\\"%s\\\"\", option,\n      arg1n != (char *) NULL ? arg1n : \"\",\n      arg2n != (char *) NULL ? arg2n : \"\");\n\n  arg1 = arg1n;\n  arg2 = arg2n;\n\n  /* Interpret Percent Escapes in Arguments - using first image */\n  if ( (((_process_flags & ProcessInterpretProperities) != 0 )\n        || ((_option_type & AlwaysInterpretArgsFlag) != 0)\n       )  && ((_option_type & NeverInterpretArgsFlag) == 0) ) {\n    /* Interpret Percent escapes in argument 1 */\n    if (arg1n != (char *) NULL) {\n      arg1=InterpretImageProperties(_image_info,_images,arg1n,_exception);\n      if (arg1 == (char *) NULL) {\n        CLIWandException(OptionWarning,\"InterpretPropertyFailure\",option);\n        arg1=arg1n;  /* use the given argument as is */\n      }\n    }\n    if (arg2n != (char *) NULL) {\n      arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n      if (arg2 == (char *) NULL) {\n        CLIWandException(OptionWarning,\"InterpretPropertyFailure\",option);\n        arg2=arg2n;  /* use the given argument as is */\n      }\n    }\n  }\n#undef _process_flags\n#undef _option_type\n\n  do {  /* break to exit code */\n    /*\n      No-op options  (ignore these)\n    */\n    if (LocaleCompare(\"noop\",option+1) == 0)   /* zero argument */\n      break;\n    if (LocaleCompare(\"sans\",option+1) == 0)   /* one argument */\n      break;\n    if (LocaleCompare(\"sans0\",option+1) == 0)  /* zero argument */\n      break;\n    if (LocaleCompare(\"sans1\",option+1) == 0)  /* one argument */\n      break;\n    if (LocaleCompare(\"sans2\",option+1) == 0)  /* two arguments */\n      break;\n    /*\n      Image Reading\n    */\n    if ( ( LocaleCompare(\"read\",option+1) == 0 ) ||\n      ( LocaleCompare(\"--\",option) == 0 ) ) {\n      /* Do Glob filename Expansion for 'arg1' then read all images.\n      *\n      * Expansion handles '@', '~', '*', and '?' meta-characters while ignoring\n      * (but attaching to the filenames in the generated argument list) any\n      * [...] read modifiers that may be present.\n      *\n      * For example: It will expand '*.gif[20x20]' into a list such as\n      * 'abc.gif[20x20]',  'foobar.gif[20x20]',  'xyzzy.gif[20x20]'\n      *\n      * NOTE: In IMv6 this was done globally across all images. This\n      * meant you could include IM options in '@filename' lists, but you\n      * could not include comments.   Doing it only for image read makes\n      * it far more secure.\n      *\n      * Note: arguments do not have percent escapes expanded for security\n      * reasons.\n      */\n      int      argc;\n      char     **argv;\n      ssize_t  i;\n\n      argc = 1;\n      argv = (char **) &arg1;\n\n      /* Expand 'glob' expressions in the given filename.\n        Expansion handles any 'coder:' prefix, or read modifiers attached\n        to the filename, including them in the resulting expanded list.\n      */\n      if (ExpandFilenames(&argc,&argv) == MagickFalse)\n        CLIWandExceptArgBreak(ResourceLimitError,\"MemoryAllocationFailed\",\n            option,GetExceptionMessage(errno));\n\n      /* loop over expanded filename list, and read then all in */\n      for (i=0; i < (ssize_t) argc; i++) {\n        Image *\n          new_images;\n        if (_image_info->ping != MagickFalse)\n          new_images=PingImages(_image_info,argv[i],_exception);\n        else\n          new_images=ReadImages(_image_info,argv[i],_exception);\n        AppendImageToList(&_images, new_images);\n        argv[i]=DestroyString(argv[i]);\n      }\n      argv=(char **) RelinquishMagickMemory(argv);\n      break;\n    }\n    /*\n      Image Writing\n      Note: Writing a empty image list is valid in specific cases\n    */\n    if (LocaleCompare(\"write\",option+1) == 0) {\n      /* Note: arguments do not have percent escapes expanded */\n      char\n        key[MagickPathExtent];\n\n      Image\n        *write_images;\n\n      ImageInfo\n        *write_info;\n\n      /* Need images, unless a \"null:\" output coder is used */\n      if ( _images == (Image *) NULL ) {\n        if ( LocaleCompare(arg1,\"null:\") == 0 )\n          break;\n        CLIWandExceptArgBreak(OptionError,\"NoImagesForWrite\",option,arg1);\n      }\n\n      (void) FormatLocaleString(key,MagickPathExtent,\"cache:%s\",arg1);\n      (void) DeleteImageRegistry(key);\n      write_images=CloneImageList(_images,_exception);\n      write_info=CloneImageInfo(_image_info);\n      (void) WriteImages(write_info,write_images,arg1,_exception);\n      write_info=DestroyImageInfo(write_info);\n      write_images=DestroyImageList(write_images);\n      break;\n    }\n    /*\n      Parenthesis and Brace operations\n    */\n    if (LocaleCompare(\"(\",option) == 0) {\n      /* stack 'push' images */\n      Stack\n        *node;\n\n      size_t\n        size;\n\n      size=0;\n      node=cli_wand->image_list_stack;\n      for ( ; node != (Stack *) NULL; node=node->next)\n        size++;\n      if ( size >= MAX_STACK_DEPTH )\n        CLIWandExceptionBreak(OptionError,\"ParenthesisNestedTooDeeply\",option);\n      node=(Stack *) AcquireMagickMemory(sizeof(*node));\n      if (node == (Stack *) NULL)\n        CLIWandExceptionBreak(ResourceLimitFatalError,\n            \"MemoryAllocationFailed\",option);\n      node->data = (void *)cli_wand->wand.images;\n      node->next = cli_wand->image_list_stack;\n      cli_wand->image_list_stack = node;\n      cli_wand->wand.images = NewImageList();\n\n      /* handle respect-parenthesis */\n      if (IsStringTrue(GetImageOption(cli_wand->wand.image_info,\n                    \"respect-parenthesis\")) != MagickFalse)\n        option=\"{\"; /* fall-thru so as to push image settings too */\n      else\n        break;\n      /* fall thru to operation */\n    }\n    if (LocaleCompare(\"{\",option) == 0) {\n      /* stack 'push' of image_info settings */\n      Stack\n        *node;\n\n      size_t\n        size;\n\n      size=0;\n      node=cli_wand->image_info_stack;\n      for ( ; node != (Stack *) NULL; node=node->next)\n        size++;\n      if ( size >= MAX_STACK_DEPTH )\n        CLIWandExceptionBreak(OptionError,\"CurlyBracesNestedTooDeeply\",option);\n      node=(Stack *) AcquireMagickMemory(sizeof(*node));\n      if (node == (Stack *) NULL)\n        CLIWandExceptionBreak(ResourceLimitFatalError,\n            \"MemoryAllocationFailed\",option);\n\n      node->data = (void *)cli_wand->wand.image_info;\n      node->next = cli_wand->image_info_stack;\n\n      cli_wand->image_info_stack = node;\n      cli_wand->wand.image_info = CloneImageInfo(cli_wand->wand.image_info);\n      if (cli_wand->wand.image_info == (ImageInfo *) NULL) {\n        CLIWandException(ResourceLimitFatalError,\"MemoryAllocationFailed\",\n            option);\n        cli_wand->wand.image_info = (ImageInfo *)node->data;\n        node = (Stack *)RelinquishMagickMemory(node);\n        break;\n      }\n\n      break;\n    }\n    if (LocaleCompare(\")\",option) == 0) {\n      /* pop images from stack */\n      Stack\n        *node;\n\n      node = (Stack *)cli_wand->image_list_stack;\n      if ( node == (Stack *) NULL)\n        CLIWandExceptionBreak(OptionError,\"UnbalancedParenthesis\",option);\n      cli_wand->image_list_stack = node->next;\n\n      AppendImageToList((Image **)&node->data,cli_wand->wand.images);\n      cli_wand->wand.images= (Image *)node->data;\n      node = (Stack *)RelinquishMagickMemory(node);\n\n      /* handle respect-parenthesis - of the previous 'pushed' settings */\n      node = cli_wand->image_info_stack;\n      if ( node != (Stack *) NULL)\n        {\n          if (IsStringTrue(GetImageOption(\n                cli_wand->wand.image_info,\"respect-parenthesis\")) != MagickFalse)\n            option=\"}\"; /* fall-thru so as to pop image settings too */\n          else\n            break;\n        }\n      else\n        break;\n      /* fall thru to next if */\n    }\n    if (LocaleCompare(\"}\",option) == 0) {\n      /* pop image_info settings from stack */\n      Stack\n        *node;\n\n      node = (Stack *)cli_wand->image_info_stack;\n      if ( node == (Stack *) NULL)\n        CLIWandExceptionBreak(OptionError,\"UnbalancedCurlyBraces\",option);\n      cli_wand->image_info_stack = node->next;\n\n      (void) DestroyImageInfo(cli_wand->wand.image_info);\n      cli_wand->wand.image_info = (ImageInfo *)node->data;\n      node = (Stack *)RelinquishMagickMemory(node);\n\n      GetDrawInfo(cli_wand->wand.image_info, cli_wand->draw_info);\n      cli_wand->quantize_info=DestroyQuantizeInfo(cli_wand->quantize_info);\n      cli_wand->quantize_info=AcquireQuantizeInfo(cli_wand->wand.image_info);\n\n      break;\n    }\n      if (LocaleCompare(\"print\",option+1) == 0)\n        {\n          (void) FormatLocaleFile(stdout,\"%s\",arg1);\n          break;\n        }\n    if (LocaleCompare(\"set\",option+1) == 0)\n      {\n        /* Settings are applied to each image in memory in turn (if any).\n           While a option: only need to be applied once globally.\n\n           NOTE: rguments have not been automatically percent expaneded\n        */\n\n        /* escape the 'key' once only, using first image. */\n        arg1=InterpretImageProperties(_image_info,_images,arg1n,_exception);\n        if (arg1 == (char *) NULL)\n          CLIWandExceptionBreak(OptionWarning,\"InterpretPropertyFailure\",\n                option);\n\n        if (LocaleNCompare(arg1,\"registry:\",9) == 0)\n          {\n            if (IfPlusOp)\n              {\n                (void) DeleteImageRegistry(arg1+9);\n                arg1=DestroyString((char *)arg1);\n                break;\n              }\n            arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n            if (arg2 == (char *) NULL) {\n              arg1=DestroyString((char *)arg1);\n              CLIWandExceptionBreak(OptionWarning,\"InterpretPropertyFailure\",\n                    option);\n            }\n            (void) SetImageRegistry(StringRegistryType,arg1+9,arg2,_exception);\n            arg1=DestroyString((char *)arg1);\n            arg2=DestroyString((char *)arg2);\n            break;\n          }\n        if (LocaleNCompare(arg1,\"option:\",7) == 0)\n          {\n            /* delete equivelent artifact from all images (if any) */\n            if (_images != (Image *) NULL)\n              {\n                MagickResetIterator(&cli_wand->wand);\n                while (MagickNextImage(&cli_wand->wand) != MagickFalse)\n                  (void) DeleteImageArtifact(_images,arg1+7);\n                MagickResetIterator(&cli_wand->wand);\n              }\n            /* now set/delete the global option as needed */\n            /* FUTURE: make escapes in a global 'option:' delayed */\n            arg2=(char *) NULL;\n            if (IfNormalOp)\n              {\n                arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n                if (arg2 == (char *) NULL)\n                  CLIWandExceptionBreak(OptionWarning,\n                       \"InterpretPropertyFailure\",option);\n              }\n            (void) SetImageOption(_image_info,arg1+7,arg2);\n            arg1=DestroyString((char *)arg1);\n            arg2=DestroyString((char *)arg2);\n            break;\n          }\n        /* Set Artifacts/Properties/Attributes all images (required) */\n        if ( _images == (Image *) NULL )\n          CLIWandExceptArgBreak(OptionWarning,\"NoImageForProperty\",option,arg1);\n\n        MagickResetIterator(&cli_wand->wand);\n        while (MagickNextImage(&cli_wand->wand) != MagickFalse)\n          {\n            arg2=(char *) NULL;\n            if (IfNormalOp)\n              {\n                arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n                if (arg2 == (char *) NULL)\n                  CLIWandExceptionBreak(OptionWarning,\n                       \"InterpretPropertyFailure\",option);\n              }\n            if (LocaleNCompare(arg1,\"artifact:\",9) == 0)\n              (void) SetImageArtifact(_images,arg1+9,arg2);\n            else if (LocaleNCompare(arg1,\"property:\",9) == 0)\n              (void) SetImageProperty(_images,arg1+9,arg2,_exception);\n            else\n              (void) SetImageProperty(_images,arg1,arg2,_exception);\n            arg2=DestroyString((char *)arg2);\n          }\n        MagickResetIterator(&cli_wand->wand);\n        arg1=DestroyString((char *)arg1);\n        break;\n     }\n    if (LocaleCompare(\"clone\",option+1) == 0) {\n        Image\n          *new_images;\n\n        if (*option == '+')\n          arg1=AcquireString(\"-1\");\n        if (IsSceneGeometry(arg1,MagickFalse) == MagickFalse)\n          CLIWandExceptionBreak(OptionError,\"InvalidArgument\",option);\n        if ( cli_wand->image_list_stack == (Stack *) NULL)\n          CLIWandExceptionBreak(OptionError,\"UnableToCloneImage\",option);\n        new_images = (Image *)cli_wand->image_list_stack->data;\n        if (new_images == (Image *) NULL)\n          CLIWandExceptionBreak(OptionError,\"UnableToCloneImage\",option);\n        new_images=CloneImages(new_images,arg1,_exception);\n        if (new_images == (Image *) NULL)\n          CLIWandExceptionBreak(OptionError,\"NoSuchImage\",option);\n        AppendImageToList(&_images,new_images);\n        break;\n      }\n    /*\n       Informational Operations.\n\n       Note that these do not require either a cli-wand or images!\n       Though currently a cli-wand much be provided regardless.\n    */\n    if (LocaleCompare(\"version\",option+1) == 0)\n      {\n        ListMagickVersion(stdout);\n        break;\n      }\n    if (LocaleCompare(\"list\",option+1) == 0) {\n      /*\n         FUTURE: This 'switch' should really be part of MagickCore\n      */\n      ssize_t\n        list;\n\n      list=ParseCommandOption(MagickListOptions,MagickFalse,arg1);\n      if ( list < 0 ) {\n        CLIWandExceptionArg(OptionError,\"UnrecognizedListType\",option,arg1);\n        break;\n      }\n      switch (list)\n      {\n        case MagickCoderOptions:\n        {\n          (void) ListCoderInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickColorOptions:\n        {\n          (void) ListColorInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickConfigureOptions:\n        {\n          (void) ListConfigureInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickDelegateOptions:\n        {\n          (void) ListDelegateInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickFontOptions:\n        {\n          (void) ListTypeInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickFormatOptions:\n          (void) ListMagickInfo((FILE *) NULL,_exception);\n          break;\n        case MagickLocaleOptions:\n          (void) ListLocaleInfo((FILE *) NULL,_exception);\n          break;\n        case MagickLogOptions:\n          (void) ListLogInfo((FILE *) NULL,_exception);\n          break;\n        case MagickMagicOptions:\n          (void) ListMagicInfo((FILE *) NULL,_exception);\n          break;\n        case MagickMimeOptions:\n          (void) ListMimeInfo((FILE *) NULL,_exception);\n          break;\n        case MagickModuleOptions:\n          (void) ListModuleInfo((FILE *) NULL,_exception);\n          break;\n        case MagickPolicyOptions:\n          (void) ListPolicyInfo((FILE *) NULL,_exception);\n          break;\n        case MagickResourceOptions:\n          (void) ListMagickResourceInfo((FILE *) NULL,_exception);\n          break;\n        case MagickThresholdOptions:\n          (void) ListThresholdMaps((FILE *) NULL,_exception);\n          break;\n        default:\n          (void) ListCommandOptions((FILE *) NULL,(CommandOption) list,\n            _exception);\n          break;\n      }\n      break;\n    }\n\n    CLIWandException(OptionError,\"UnrecognizedOption\",option);\n\nDisableMSCWarning(4127)\n  } while (0);  /* break to exit code. */\nRestoreMSCWarning\n\n  /* clean up percent escape interpreted strings */\n  if (arg1 != arg1n )\n    arg1=DestroyString((char *)arg1);\n  if (arg2 != arg2n )\n    arg2=DestroyString((char *)arg2);\n\n#undef _image_info\n#undef _images\n#undef _exception\n#undef IfNormalOp\n#undef IfPlusOp\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 465,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    // If the incoming operation is sharded, use the CSS to infer the filtering metadata for the\n    // collection, otherwise treat it as unsharded\n    auto collectionFilter =\n        CollectionShardingState::get(opCtx, coll->ns())\n            ->getOwnershipFilter(\n                opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (collectionFilter.isSharded() && !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // We can't take ARHASH optimization path for a direct $sample on the system.buckets\n        // collection because data is in compressed form. If we did have a direct $sample on the\n        // system.buckets collection, then the 'bucketUnpacker' would not be set up properly. We\n        // also should bail out early if a $sample is made against a time series collection that is\n        // empty. If we don't the 'minAdvancedToWorkRatio' can be nan/-nan depending on the\n        // architecture.\n        if (!(bucketUnpacker && numRecords)) {\n            return std::pair{nullptr, false};\n        }\n\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 607,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    // If the incoming operation is sharded, use the CSS to infer the filtering metadata for the\n    // collection, otherwise treat it as unsharded\n    auto collectionFilter =\n        CollectionShardingState::get(opCtx, coll->ns())\n            ->getOwnershipFilter(\n                opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (collectionFilter.isSharded() && !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}",
    "target": 1,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 783,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "Status ConstantFolding::IsSimplifiableReshape(\n    const NodeDef& node, const GraphProperties& properties) const {\n  if (!IsReshape(node)) {\n    return errors::Internal(\"Node \", node.name(), \" is not a Reshape node\");\n  }\n  if (2 > node.input_size()) {\n    return errors::Internal(\"Node \", node.name(),\n                            \" must have at most 2 inputs but has \",\n                            node.input_size());\n  }\n  const NodeDef* new_shape = node_map_->GetNode(node.input(1));\n  if (!IsReallyConstant(*new_shape)) {\n    return errors::Internal(\"Node \", node.name(), \" has shape \",\n                            new_shape->DebugString(),\n                            \" which is not a constant\");\n  }\n  TensorVector outputs;\n  auto outputs_cleanup = gtl::MakeCleanup([&outputs] {\n    for (const auto& output : outputs) {\n      delete output.tensor;\n    }\n  });\n\n  Status s = EvaluateNode(*new_shape, TensorVector(), &outputs);\n  if (!s.ok()) {\n    return errors::Internal(\"Could not evaluate node \", node.name());\n  }\n  if (outputs.size() != 1) {\n    return errors::Internal(\"Node \", node.name(),\n                            \" must have exactly 1 output but has \",\n                            outputs.size());\n  }\n\n  const std::vector<OpInfo::TensorProperties>& props =\n      properties.GetInputProperties(node.name());\n  if (props.empty()) {\n    return errors::Internal(\"Node \", node.name(), \" has no properties\");\n  }\n  const OpInfo::TensorProperties& prop = props[0];\n  if (prop.dtype() == DT_INVALID) {\n    return errors::Internal(\"Node \", node.name(), \" has property \",\n                            prop.DebugString(), \" with invalid dtype\");\n  }\n  const PartialTensorShape shape(prop.shape());\n  if (!shape.IsFullyDefined()) {\n    return errors::Internal(\"Node \", node.name(), \" has property \",\n                            prop.DebugString(), \" with shape \",\n                            shape.DebugString(), \" which is not fully defined\");\n  }\n\n  PartialTensorShape new_dims;\n  if (outputs[0]->dtype() == DT_INT32) {\n    std::vector<int32> shp;\n    for (int i = 0; i < outputs[0]->NumElements(); ++i) {\n      int32_t dim = outputs[0]->flat<int32>()(i);\n      shp.push_back(dim);\n    }\n    s = TensorShapeUtils::MakeShape(shp, &new_dims);\n    if (!s.ok()) return s;\n  } else {\n    std::vector<int64_t> shp;\n    for (int i = 0; i < outputs[0]->NumElements(); ++i) {\n      int64_t dim = outputs[0]->flat<int64_t>()(i);\n      shp.push_back(dim);\n    }\n    s = TensorShapeUtils::MakeShape(shp, &new_dims);\n    if (!s.ok()) return s;\n  }\n\n  if (!shape.IsCompatibleWith(new_dims)) {\n    return errors::Internal(\"Expected shape \", shape.DebugString(),\n                            \"to be compatible with \", new_dims.DebugString());\n  }\n\n  return Status::OK();\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 9,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                                            std::vector<NodeDef>* outputs,\n                                            bool* result_too_large) {\n  TensorVector inputs;\n  TensorVector output_tensors;\n  auto inputs_cleanup = gtl::MakeCleanup([&inputs, &output_tensors] {\n    for (const auto& input : inputs) {\n      delete input.tensor;\n    }\n    for (const auto& output : output_tensors) {\n      if (output.tensor) {\n        delete output.tensor;\n      }\n    }\n  });\n\n  size_t total_inputs_size = 0;\n  for (const auto& input : node.input()) {\n    const TensorId input_tensor = ParseTensorName(input);\n    if (input_tensor.index() < 0) {\n      // Control dependency\n      break;\n    }\n    const NodeDef* input_node = node_map_->GetNode(input);\n    if (!IsReallyConstant(*input_node)) {\n      return Status(error::INVALID_ARGUMENT,\n                    strings::StrCat(\"Can't fold \", node.name(), \", its \", input,\n                                    \" isn't constant\"));\n    }\n    TF_RETURN_IF_ERROR(CheckAttrExists(*input_node, \"value\"));\n    const TensorProto& raw_val = input_node->attr().at(\"value\").tensor();\n    if (raw_val.dtype() == DT_INVALID) {\n      return Status(\n          error::INVALID_ARGUMENT,\n          strings::StrCat(\"A tensor in the input node, with TensorId of \",\n                          input_tensor.ToString(),\n                          \" has a dtype of DT_INVALID.\"));\n    }\n    if (IsRefType(raw_val.dtype())) {\n      return errors::InvalidArgument(\n          \"Not allowed to construct a tensor with reference dtype, got \",\n          DataTypeString(raw_val.dtype()));\n    }\n    Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n    if (!value->FromProto(raw_val)) {\n      delete (value);\n      return errors::InvalidArgument(\"Unable to make Tensor from proto for \",\n                                     node.name(), \" with shape \",\n                                     raw_val.tensor_shape().DebugString());\n    }\n    inputs.emplace_back(value);\n    total_inputs_size += value->TotalBytes();\n  }\n\n  TF_RETURN_IF_ERROR(EvaluateNode(node, inputs, &output_tensors));\n  if (output_tensors.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"Expected at least one output.\");\n  }\n\n  outputs->resize(output_tensors.size());\n  for (size_t i = 0; i < output_tensors.size(); i++) {\n    string node_name = OptimizedNodeName(node, \"-folded\");\n    if (output_tensors.size() > 1) {\n      node_name = strings::StrCat(node_name, \"-\", i);\n    }\n    if (output_tensors[i].tensor) {\n      Status s = CreateNodeDef(node_name, output_tensors[i], &outputs->at(i),\n                               total_inputs_size);\n      if (!s.ok()) {\n        *result_too_large = true;\n        return s;\n      }\n    } else {\n      // Create an empty NodeDef to identify dead outputs (e.g. the output of a\n      // switch that's not selected by the switch predicate).\n      outputs->at(i) = NodeDef();\n    }\n  }\n  return Status::OK();\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 13,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "bool DependencyOptimizer::SafeToRemoveIdentity(const NodeDef& node) const {\n  if (!IsIdentity(node) && !IsIdentityN(node)) {\n    return true;\n  }\n\n  if (nodes_to_preserve_.find(node.name()) != nodes_to_preserve_.end()) {\n    return false;\n  }\n  if (!fetch_nodes_known_) {\n    // The output values of this node may be needed.\n    return false;\n  }\n\n  if (node.input_size() < 1) {\n    // Node lacks input, is invalid\n    return false;\n  }\n\n  const NodeDef* input = node_map_->GetNode(NodeName(node.input(0)));\n  if (input == nullptr) {\n    VLOG(1) << \"node = \" << node.name() << \" input = \" << node.input(0);\n    return false;\n  }\n  // Don't remove Identity nodes corresponding to Variable reads or following\n  // Recv.\n  if (IsVariable(*input) || IsRecv(*input)) {\n    return false;\n  }\n  for (const auto& consumer : node_map_->GetOutputs(node.name())) {\n    if (node.input_size() > 1 && (IsRetval(*consumer) || IsMerge(*consumer))) {\n      return false;\n    }\n    if (IsSwitch(*input)) {\n      for (const string& consumer_input : consumer->input()) {\n        if (consumer_input == AsControlDependency(node.name())) {\n          return false;\n        }\n      }\n    }\n  }\n  return true;\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 35,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "  Status BuildInputArgIndex(const OpDef::ArgDef& arg_def, AttrSlice attr_values,\n                            const FunctionDef::ArgAttrs* arg_attrs,\n                            bool ints_on_device,\n                            int64_t resource_arg_unique_id) {\n    bool is_type_list;\n    DataTypeVector dtypes;\n    TF_RETURN_IF_ERROR(\n        ArgNumType(attr_values, arg_def, &is_type_list, &dtypes));\n    if (dtypes.size() < size_t{1}) {\n      return errors::Internal(\"Expected a list of at least one dtype\");\n    }\n    int arg_index = result_.nodes.size();\n    TF_RETURN_IF_ERROR(\n        AddItem(arg_def.name(), {true, arg_index, 0, is_type_list, dtypes}));\n    // Creates dtypes.size() nodes in the graph.\n    for (size_t i = 0; i < dtypes.size(); ++i) {\n      TF_RETURN_IF_ERROR(AddItem(strings::StrCat(arg_def.name(), \":\", i),\n                                 {true, arg_index, 0, false, {dtypes[i]}}));\n      if (arg_index != result_.nodes.size()) {\n        return errors::Internal(\n            \"Expected arg_index to be equal to the number of nodes in result.\",\n            \" Got \", arg_index, \" and \", result_.nodes.size());\n      }\n      string name = arg_def.name();\n      if (dtypes.size() > 1) {\n        strings::StrAppend(&name, \"_\", i);\n      }\n      NodeDef* gnode = AddNode(name);\n      if (ints_on_device && dtypes[i] == DataType::DT_INT32) {\n        gnode->set_op(FunctionLibraryDefinition::kDeviceArgOp);\n      } else {\n        gnode->set_op(FunctionLibraryDefinition::kArgOp);\n      }\n      DataType dtype = arg_def.is_ref() ? MakeRefType(dtypes[i]) : dtypes[i];\n      AddAttr(\"T\", dtype, gnode);\n      AddAttr(\"index\", arg_index, gnode);\n      if (resource_arg_unique_id >= 0) {\n        AddAttr(\"_resource_arg_unique_id\", resource_arg_unique_id, gnode);\n      }\n      if (arg_attrs) {\n        for (const auto& arg_attr : arg_attrs->attr()) {\n          AddAttr(arg_attr.first, arg_attr.second, gnode->mutable_attr());\n        }\n      }\n      result_.arg_types.push_back(dtypes[i]);\n      ++arg_index;\n    }\n    return Status::OK();\n  }",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 63,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "bool RepeatedAttrDefEqual(\n    const protobuf::RepeatedPtrField<OpDef::AttrDef>& a1,\n    const protobuf::RepeatedPtrField<OpDef::AttrDef>& a2) {\n  std::unordered_map<string, const OpDef::AttrDef*> a1_set;\n  for (const OpDef::AttrDef& def : a1) {\n    if (a1_set.find(def.name()) != a1_set.end()) {\n      LOG(ERROR) << \"AttrDef names must be unique, but '\" << def.name()\n                 << \"' appears more than once\";\n    }\n    a1_set[def.name()] = &def;\n  }\n  for (const OpDef::AttrDef& def : a2) {\n    auto iter = a1_set.find(def.name());\n    if (iter == a1_set.end()) return false;\n    if (!AttrDefEqual(*iter->second, def)) return false;\n    a1_set.erase(iter);\n  }\n  if (!a1_set.empty()) return false;\n  return true;\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 99,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "issuerAndThisUpdateCheck(\n\tstruct berval *in,\n\tstruct berval *is,\n\tstruct berval *tu,\n\tvoid *ctx )\n{\n\tint numdquotes = 0;\n\tstruct berval x = *in;\n\tstruct berval ni = BER_BVNULL;\n\t/* Parse GSER format */ \n\tenum {\n\t\tHAVE_NONE = 0x0,\n\t\tHAVE_ISSUER = 0x1,\n\t\tHAVE_THISUPDATE = 0x2,\n\t\tHAVE_ALL = ( HAVE_ISSUER | HAVE_THISUPDATE )\n\t} have = HAVE_NONE;\n\n\n\tif ( in->bv_len < STRLENOF( \"{issuer \\\"\\\",thisUpdate \\\"YYMMDDhhmmssZ\\\"}\" ) ) return LDAP_INVALID_SYNTAX;\n\n\tif ( in->bv_val[0] != '{' || in->bv_val[in->bv_len-1] != '}' ) {\n\t\treturn LDAP_INVALID_SYNTAX;\n\t}\n\n\tx.bv_val++;\n\tx.bv_len -= STRLENOF(\"{}\");\n\n\tdo {\n\t\t/* eat leading spaces */\n\t\tfor ( ; (x.bv_val[0] == ' ') && x.bv_len; x.bv_val++, x.bv_len-- ) {\n\t\t\t/* empty */;\n\t\t}\n\n\t\t/* should be at issuer or thisUpdate */\n\t\tif ( strncasecmp( x.bv_val, \"issuer\", STRLENOF(\"issuer\") ) == 0 ) {\n\t\t\tif ( have & HAVE_ISSUER ) return LDAP_INVALID_SYNTAX;\n\n\t\t\t/* parse issuer */\n\t\t\tx.bv_val += STRLENOF(\"issuer\");\n\t\t\tx.bv_len -= STRLENOF(\"issuer\");\n\n\t\t\tif ( x.bv_val[0] != ' ' ) return LDAP_INVALID_SYNTAX;\n\t\t\tx.bv_val++;\n\t\t\tx.bv_len--;\n\n\t\t\t/* eat leading spaces */\n\t\t\tfor ( ; (x.bv_val[0] == ' ') && x.bv_len; x.bv_val++, x.bv_len-- ) {\n\t\t\t\t/* empty */;\n\t\t\t}\n\n\t\t\t/* For backward compatibility, this part is optional */\n\t\t\tif ( strncasecmp( x.bv_val, \"rdnSequence:\", STRLENOF(\"rdnSequence:\") ) != 0 ) {\n\t\t\t\treturn LDAP_INVALID_SYNTAX;\n\t\t\t}\n\t\t\tx.bv_val += STRLENOF(\"rdnSequence:\");\n\t\t\tx.bv_len -= STRLENOF(\"rdnSequence:\");\n\n\t\t\tif ( x.bv_val[0] != '\"' ) return LDAP_INVALID_SYNTAX;\n\t\t\tx.bv_val++;\n\t\t\tx.bv_len--;\n\n\t\t\tis->bv_val = x.bv_val;\n\t\t\tis->bv_len = 0;\n\n\t\t\tfor ( ; is->bv_len < x.bv_len; ) {\n\t\t\t\tif ( is->bv_val[is->bv_len] != '\"' ) {\n\t\t\t\t\tis->bv_len++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif ( is->bv_val[is->bv_len+1] == '\"' ) {\n\t\t\t\t\t/* double dquote */\n\t\t\t\t\tnumdquotes++;\n\t\t\t\t\tis->bv_len += 2;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tx.bv_val += is->bv_len + 1;\n\t\t\tx.bv_len -= is->bv_len + 1;\n\n\t\t\thave |= HAVE_ISSUER;\n\n\t\t} else if ( strncasecmp( x.bv_val, \"thisUpdate\", STRLENOF(\"thisUpdate\") ) == 0 )\n\t\t{\n\t\t\tif ( have & HAVE_THISUPDATE ) return LDAP_INVALID_SYNTAX;\n\n\t\t\t/* parse thisUpdate */\n\t\t\tx.bv_val += STRLENOF(\"thisUpdate\");\n\t\t\tx.bv_len -= STRLENOF(\"thisUpdate\");\n\n\t\t\tif ( x.bv_val[0] != ' ' ) return LDAP_INVALID_SYNTAX;\n\t\t\tx.bv_val++;\n\t\t\tx.bv_len--;\n\n\t\t\t/* eat leading spaces */\n\t\t\tfor ( ; (x.bv_val[0] == ' ') && x.bv_len; x.bv_val++, x.bv_len-- ) {\n\t\t\t\t/* empty */;\n\t\t\t}\n\n\t\t\tif ( !x.bv_len || x.bv_val[0] != '\"' ) return LDAP_INVALID_SYNTAX;\n\t\t\tx.bv_val++;\n\t\t\tx.bv_len--;\n\n\t\t\ttu->bv_val = x.bv_val;\n\t\t\ttu->bv_len = 0;\n\n\t\t\tfor ( ; tu->bv_len < x.bv_len; tu->bv_len++ ) {\n\t\t\t\tif ( tu->bv_val[tu->bv_len] == '\"' ) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ( tu->bv_len < STRLENOF(\"YYYYmmddHHmmssZ\") ) return LDAP_INVALID_SYNTAX;\n\n\t\t\tx.bv_val += tu->bv_len + 1;\n\t\t\tx.bv_len -= tu->bv_len + 1;\n\n\t\t\thave |= HAVE_THISUPDATE;\n\n\t\t} else {\n\t\t\treturn LDAP_INVALID_SYNTAX;\n\t\t}\n\n\t\t/* eat leading spaces */\n\t\tfor ( ; (x.bv_val[0] == ' ') && x.bv_len; x.bv_val++, x.bv_len-- ) {\n\t\t\t/* empty */;\n\t\t}\n\n\t\tif ( have == HAVE_ALL ) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif ( x.bv_val[0] != ',' ) {\n\t\t\treturn LDAP_INVALID_SYNTAX;\n\t\t}\n\n\t\tx.bv_val++;\n\t\tx.bv_len--;\n\t} while ( 1 );\n\n\t/* should have no characters left... */\n\tif ( x.bv_len ) return LDAP_INVALID_SYNTAX;\n\n\tif ( numdquotes == 0 ) {\n\t\tber_dupbv_x( &ni, is, ctx );\n\n\t} else {\n\t\tber_len_t src, dst;\n\n\t\tni.bv_len = is->bv_len - numdquotes;\n\t\tni.bv_val = slap_sl_malloc( ni.bv_len + 1, ctx );\n\t\tfor ( src = 0, dst = 0; src < is->bv_len; src++, dst++ ) {\n\t\t\tif ( is->bv_val[src] == '\"' ) {\n\t\t\t\tsrc++;\n\t\t\t}\n\t\t\tni.bv_val[dst] = is->bv_val[src];\n\t\t}\n\t\tni.bv_val[dst] = '\\0';\n\t}\n\t\t\n\t*is = ni;\n\n\treturn 0;\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 450,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "WandPrivate void CLINoImageOperator(MagickCLI *cli_wand,\n  const char *option,const char *arg1n,const char *arg2n)\n{\n  const char    /* percent escaped versions of the args */\n    *arg1,\n    *arg2;\n\n#define _image_info     (cli_wand->wand.image_info)\n#define _images         (cli_wand->wand.images)\n#define _exception      (cli_wand->wand.exception)\n#define _process_flags  (cli_wand->process_flags)\n#define _option_type    ((CommandOptionFlags) cli_wand->command->flags)\n#define IfNormalOp      (*option=='-')\n#define IfPlusOp        (*option!='-')\n\n  assert(cli_wand != (MagickCLI *) NULL);\n  assert(cli_wand->signature == MagickWandSignature);\n  assert(cli_wand->wand.signature == MagickWandSignature);\n\n  if (cli_wand->wand.debug != MagickFalse)\n    (void) CLILogEvent(cli_wand,CommandEvent,GetMagickModule(),\n      \"- NoImage Operator: %s \\\"%s\\\" \\\"%s\\\"\", option,\n      arg1n != (char *) NULL ? arg1n : \"\",\n      arg2n != (char *) NULL ? arg2n : \"\");\n\n  arg1 = arg1n;\n  arg2 = arg2n;\n\n  /* Interpret Percent Escapes in Arguments - using first image */\n  if ( (((_process_flags & ProcessInterpretProperities) != 0 )\n        || ((_option_type & AlwaysInterpretArgsFlag) != 0)\n       )  && ((_option_type & NeverInterpretArgsFlag) == 0) ) {\n    /* Interpret Percent escapes in argument 1 */\n    if (arg1n != (char *) NULL) {\n      arg1=InterpretImageProperties(_image_info,_images,arg1n,_exception);\n      if (arg1 == (char *) NULL) {\n        CLIWandException(OptionWarning,\"InterpretPropertyFailure\",option);\n        arg1=arg1n;  /* use the given argument as is */\n      }\n    }\n    if (arg2n != (char *) NULL) {\n      arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n      if (arg2 == (char *) NULL) {\n        CLIWandException(OptionWarning,\"InterpretPropertyFailure\",option);\n        arg2=arg2n;  /* use the given argument as is */\n      }\n    }\n  }\n#undef _process_flags\n#undef _option_type\n\n  do {  /* break to exit code */\n    /*\n      No-op options  (ignore these)\n    */\n    if (LocaleCompare(\"noop\",option+1) == 0)   /* zero argument */\n      break;\n    if (LocaleCompare(\"sans\",option+1) == 0)   /* one argument */\n      break;\n    if (LocaleCompare(\"sans0\",option+1) == 0)  /* zero argument */\n      break;\n    if (LocaleCompare(\"sans1\",option+1) == 0)  /* one argument */\n      break;\n    if (LocaleCompare(\"sans2\",option+1) == 0)  /* two arguments */\n      break;\n    /*\n      Image Reading\n    */\n    if ( ( LocaleCompare(\"read\",option+1) == 0 ) ||\n      ( LocaleCompare(\"--\",option) == 0 ) ) {\n      /* Do Glob filename Expansion for 'arg1' then read all images.\n      *\n      * Expansion handles '@', '~', '*', and '?' meta-characters while ignoring\n      * (but attaching to the filenames in the generated argument list) any\n      * [...] read modifiers that may be present.\n      *\n      * For example: It will expand '*.gif[20x20]' into a list such as\n      * 'abc.gif[20x20]',  'foobar.gif[20x20]',  'xyzzy.gif[20x20]'\n      *\n      * NOTE: In IMv6 this was done globally across all images. This\n      * meant you could include IM options in '@filename' lists, but you\n      * could not include comments.   Doing it only for image read makes\n      * it far more secure.\n      *\n      * Note: arguments do not have percent escapes expanded for security\n      * reasons.\n      */\n      int      argc;\n      char     **argv;\n      ssize_t  i;\n\n      argc = 1;\n      argv = (char **) &arg1;\n\n      /* Expand 'glob' expressions in the given filename.\n        Expansion handles any 'coder:' prefix, or read modifiers attached\n        to the filename, including them in the resulting expanded list.\n      */\n      if (ExpandFilenames(&argc,&argv) == MagickFalse)\n        CLIWandExceptArgBreak(ResourceLimitError,\"MemoryAllocationFailed\",\n            option,GetExceptionMessage(errno));\n\n      /* loop over expanded filename list, and read then all in */\n      for (i=0; i < (ssize_t) argc; i++) {\n        Image *\n          new_images;\n        if (_image_info->ping != MagickFalse)\n          new_images=PingImages(_image_info,argv[i],_exception);\n        else\n          new_images=ReadImages(_image_info,argv[i],_exception);\n        AppendImageToList(&_images, new_images);\n        argv[i]=DestroyString(argv[i]);\n      }\n      argv=(char **) RelinquishMagickMemory(argv);\n      break;\n    }\n    /*\n      Image Writing\n      Note: Writing a empty image list is valid in specific cases\n    */\n    if (LocaleCompare(\"write\",option+1) == 0) {\n      /* Note: arguments do not have percent escapes expanded */\n      char\n        key[MagickPathExtent];\n\n      Image\n        *write_images;\n\n      ImageInfo\n        *write_info;\n\n      /* Need images, unless a \"null:\" output coder is used */\n      if ( _images == (Image *) NULL ) {\n        if ( LocaleCompare(arg1,\"null:\") == 0 )\n          break;\n        CLIWandExceptArgBreak(OptionError,\"NoImagesForWrite\",option,arg1);\n      }\n\n      (void) FormatLocaleString(key,MagickPathExtent,\"cache:%s\",arg1);\n      (void) DeleteImageRegistry(key);\n      write_images=CloneImageList(_images,_exception);\n      write_info=CloneImageInfo(_image_info);\n      if (write_images != (Image *) NULL)\n        (void) WriteImages(write_info,write_images,arg1,_exception);\n      write_info=DestroyImageInfo(write_info);\n      write_images=DestroyImageList(write_images);\n      break;\n    }\n    /*\n      Parenthesis and Brace operations\n    */\n    if (LocaleCompare(\"(\",option) == 0) {\n      /* stack 'push' images */\n      Stack\n        *node;\n\n      size_t\n        size;\n\n      size=0;\n      node=cli_wand->image_list_stack;\n      for ( ; node != (Stack *) NULL; node=node->next)\n        size++;\n      if ( size >= MAX_STACK_DEPTH )\n        CLIWandExceptionBreak(OptionError,\"ParenthesisNestedTooDeeply\",option);\n      node=(Stack *) AcquireMagickMemory(sizeof(*node));\n      if (node == (Stack *) NULL)\n        CLIWandExceptionBreak(ResourceLimitFatalError,\n            \"MemoryAllocationFailed\",option);\n      node->data = (void *)cli_wand->wand.images;\n      node->next = cli_wand->image_list_stack;\n      cli_wand->image_list_stack = node;\n      cli_wand->wand.images = NewImageList();\n\n      /* handle respect-parenthesis */\n      if (IsStringTrue(GetImageOption(cli_wand->wand.image_info,\n                    \"respect-parenthesis\")) != MagickFalse)\n        option=\"{\"; /* fall-thru so as to push image settings too */\n      else\n        break;\n      /* fall thru to operation */\n    }\n    if (LocaleCompare(\"{\",option) == 0) {\n      /* stack 'push' of image_info settings */\n      Stack\n        *node;\n\n      size_t\n        size;\n\n      size=0;\n      node=cli_wand->image_info_stack;\n      for ( ; node != (Stack *) NULL; node=node->next)\n        size++;\n      if ( size >= MAX_STACK_DEPTH )\n        CLIWandExceptionBreak(OptionError,\"CurlyBracesNestedTooDeeply\",option);\n      node=(Stack *) AcquireMagickMemory(sizeof(*node));\n      if (node == (Stack *) NULL)\n        CLIWandExceptionBreak(ResourceLimitFatalError,\n            \"MemoryAllocationFailed\",option);\n\n      node->data = (void *)cli_wand->wand.image_info;\n      node->next = cli_wand->image_info_stack;\n\n      cli_wand->image_info_stack = node;\n      cli_wand->wand.image_info = CloneImageInfo(cli_wand->wand.image_info);\n      if (cli_wand->wand.image_info == (ImageInfo *) NULL) {\n        CLIWandException(ResourceLimitFatalError,\"MemoryAllocationFailed\",\n            option);\n        cli_wand->wand.image_info = (ImageInfo *)node->data;\n        node = (Stack *)RelinquishMagickMemory(node);\n        break;\n      }\n\n      break;\n    }\n    if (LocaleCompare(\")\",option) == 0) {\n      /* pop images from stack */\n      Stack\n        *node;\n\n      node = (Stack *)cli_wand->image_list_stack;\n      if ( node == (Stack *) NULL)\n        CLIWandExceptionBreak(OptionError,\"UnbalancedParenthesis\",option);\n      cli_wand->image_list_stack = node->next;\n\n      AppendImageToList((Image **)&node->data,cli_wand->wand.images);\n      cli_wand->wand.images= (Image *)node->data;\n      node = (Stack *)RelinquishMagickMemory(node);\n\n      /* handle respect-parenthesis - of the previous 'pushed' settings */\n      node = cli_wand->image_info_stack;\n      if ( node != (Stack *) NULL)\n        {\n          if (IsStringTrue(GetImageOption(\n                cli_wand->wand.image_info,\"respect-parenthesis\")) != MagickFalse)\n            option=\"}\"; /* fall-thru so as to pop image settings too */\n          else\n            break;\n        }\n      else\n        break;\n      /* fall thru to next if */\n    }\n    if (LocaleCompare(\"}\",option) == 0) {\n      /* pop image_info settings from stack */\n      Stack\n        *node;\n\n      node = (Stack *)cli_wand->image_info_stack;\n      if ( node == (Stack *) NULL)\n        CLIWandExceptionBreak(OptionError,\"UnbalancedCurlyBraces\",option);\n      cli_wand->image_info_stack = node->next;\n\n      (void) DestroyImageInfo(cli_wand->wand.image_info);\n      cli_wand->wand.image_info = (ImageInfo *)node->data;\n      node = (Stack *)RelinquishMagickMemory(node);\n\n      GetDrawInfo(cli_wand->wand.image_info, cli_wand->draw_info);\n      cli_wand->quantize_info=DestroyQuantizeInfo(cli_wand->quantize_info);\n      cli_wand->quantize_info=AcquireQuantizeInfo(cli_wand->wand.image_info);\n\n      break;\n    }\n      if (LocaleCompare(\"print\",option+1) == 0)\n        {\n          (void) FormatLocaleFile(stdout,\"%s\",arg1);\n          break;\n        }\n    if (LocaleCompare(\"set\",option+1) == 0)\n      {\n        /* Settings are applied to each image in memory in turn (if any).\n           While a option: only need to be applied once globally.\n\n           NOTE: rguments have not been automatically percent expaneded\n        */\n\n        /* escape the 'key' once only, using first image. */\n        arg1=InterpretImageProperties(_image_info,_images,arg1n,_exception);\n        if (arg1 == (char *) NULL)\n          CLIWandExceptionBreak(OptionWarning,\"InterpretPropertyFailure\",\n                option);\n\n        if (LocaleNCompare(arg1,\"registry:\",9) == 0)\n          {\n            if (IfPlusOp)\n              {\n                (void) DeleteImageRegistry(arg1+9);\n                arg1=DestroyString((char *)arg1);\n                break;\n              }\n            arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n            if (arg2 == (char *) NULL) {\n              arg1=DestroyString((char *)arg1);\n              CLIWandExceptionBreak(OptionWarning,\"InterpretPropertyFailure\",\n                    option);\n            }\n            (void) SetImageRegistry(StringRegistryType,arg1+9,arg2,_exception);\n            arg1=DestroyString((char *)arg1);\n            arg2=DestroyString((char *)arg2);\n            break;\n          }\n        if (LocaleNCompare(arg1,\"option:\",7) == 0)\n          {\n            /* delete equivelent artifact from all images (if any) */\n            if (_images != (Image *) NULL)\n              {\n                MagickResetIterator(&cli_wand->wand);\n                while (MagickNextImage(&cli_wand->wand) != MagickFalse)\n                  (void) DeleteImageArtifact(_images,arg1+7);\n                MagickResetIterator(&cli_wand->wand);\n              }\n            /* now set/delete the global option as needed */\n            /* FUTURE: make escapes in a global 'option:' delayed */\n            arg2=(char *) NULL;\n            if (IfNormalOp)\n              {\n                arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n                if (arg2 == (char *) NULL)\n                  CLIWandExceptionBreak(OptionWarning,\n                       \"InterpretPropertyFailure\",option);\n              }\n            (void) SetImageOption(_image_info,arg1+7,arg2);\n            arg1=DestroyString((char *)arg1);\n            arg2=DestroyString((char *)arg2);\n            break;\n          }\n        /* Set Artifacts/Properties/Attributes all images (required) */\n        if ( _images == (Image *) NULL )\n          CLIWandExceptArgBreak(OptionWarning,\"NoImageForProperty\",option,arg1);\n\n        MagickResetIterator(&cli_wand->wand);\n        while (MagickNextImage(&cli_wand->wand) != MagickFalse)\n          {\n            arg2=(char *) NULL;\n            if (IfNormalOp)\n              {\n                arg2=InterpretImageProperties(_image_info,_images,arg2n,_exception);\n                if (arg2 == (char *) NULL)\n                  CLIWandExceptionBreak(OptionWarning,\n                       \"InterpretPropertyFailure\",option);\n              }\n            if (LocaleNCompare(arg1,\"artifact:\",9) == 0)\n              (void) SetImageArtifact(_images,arg1+9,arg2);\n            else if (LocaleNCompare(arg1,\"property:\",9) == 0)\n              (void) SetImageProperty(_images,arg1+9,arg2,_exception);\n            else\n              (void) SetImageProperty(_images,arg1,arg2,_exception);\n            arg2=DestroyString((char *)arg2);\n          }\n        MagickResetIterator(&cli_wand->wand);\n        arg1=DestroyString((char *)arg1);\n        break;\n     }\n    if (LocaleCompare(\"clone\",option+1) == 0) {\n        Image\n          *new_images;\n\n        if (*option == '+')\n          arg1=AcquireString(\"-1\");\n        if (IsSceneGeometry(arg1,MagickFalse) == MagickFalse)\n          CLIWandExceptionBreak(OptionError,\"InvalidArgument\",option);\n        if ( cli_wand->image_list_stack == (Stack *) NULL)\n          CLIWandExceptionBreak(OptionError,\"UnableToCloneImage\",option);\n        new_images = (Image *)cli_wand->image_list_stack->data;\n        if (new_images == (Image *) NULL)\n          CLIWandExceptionBreak(OptionError,\"UnableToCloneImage\",option);\n        new_images=CloneImages(new_images,arg1,_exception);\n        if (new_images == (Image *) NULL)\n          CLIWandExceptionBreak(OptionError,\"NoSuchImage\",option);\n        AppendImageToList(&_images,new_images);\n        break;\n      }\n    /*\n       Informational Operations.\n\n       Note that these do not require either a cli-wand or images!\n       Though currently a cli-wand much be provided regardless.\n    */\n    if (LocaleCompare(\"version\",option+1) == 0)\n      {\n        ListMagickVersion(stdout);\n        break;\n      }\n    if (LocaleCompare(\"list\",option+1) == 0) {\n      /*\n         FUTURE: This 'switch' should really be part of MagickCore\n      */\n      ssize_t\n        list;\n\n      list=ParseCommandOption(MagickListOptions,MagickFalse,arg1);\n      if ( list < 0 ) {\n        CLIWandExceptionArg(OptionError,\"UnrecognizedListType\",option,arg1);\n        break;\n      }\n      switch (list)\n      {\n        case MagickCoderOptions:\n        {\n          (void) ListCoderInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickColorOptions:\n        {\n          (void) ListColorInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickConfigureOptions:\n        {\n          (void) ListConfigureInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickDelegateOptions:\n        {\n          (void) ListDelegateInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickFontOptions:\n        {\n          (void) ListTypeInfo((FILE *) NULL,_exception);\n          break;\n        }\n        case MagickFormatOptions:\n          (void) ListMagickInfo((FILE *) NULL,_exception);\n          break;\n        case MagickLocaleOptions:\n          (void) ListLocaleInfo((FILE *) NULL,_exception);\n          break;\n        case MagickLogOptions:\n          (void) ListLogInfo((FILE *) NULL,_exception);\n          break;\n        case MagickMagicOptions:\n          (void) ListMagicInfo((FILE *) NULL,_exception);\n          break;\n        case MagickMimeOptions:\n          (void) ListMimeInfo((FILE *) NULL,_exception);\n          break;\n        case MagickModuleOptions:\n          (void) ListModuleInfo((FILE *) NULL,_exception);\n          break;\n        case MagickPolicyOptions:\n          (void) ListPolicyInfo((FILE *) NULL,_exception);\n          break;\n        case MagickResourceOptions:\n          (void) ListMagickResourceInfo((FILE *) NULL,_exception);\n          break;\n        case MagickThresholdOptions:\n          (void) ListThresholdMaps((FILE *) NULL,_exception);\n          break;\n        default:\n          (void) ListCommandOptions((FILE *) NULL,(CommandOption) list,\n            _exception);\n          break;\n      }\n      break;\n    }\n\n    CLIWandException(OptionError,\"UnrecognizedOption\",option);\n\nDisableMSCWarning(4127)\n  } while (0);  /* break to exit code. */\nRestoreMSCWarning\n\n  /* clean up percent escape interpreted strings */\n  if (arg1 != arg1n )\n    arg1=DestroyString((char *)arg1);\n  if (arg2 != arg2n )\n    arg2=DestroyString((char *)arg2);\n\n#undef _image_info\n#undef _images\n#undef _exception\n#undef IfNormalOp\n#undef IfPlusOp\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 466,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (auto css = CollectionShardingState::get(opCtx, coll->ns());\n        css->getCollectionDescription(opCtx).isSharded() &&\n        !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // Since the incoming operation is sharded, use the CSS to infer the filtering metadata for\n        // the collection. We get the shard ownership filter after checking to see if the collection\n        // is sharded to avoid an invariant from being fired in this call.\n        auto collectionFilter = css->getOwnershipFilter(\n            opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // We can't take ARHASH optimization path for a direct $sample on the system.buckets\n        // collection because data is in compressed form. If we did have a direct $sample on the\n        // system.buckets collection, then the 'bucketUnpacker' would not be set up properly. We\n        // also should bail out early if a $sample is made against a time series collection that is\n        // empty. If we don't the 'minAdvancedToWorkRatio' can be nan/-nan depending on the\n        // architecture.\n        if (!(bucketUnpacker && numRecords)) {\n            return std::pair{nullptr, false};\n        }\n\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 608,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (auto css = CollectionShardingState::get(opCtx, coll->ns());\n        css->getCollectionDescription(opCtx).isSharded() &&\n        !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // Since the incoming operation is sharded, use the CSS to infer the filtering metadata for\n        // the collection. We get the shard ownership filter after checking to see if the collection\n        // is sharded to avoid an invariant from being fired in this call.\n        auto collectionFilter = css->getOwnershipFilter(\n            opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 784,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "bool Item_equal::create_pushable_equalities(THD *thd,\n                                            List<Item> *equalities,\n                                            Pushdown_checker checker,\n                                            uchar *arg,\n                                            bool clone_const)\n{\n  Item *item;\n  Item *left_item= NULL;\n  Item *right_item = get_const();\n  Item_equal_fields_iterator it(*this);\n\n  while ((item=it++))\n  {\n    left_item= item;\n    if (checker && !((item->*checker) (arg)))\n      continue;\n    break;\n  }\n\n  if (!left_item)\n    return false;\n\n  if (right_item)\n  {\n    Item_func_eq *eq= 0;\n    Item *left_item_clone= left_item->build_clone(thd);\n    Item *right_item_clone= !clone_const ?\n                            right_item : right_item->build_clone(thd);\n    if (!left_item_clone || !right_item_clone)\n      return true;\n    eq= new (thd->mem_root) Item_func_eq(thd,\n                                         left_item_clone,\n                                         right_item_clone);\n    if (!eq ||  equalities->push_back(eq, thd->mem_root))\n      return true;\n    if (!clone_const)\n    {\n      /*\n        Also set IMMUTABLE_FL for any sub-items of the right_item.\n        This is needed to prevent Item::cleanup_excluding_immutables_processor\n        from peforming cleanup of the sub-items and so creating an item tree\n        where a fixed item has non-fixed items inside it.\n      */\n      int new_flag= IMMUTABLE_FL;\n      right_item->walk(&Item::set_extraction_flag_processor, false,\n                       (void*)&new_flag);\n    }\n  }\n\n  while ((item=it++))\n  {\n    if (checker && !((item->*checker) (arg)))\n      continue;\n    Item_func_eq *eq= 0;\n    Item *left_item_clone= left_item->build_clone(thd);\n    Item *right_item_clone= item->build_clone(thd);\n    if (!(left_item_clone && right_item_clone))\n      return true;\n    left_item_clone->set_item_equal(NULL);\n    right_item_clone->set_item_equal(NULL);\n    eq= new (thd->mem_root) Item_func_eq(thd,\n                                         right_item_clone,\n                                         left_item_clone);\n    if (!eq || equalities->push_back(eq, thd->mem_root))\n      return true;\n  }\n  return false;\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 830,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  },
  {
    "CWE_ID": [
      "CWE-617"
    ],
    "code": "multi_update::initialize_tables(JOIN *join)\n{\n  TABLE_LIST *table_ref;\n  DBUG_ENTER(\"initialize_tables\");\n\n  if (unlikely((thd->variables.option_bits & OPTION_SAFE_UPDATES) &&\n               error_if_full_join(join)))\n    DBUG_RETURN(1);\n  if (join->implicit_grouping)\n  {\n    my_error(ER_INVALID_GROUP_FUNC_USE, MYF(0));\n    DBUG_RETURN(1);\n  }\n  main_table=join->join_tab->table;\n  table_to_update= 0;\n\n  /* Any update has at least one pair (field, value) */\n  DBUG_ASSERT(fields->elements);\n  /*\n   Only one table may be modified by UPDATE of an updatable view.\n   For an updatable view first_table_for_update indicates this\n   table.\n   For a regular multi-update it refers to some updated table.\n  */ \n  TABLE *first_table_for_update= ((Item_field *) fields->head())->field->table;\n\n  /* Create a temporary table for keys to all tables, except main table */\n  for (table_ref= update_tables; table_ref; table_ref= table_ref->next_local)\n  {\n    TABLE *table=table_ref->table;\n    uint cnt= table_ref->shared;\n    List<Item> temp_fields;\n    ORDER     group;\n    TMP_TABLE_PARAM *tmp_param;\n\n    if (ignore)\n      table->file->extra(HA_EXTRA_IGNORE_DUP_KEY);\n    if (table == main_table)\t\t\t// First table in join\n    {\n      if (safe_update_on_fly(thd, join->join_tab, table_ref, all_tables))\n      {\n\ttable_to_update= table;\t\t\t// Update table on the fly\n        has_vers_fields= table->vers_check_update(*fields);\n\tcontinue;\n      }\n    }\n    table->prepare_for_position();\n    join->map2table[table->tablenr]->keep_current_rowid= true;\n\n    /*\n      enable uncacheable flag if we update a view with check option\n      and check option has a subselect, otherwise, the check option\n      can be evaluated after the subselect was freed as independent\n      (See full_local in JOIN::join_free()).\n    */\n    if (table_ref->check_option && !join->select_lex->uncacheable)\n    {\n      SELECT_LEX_UNIT *tmp_unit;\n      SELECT_LEX *sl;\n      for (tmp_unit= join->select_lex->first_inner_unit();\n           tmp_unit;\n           tmp_unit= tmp_unit->next_unit())\n      {\n        for (sl= tmp_unit->first_select(); sl; sl= sl->next_select())\n        {\n          if (sl->master_unit()->item)\n          {\n            join->select_lex->uncacheable|= UNCACHEABLE_CHECKOPTION;\n            goto loop_end;\n          }\n        }\n      }\n    }\nloop_end:\n\n    if (table == first_table_for_update && table_ref->check_option)\n    {\n      table_map unupdated_tables= table_ref->check_option->used_tables() &\n                                  ~first_table_for_update->map;\n      List_iterator<TABLE_LIST> ti(*leaves);\n      TABLE_LIST *tbl_ref;\n      while ((tbl_ref= ti++) && unupdated_tables)\n      {\n        if (unupdated_tables & tbl_ref->table->map)\n          unupdated_tables&= ~tbl_ref->table->map;\n        else\n          continue;\n        if (unupdated_check_opt_tables.push_back(tbl_ref->table))\n          DBUG_RETURN(1);\n      }\n    }\n\n    tmp_param= tmp_table_param+cnt;\n\n    /*\n      Create a temporary table to store all fields that are changed for this\n      table. The first field in the temporary table is a pointer to the\n      original row so that we can find and update it. For the updatable\n      VIEW a few following fields are rowids of tables used in the CHECK\n      OPTION condition.\n    */\n\n    List_iterator_fast<TABLE> tbl_it(unupdated_check_opt_tables);\n    TABLE *tbl= table;\n    do\n    {\n      LEX_CSTRING field_name;\n      field_name.str= tbl->alias.c_ptr();\n      field_name.length= strlen(field_name.str);\n      /*\n        Signal each table (including tables referenced by WITH CHECK OPTION\n        clause) for which we will store row position in the temporary table\n        that we need a position to be read first.\n      */\n      tbl->prepare_for_position();\n      join->map2table[tbl->tablenr]->keep_current_rowid= true;\n\n      Item_temptable_rowid *item=\n        new (thd->mem_root) Item_temptable_rowid(tbl);\n      if (!item)\n         DBUG_RETURN(1);\n      item->fix_fields(thd, 0);\n      if (temp_fields.push_back(item, thd->mem_root))\n        DBUG_RETURN(1);\n    } while ((tbl= tbl_it++));\n\n    temp_fields.append(fields_for_table[cnt]);\n\n    /* Make an unique key over the first field to avoid duplicated updates */\n    bzero((char*) &group, sizeof(group));\n    group.direction= ORDER::ORDER_ASC;\n    group.item= (Item**) temp_fields.head_ref();\n\n    tmp_param->quick_group= 1;\n    tmp_param->field_count= temp_fields.elements;\n    tmp_param->func_count=  temp_fields.elements - 1;\n    calc_group_buffer(tmp_param, &group);\n    /* small table, ignore SQL_BIG_TABLES */\n    my_bool save_big_tables= thd->variables.big_tables; \n    thd->variables.big_tables= FALSE;\n    tmp_tables[cnt]=create_tmp_table(thd, tmp_param, temp_fields,\n                                     (ORDER*) &group, 0, 0,\n                                     TMP_TABLE_ALL_COLUMNS, HA_POS_ERROR, &empty_clex_str);\n    thd->variables.big_tables= save_big_tables;\n    if (!tmp_tables[cnt])\n      DBUG_RETURN(1);\n    tmp_tables[cnt]->file->extra(HA_EXTRA_WRITE_CACHE);\n  }\n  join->tmp_table_keep_current_rowid= TRUE;\n  DBUG_RETURN(0);\n}",
    "target": 0,
    "language": "c",
    "dataset": "primevul_pair",
    "idx": 834,
    "RELATED_CWE": [
      "CWE-404",
      "CWE-252",
      "CWE-125"
    ]
  }
]